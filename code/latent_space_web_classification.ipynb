{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying classes in the latent space\n",
    "Domain invariant classifier in the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n",
      "Loading Dataset...\n",
      "Training Websites: [1309, 228, 51, 563, 501, 457, 285, 209, 1385, 1116, 178, 1209, 864, 65, 61, 191, 447, 476, 1034, 1232, 54, 1149, 407, 1466, 1330, 1436, 1490, 859, 451, 919, 1206, 569, 13, 326, 1429, 865, 696, 1468, 318, 440, 689, 1492, 189, 778, 198, 735, 704, 1236, 541, 88, 940, 1098, 255, 775, 161, 1130, 600, 1287, 1266, 740, 1182, 393, 142, 93, 1354, 466, 592, 163, 1482, 206, 1456, 1462, 928, 1301, 747, 333, 758, 727, 429, 1372, 546, 1399, 1327, 146, 1247, 1300, 350, 1093, 1495, 334, 946, 777, 552, 1310, 1140, 449, 1402, 664, 114, 469, 1486, 646, 821, 548, 135, 432, 1161, 644, 435, 1342, 1022, 810, 1316, 939, 292, 542, 1493, 505, 1478, 1103, 538, 1197, 877, 1195, 817, 741, 1404, 283, 1043, 1010, 186, 96, 224, 313, 1285, 327, 1487, 1221, 130, 788, 781, 1220, 958, 1083, 514, 1133, 23, 234, 1099, 1419, 1312, 1463, 1498, 601, 890, 323, 929, 6, 539, 1025, 365, 1039, 217, 1280, 611, 1308, 1338, 1415, 1477, 1366, 765, 330, 1104, 1086, 1, 1226, 663, 1000, 39, 229, 743, 629, 490, 118, 493, 1393, 1445, 175, 995, 141, 1090, 257, 262, 973, 1125, 338, 1384, 1080, 1242, 866, 433, 1417, 411, 638, 1375, 764, 897, 1059, 924, 247, 507, 460, 131, 692, 43, 1204, 1134, 471, 1205, 1471, 14, 145, 120, 468, 138, 64, 676, 1278, 1052, 487, 570, 994, 438, 1298, 270, 1169, 1180, 968, 497, 1262, 833, 389, 193, 1455, 882, 725, 867, 841, 956, 110, 201, 124, 824, 694, 223, 509, 392, 1258, 1448, 918, 287, 1363, 375, 1269, 947, 511, 154, 907, 1127, 200, 103, 1107, 30, 1484, 484, 340, 832, 1268, 985, 437, 1397, 1277, 337, 776, 4, 799, 543, 931, 584, 1414, 1138, 996, 317, 388, 607, 445, 119, 1186, 1110, 1248, 642, 117, 102, 1196, 976, 1029, 1087, 322, 116, 1040, 164, 380, 140, 139, 481, 826, 245, 1166, 504, 81, 167, 858, 1157, 1070, 647, 534, 418, 643, 488, 1213, 1388, 268, 614, 936, 1175, 148, 19, 938, 1153, 204, 150, 1101, 436, 1036, 1170, 271, 714, 1187, 500, 756, 583, 1344, 1293, 1112, 619, 1356, 16, 1135, 613, 212, 275, 1451, 236, 219, 1435, 1461, 557, 577, 431, 702, 416, 540, 1035, 1322, 1355, 104, 1457, 1253, 566, 90, 7, 683, 267, 536, 1328, 904, 875, 1163, 1320, 1233, 305, 73, 1150, 303, 880, 261, 85, 631, 746, 1263, 732, 430, 1234, 210, 724, 1223, 316, 1225, 332, 362, 844, 50, 367, 680, 843, 508, 1350, 1476, 221, 783, 79, 963, 455, 408, 942, 716, 625, 1434, 456, 48, 395, 816, 672, 1452, 1437, 571, 719, 1371, 818, 678, 56, 1137, 1174, 1339, 1155, 78, 222, 889, 707, 1199, 893, 1047, 1058, 1360, 1426, 521, 1120, 1049, 3, 403, 745, 883, 143, 1273, 1050, 1447, 615, 633, 836, 668, 1332, 605, 260, 1243, 861, 1216, 356, 630, 582, 308, 415, 561, 853, 0, 311, 293, 215, 1460, 804, 593, 621, 670, 329, 1431, 452, 1005, 691, 218, 523, 1092, 812, 922, 982, 815, 753, 173, 674, 86, 290, 527, 679, 648, 634, 343, 95, 838, 974, 769, 240, 688, 1207, 230, 825, 203, 1159, 25, 47, 250, 486, 1073, 870, 786, 74, 1072, 424, 1480, 1392, 589, 199, 1454, 713, 1438, 506, 409, 249, 151, 671, 1453, 5, 914, 768, 881, 1046, 906, 109, 797, 1391, 1367, 180, 823, 712, 530, 475, 1497, 1066, 1481, 868, 1200, 467, 136, 820, 937, 1118, 1055, 572, 609, 324, 773, 912, 453, 627, 834, 736, 913, 516, 1177, 850, 1018, 1071, 162, 761, 1255, 971, 1288, 265, 997, 253, 860, 652, 1420, 784, 796, 533, 496, 641, 244, 281, 450, 1079, 730, 1491, 981, 278, 986, 1364, 553, 82, 1406, 1201, 1048, 1026, 710, 156, 723, 1136, 1418, 965, 417, 1304, 555, 477, 425, 63, 211, 852, 1241, 398, 1235, 598, 1386, 20, 1302, 962, 1045, 1171, 1390, 360, 1109, 771, 399, 1421, 551, 1329, 752, 559, 819, 617, 225, 499, 1075, 279, 446, 1261, 29, 863, 344, 684, 695, 1295, 414, 1374, 169, 478, 1361, 637, 1144, 27, 1190, 606, 1132, 1060, 1449, 1380, 658, 1267, 1275, 472, 1369, 1439, 266, 1469, 335, 216, 465, 1410, 345, 779, 809, 284, 770, 1131, 258, 83, 1185, 1146, 767, 1407, 53, 358, 1111, 665, 70, 667, 41, 772, 31, 903, 1160, 1472, 636, 1377, 894, 129, 1126, 685, 1198, 1343, 1245, 1006, 1074, 1307, 377, 171, 620, 1009, 960, 774, 1179, 1283, 1250, 1433, 26, 319, 857, 693, 384, 406, 1351, 1376, 77, 1219, 1051, 1231, 248, 1124, 959, 1020, 700, 1167, 123, 579, 42, 355, 545, 1208, 677, 379, 862, 518, 1323, 349, 12, 1238, 1411, 108, 443, 370, 650, 470, 803, 1284, 941, 1057, 666, 276, 1389, 848, 495, 851, 984, 749, 274, 1115, 251, 1450, 1383, 461, 955, 1427, 1381, 624, 1259, 802, 1240, 1031, 957, 1091, 999, 1252, 1337, 363, 264, 348, 286, 610, 282, 1428, 10, 529, 195, 87, 1290, 1129, 1151, 568, 246, 1270, 661, 502, 458, 17, 1362, 301, 226, 830, 1444, 1475, 595, 949, 1024, 1121, 926, 352, 943, 1496, 871, 1017, 464, 277, 1345, 1334, 1105, 1440, 197, 1148, 122, 1396, 1123, 196, 1081, 902, 900, 603, 537, 1335, 289, 1378, 1256, 1106, 232, 369, 183, 309, 1279, 1194, 1408, 280, 46, 55, 659, 299, 699, 953, 105, 728, 587, 291, 480, 1317, 1336, 687, 188, 52, 798, 489, 1191, 66, 410, 503, 75, 590, 1479, 155, 152, 576, 1015, 989, 254, 121, 1064, 426, 231, 535, 856, 703, 920, 304, 439, 312, 1485, 101, 1405, 807, 1265, 944, 160, 1183, 177, 565, 76, 574, 2, 1173, 585, 898, 298, 33, 237, 295, 987, 901, 72, 239, 662, 202, 656, 763, 978, 596, 272, 1272, 1108, 580, 828, 314, 921, 1373, 127, 479, 594, 412, 887, 512, 1382, 448, 1038, 40, 442, 748, 256, 1423, 1474, 1352, 21, 1139, 1260, 179, 599, 1004, 801, 185, 878, 1346, 528, 522, 1023, 567, 341, 328, 886, 792, 1021, 717, 168, 1096, 737, 1178, 147, 339, 483, 205, 734, 586, 1042, 18, 1314, 45, 1313, 618, 165, 59, 1069, 1430, 532, 263, 422, 1016, 336, 1063, 651, 988, 1210, 1061, 1368, 905, 519, 909, 387, 934, 320, 800, 837, 681, 1333, 930, 896, 67, 1085, 840, 892, 357, 1158, 62, 626, 1192, 1128, 1251, 1078, 1459, 1100, 159, 698, 1119, 829, 208, 1306, 115, 1422, 58, 1488, 60, 331, 1228, 1054, 1282, 366, 149, 1027, 361, 1202, 578, 427, 1089, 241, 932, 233, 731, 967, 895, 97, 306, 1394, 382, 69, 35, 908, 855, 404, 849, 174, 822, 259, 806, 1325, 144, 371, 744, 300, 296, 1217, 972, 935, 1347, 525, 428, 176, 170, 423, 390, 1379, 1257, 873, 1189, 711, 459, 1044, 1271, 421, 1203, 1473, 22, 910, 242, 1214, 1326, 1398, 726, 1424, 750, 517, 639, 1274, 649, 302, 970, 811, 842, 364, 269, 697, 1483, 1172, 1458, 808, 891, 38, 888, 1395, 1222, 757, 751, 755, 524, 1246, 1011, 273, 194, 378, 721, 1403, 612, 1318, 1412, 1019, 1218, 645, 462, 604, 622, 1053, 1088, 923, 1499, 227, 831, 153, 911, 1353, 166, 28, 975, 628, 1324, 220, 660, 125, 1154, 1188, 560, 92, 1370, 89, 1147, 1237, 1165, 759, 564, 791, 1387, 1012]\n",
      "Training Locations: ['LOC1', 'LOC2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asil0892/doh_traffic_analysis/code/scripts/init_dataset.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df.sort_values(by=[\"Location\"], inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Website</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOC1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.086861</td>\n",
       "      <td>0.690199</td>\n",
       "      <td>0.647933</td>\n",
       "      <td>0.201517</td>\n",
       "      <td>0.207139</td>\n",
       "      <td>-0.031855</td>\n",
       "      <td>-0.316768</td>\n",
       "      <td>0.824649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055571</td>\n",
       "      <td>0.160203</td>\n",
       "      <td>0.121677</td>\n",
       "      <td>0.093533</td>\n",
       "      <td>0.09274</td>\n",
       "      <td>0.057062</td>\n",
       "      <td>0.155991</td>\n",
       "      <td>0.109839</td>\n",
       "      <td>0.086282</td>\n",
       "      <td>0.08604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LOC1</td>\n",
       "      <td>1005</td>\n",
       "      <td>1.095547</td>\n",
       "      <td>-1.585943</td>\n",
       "      <td>0.647933</td>\n",
       "      <td>0.343653</td>\n",
       "      <td>0.207139</td>\n",
       "      <td>-0.031855</td>\n",
       "      <td>-0.316768</td>\n",
       "      <td>0.824649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055571</td>\n",
       "      <td>0.160203</td>\n",
       "      <td>0.121677</td>\n",
       "      <td>0.093533</td>\n",
       "      <td>0.09274</td>\n",
       "      <td>0.057062</td>\n",
       "      <td>0.155991</td>\n",
       "      <td>0.109839</td>\n",
       "      <td>0.086282</td>\n",
       "      <td>0.08604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOC1</td>\n",
       "      <td>1005</td>\n",
       "      <td>1.095547</td>\n",
       "      <td>0.780047</td>\n",
       "      <td>-1.972790</td>\n",
       "      <td>0.343653</td>\n",
       "      <td>0.207139</td>\n",
       "      <td>-0.031855</td>\n",
       "      <td>-0.316768</td>\n",
       "      <td>0.824649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055571</td>\n",
       "      <td>0.160203</td>\n",
       "      <td>0.121677</td>\n",
       "      <td>0.093533</td>\n",
       "      <td>0.09274</td>\n",
       "      <td>0.057062</td>\n",
       "      <td>0.155991</td>\n",
       "      <td>0.109839</td>\n",
       "      <td>0.086282</td>\n",
       "      <td>0.08604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LOC1</td>\n",
       "      <td>1005</td>\n",
       "      <td>-1.086861</td>\n",
       "      <td>0.690199</td>\n",
       "      <td>0.647933</td>\n",
       "      <td>0.343653</td>\n",
       "      <td>0.207139</td>\n",
       "      <td>-0.031855</td>\n",
       "      <td>-0.316768</td>\n",
       "      <td>0.824649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055571</td>\n",
       "      <td>0.160203</td>\n",
       "      <td>0.121677</td>\n",
       "      <td>0.093533</td>\n",
       "      <td>0.09274</td>\n",
       "      <td>0.057062</td>\n",
       "      <td>0.155991</td>\n",
       "      <td>0.109839</td>\n",
       "      <td>0.086282</td>\n",
       "      <td>0.08604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LOC1</td>\n",
       "      <td>1005</td>\n",
       "      <td>1.095547</td>\n",
       "      <td>0.780047</td>\n",
       "      <td>0.183501</td>\n",
       "      <td>-0.828965</td>\n",
       "      <td>-2.083179</td>\n",
       "      <td>-0.031855</td>\n",
       "      <td>-0.316768</td>\n",
       "      <td>0.824649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055571</td>\n",
       "      <td>0.160203</td>\n",
       "      <td>0.121677</td>\n",
       "      <td>0.093533</td>\n",
       "      <td>0.09274</td>\n",
       "      <td>0.057062</td>\n",
       "      <td>0.155991</td>\n",
       "      <td>0.109839</td>\n",
       "      <td>0.086282</td>\n",
       "      <td>0.08604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Location  Website         0         1         2         3         4  \\\n",
       "0     LOC1        0 -1.086861  0.690199  0.647933  0.201517  0.207139   \n",
       "1     LOC1     1005  1.095547 -1.585943  0.647933  0.343653  0.207139   \n",
       "2     LOC1     1005  1.095547  0.780047 -1.972790  0.343653  0.207139   \n",
       "3     LOC1     1005 -1.086861  0.690199  0.647933  0.343653  0.207139   \n",
       "4     LOC1     1005  1.095547  0.780047  0.183501 -0.828965 -2.083179   \n",
       "\n",
       "          5         6         7  ...       116       117       118       119  \\\n",
       "0 -0.031855 -0.316768  0.824649  ...  0.055571  0.160203  0.121677  0.093533   \n",
       "1 -0.031855 -0.316768  0.824649  ...  0.055571  0.160203  0.121677  0.093533   \n",
       "2 -0.031855 -0.316768  0.824649  ...  0.055571  0.160203  0.121677  0.093533   \n",
       "3 -0.031855 -0.316768  0.824649  ...  0.055571  0.160203  0.121677  0.093533   \n",
       "4 -0.031855 -0.316768  0.824649  ...  0.055571  0.160203  0.121677  0.093533   \n",
       "\n",
       "       120       121       122       123       124      125  \n",
       "0  0.09274  0.057062  0.155991  0.109839  0.086282  0.08604  \n",
       "1  0.09274  0.057062  0.155991  0.109839  0.086282  0.08604  \n",
       "2  0.09274  0.057062  0.155991  0.109839  0.086282  0.08604  \n",
       "3  0.09274  0.057062  0.155991  0.109839  0.086282  0.08604  \n",
       "4  0.09274  0.057062  0.155991  0.109839  0.086282  0.08604  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scripts.init_gpu as init_gpu\n",
    "import scripts.init_dataset as init_dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "init_gpu.initialize_gpus()\n",
    "\n",
    "locations = ['LOC1', 'LOC2']\n",
    "\n",
    "print(\"Loading Dataset...\")\n",
    "# load the dataset\n",
    "df = pd.read_csv(\n",
    "    f\"../dataset/processed/{locations[0]}-{locations[1]}-scaled-balanced.csv\")\n",
    "\n",
    "length = len(df.columns) - 2  # subtract the two label columns\n",
    "\n",
    "# get train-test set\n",
    "train_df, test_df, train_web_sam1ples, test_web_samples = init_dataset.get_sample(\n",
    "    df, locations, range(1500), 1200)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asil0892/doh_traffic_analysis/.venv/lib/python3.10/site-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from scripts.train_vae import VAE, Sampling, ConvVAE_BatchNorm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# web_model = tf.keras.models.load_model(f\"../models/website/{locations[0]}-{locations[1]}-baseGRU-epochs300-train_samples1200-triplet_samples5.keras\")\n",
    "\n",
    "# location = 'LOC1'\n",
    "vae_model = tf.keras.models.load_model(\"../models/vae/ci_vae/ConvBased/domain_and_class/LOC1-LOC2-e880-mse1-kl0.01-cl1.0-ConvBatchNorm-ldim96-hdim128.keras\", custom_objects={'ConvVAE_BatchNorm': ConvVAE_BatchNorm, 'Sampling': Sampling})\n",
    "\n",
    "def get_z_embeddings(data, vae_model=vae_model):\n",
    "    embeddings = []\n",
    "    chunk_size = 200\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        chunk = data[i:i+chunk_size]\n",
    "        _, _, transformed_chunk = vae_model.encode(chunk)\n",
    "        embeddings.append(transformed_chunk)\n",
    "\n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# get the training data\n",
    "# from train set\n",
    "x_train_traces = train_df.iloc[:, 2:].to_numpy().astype(np.float32)\n",
    "\n",
    "# get the classes from the available data in the LOC1 but missing in LO2 \n",
    "x_available_classes_from_source = test_df[test_df['Location']\n",
    "                                            == 'LOC1'].iloc[:, 2:].to_numpy().astype(np.float32)\n",
    "# combine both: duplicate the data for missing classes \n",
    "x_train_traces = np.vstack((x_train_traces, x_available_classes_from_source, x_available_classes_from_source))\n",
    "\n",
    "# get latent embeddings\n",
    "x_train = get_z_embeddings(x_train_traces, vae_model)\n",
    "\n",
    "# get classes for class informed latent space\n",
    "le = LabelEncoder()\n",
    "y_train = train_df.Website\n",
    "y_available_classes = test_df[test_df['Location']\n",
    "                                == 'LOC1'].Website.to_numpy()\n",
    "y_train = le.fit_transform(np.hstack((y_train, y_available_classes, y_available_classes)))\n",
    "\n",
    "# Get domain labels (0 for LOC1, 1 for LOC2)\n",
    "d_train = (train_df.Location == 'LOC2').astype(int)\n",
    "d_available_classes = np.zeros(len(y_available_classes))  # All from LOC1, so 0\n",
    "d_train = np.hstack((d_train, d_available_classes, d_available_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "x_test_traces = test_df[test_df['Location'] == 'LOC2'].iloc[:, 2:].to_numpy().astype(np.float32)\n",
    "x_test = get_z_embeddings(x_test_traces, vae_model)\n",
    "\n",
    "y_test = le.transform(test_df[test_df['Location'] == 'LOC2'].Website.to_numpy())\n",
    "d_test = np.ones(len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "# from scripts.classification import evaluate_classification_model\n",
    "\n",
    "# clf = XGBClassifier()\n",
    "# evaluate_classification_model(x_train, y_train, x_test, y_test, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Adapting Classifier Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asil0892/doh_traffic_analysis/.venv/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/home/asil0892/doh_traffic_analysis/.venv/lib/python3.10/site-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "class GradientReversalLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, lambda_value=1.0, **kwargs):\n",
    "        super(GradientReversalLayer, self).__init__(**kwargs)\n",
    "        self.lambda_value = lambda_value\n",
    "\n",
    "    def call(self, x):\n",
    "        @tf.custom_gradient\n",
    "        def grad_reverse(x):\n",
    "            def custom_grad(dy):\n",
    "                return -self.lambda_value * dy\n",
    "            return x, custom_grad\n",
    "        return grad_reverse(x)\n",
    "    \n",
    "def classifier_network(input_dim, num_classes, output_name):\n",
    "    return keras.Sequential([\n",
    "        # First dense layer\n",
    "        keras.layers.Dense(\n",
    "            1024,\n",
    "            activation='relu',\n",
    "            input_shape=(input_dim,),\n",
    "            kernel_initializer='he_normal',\n",
    "            bias_initializer='zeros',\n",
    "            kernel_regularizer=keras.regularizers.l2(1e-4)\n",
    "        ),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        \n",
    "        # Second dense layer\n",
    "        keras.layers.Dense(\n",
    "            512,\n",
    "            activation='relu',\n",
    "            kernel_initializer='he_normal',\n",
    "            bias_initializer='zeros',\n",
    "            kernel_regularizer=keras.regularizers.l2(1e-4)\n",
    "        ),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        \n",
    "        # Output layer\n",
    "        keras.layers.Dense(\n",
    "            num_classes,\n",
    "            activation=None,  # No activation for logits\n",
    "            kernel_initializer='glorot_uniform',  # Better for linear activation\n",
    "            bias_initializer='zeros',\n",
    "            kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "            name=output_name\n",
    "        )\n",
    "    ])\n",
    "def cnn_classifier_network(input_shape, num_classes, output_name):\n",
    "    return keras.Sequential([\n",
    "        # Reshape the input to add the channel dimension\n",
    "        keras.layers.InputLayer(input_shape=(input_shape,)),\n",
    "        keras.layers.Reshape((-1, 1)),  # Reshape to (input_shape, 1) for Conv1D\n",
    "        keras.layers.Conv1D(64, kernel_size=3, activation='relu', padding='same'),\n",
    "        keras.layers.MaxPooling1D(pool_size=2),\n",
    "        keras.layers.Conv1D(128, kernel_size=3, activation='relu', padding='same'),\n",
    "        keras.layers.MaxPooling1D(pool_size=2),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(num_classes, activation='relu'),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(num_classes, activation=None, name=output_name)\n",
    "    ])  \n",
    "    \n",
    "def build_domain_invariant_classifier(latent_dim, feature_extractor, class_classifier, domain_classifier, lambda_value=1.0):\n",
    "    # Input\n",
    "    inputs = keras.layers.Input(shape=(latent_dim, 1), name=\"input_layer\")\n",
    "    \n",
    "    # Extract features\n",
    "    features = feature_extractor(inputs)\n",
    "    \n",
    "    # Class predictor\n",
    "    class_output = class_classifier(features)\n",
    "    class_output = tf.keras.layers.Lambda(lambda x: tf.identity(x), name=\"class_output\")(class_output)  # Explicitly name the output\n",
    "    \n",
    "    # Gradient reversal and domain classifier\n",
    "    reversed_features = GradientReversalLayer(lambda_value=lambda_value)(features)\n",
    "    domain_output = domain_classifier(reversed_features)\n",
    "    domain_output = tf.keras.layers.Lambda(lambda x: tf.identity(x), name=\"domain_output\")(domain_output)  # Explicitly name the output\n",
    "    \n",
    "    # Build the model\n",
    "    model = keras.Model(inputs, [class_output, domain_output], name=\"domain_invariant_classifier\")\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Define input shape and parameters\n",
    "latent_dim = 96  # Example input feature size\n",
    "num_classes = 1500    # Number of classes\n",
    "feature_dim = 2048\n",
    "num_domains = 2      # Number of domains\n",
    "lambda_ = 0.1        # Gradient reversal strength\n",
    "\n",
    "domain_classifier = classifier_network(feature_dim, num_domains, \"domain_output\")\n",
    "class_classifier = keras.Sequential([\n",
    "        # Input layer with normalization\n",
    "        keras.layers.InputLayer(input_shape=(feature_dim,)),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        \n",
    "        # First dense block - expanding features\n",
    "        keras.layers.Dense(\n",
    "            2048,\n",
    "            activation='relu',\n",
    "            kernel_initializer='he_normal',\n",
    "            bias_initializer='zeros',\n",
    "            kernel_regularizer=keras.regularizers.l2(1e-4)\n",
    "        ),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        \n",
    "        # Second dense block - maintaining width\n",
    "        keras.layers.Dense(\n",
    "            2048,\n",
    "            activation='relu',\n",
    "            kernel_initializer='he_normal',\n",
    "            bias_initializer='zeros',\n",
    "            kernel_regularizer=keras.regularizers.l2(1e-4)\n",
    "        ),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        \n",
    "        # Third dense block - still wide\n",
    "        keras.layers.Dense(\n",
    "            1800,\n",
    "            activation='relu',\n",
    "            kernel_initializer='he_normal',\n",
    "            bias_initializer='zeros',\n",
    "            kernel_regularizer=keras.regularizers.l2(1e-4)\n",
    "        ),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        \n",
    "        # Output layer\n",
    "        keras.layers.Dense(\n",
    "            num_classes,\n",
    "            activation=None,\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            bias_initializer='zeros',\n",
    "            kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "            name='class_output'\n",
    "        )\n",
    "    ])\n",
    "feature_extractor = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(latent_dim, 1)),\n",
    "    \n",
    "    # First conv block\n",
    "    keras.layers.Conv1D(\n",
    "        filters=128,\n",
    "        kernel_size=5,\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal',\n",
    "        bias_initializer='zeros',\n",
    "        kernel_regularizer=keras.regularizers.l2(1e-4)\n",
    "    ),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling1D(pool_size=2),  # 48\n",
    "    \n",
    "    # Second conv block\n",
    "    keras.layers.Conv1D(\n",
    "        filters=256,\n",
    "        kernel_size=3,\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal',\n",
    "        bias_initializer='zeros',\n",
    "        kernel_regularizer=keras.regularizers.l2(1e-4)\n",
    "    ),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling1D(pool_size=2),  # 24\n",
    "    \n",
    "    # Third conv block\n",
    "    keras.layers.Conv1D(\n",
    "        filters=256,\n",
    "        kernel_size=3,\n",
    "        activation='relu',\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal',\n",
    "        bias_initializer='zeros',\n",
    "        kernel_regularizer=keras.regularizers.l2(1e-4)\n",
    "    ),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.MaxPooling1D(pool_size=2),  # 12\n",
    "    \n",
    "    # Dense layers\n",
    "    keras.layers.Flatten(),  # 12 * 256\n",
    "    keras.layers.Dense(\n",
    "        feature_dim,\n",
    "        activation='relu',\n",
    "        kernel_initializer='he_normal',\n",
    "        bias_initializer='zeros',\n",
    "        kernel_regularizer=keras.regularizers.l2(1e-4)\n",
    "    ),\n",
    "    keras.layers.BatchNormalization()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListWrapper(['class_output', 'domain_output'])\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732258211.770156   20255 service.cc:148] XLA service 0x7f3d00015940 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1732258211.770170   20255 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "I0000 00:00:1732258211.770172   20255 service.cc:156]   StreamExecutor device (1): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2024-11-22 17:50:11.903363: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-11-22 17:50:13.181372: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:930] The NVIDIA driver's CUDA version is 12.2 which is older than the PTX compiler version 12.5.82. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "2024-11-22 17:50:13.587260: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4742', 104 bytes spill stores, 104 bytes spill loads\n",
      "\n",
      "2024-11-22 17:50:13.734381: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4103', 80 bytes spill stores, 80 bytes spill loads\n",
      "\n",
      "2024-11-22 17:50:13.802908: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4742_0', 1700 bytes spill stores, 2196 bytes spill loads\n",
      "\n",
      "2024-11-22 17:50:13.907496: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4103', 312 bytes spill stores, 312 bytes spill loads\n",
      "\n",
      "2024-11-22 17:50:13.936284: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4103', 132 bytes spill stores, 132 bytes spill loads\n",
      "\n",
      "2024-11-22 17:50:13.969486: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4103_0', 888 bytes spill stores, 608 bytes spill loads\n",
      "\n",
      "2024-11-22 17:50:14.002644: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4103', 1480 bytes spill stores, 1076 bytes spill loads\n",
      "\n",
      "2024-11-22 17:50:14.087496: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4112_0', 240 bytes spill stores, 240 bytes spill loads\n",
      "\n",
      "2024-11-22 17:50:14.171731: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4103_0', 52 bytes spill stores, 52 bytes spill loads\n",
      "\n",
      "I0000 00:00:1732258216.742697   20255 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  55/4688\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28:46\u001b[0m 373ms/step - class_output_accuracy: 3.6293e-04 - class_output_loss: 7.9708 - domain_output_accuracy: 0.4692 - domain_output_loss: 12.0200 - loss: 22.1902"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 17:50:33.918785: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4742', 104 bytes spill stores, 104 bytes spill loads\n",
      "\n",
      "2024-11-22 17:50:34.088704: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4103', 116 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "2024-11-22 17:50:34.142014: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4103', 132 bytes spill stores, 132 bytes spill loads\n",
      "\n",
      "2024-11-22 17:50:34.160567: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4103', 80 bytes spill stores, 80 bytes spill loads\n",
      "\n",
      "2024-11-22 17:50:34.161427: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4103', 1584 bytes spill stores, 1168 bytes spill loads\n",
      "\n",
      "2024-11-22 17:50:34.302680: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4103_0', 60 bytes spill stores, 60 bytes spill loads\n",
      "\n",
      "2024-11-22 17:50:34.320875: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4112', 236 bytes spill stores, 236 bytes spill loads\n",
      "\n",
      "2024-11-22 17:50:34.448018: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4103', 312 bytes spill stores, 312 bytes spill loads\n",
      "\n",
      "2024-11-22 17:50:34.459774: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4112', 228 bytes spill stores, 228 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 8ms/step - class_output_accuracy: 0.0066 - class_output_loss: 7.4945 - domain_output_accuracy: 0.5050 - domain_output_loss: 10.4389 - loss: 20.1167\n",
      "Epoch 2/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - class_output_accuracy: 0.1766 - class_output_loss: 5.1511 - domain_output_accuracy: 0.5100 - domain_output_loss: 8.7085 - loss: 16.0327\n",
      "Epoch 3/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - class_output_accuracy: 0.4793 - class_output_loss: 3.0615 - domain_output_accuracy: 0.5041 - domain_output_loss: 8.1886 - loss: 13.4159\n",
      "Epoch 4/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - class_output_accuracy: 0.6393 - class_output_loss: 1.9857 - domain_output_accuracy: 0.5149 - domain_output_loss: 7.6374 - loss: 11.7788\n",
      "Epoch 5/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 9ms/step - class_output_accuracy: 0.7098 - class_output_loss: 1.5032 - domain_output_accuracy: 0.5360 - domain_output_loss: 7.2248 - loss: 10.8683\n",
      "Epoch 6/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.7493 - class_output_loss: 1.2580 - domain_output_accuracy: 0.5589 - domain_output_loss: 6.9928 - loss: 10.3714\n",
      "Epoch 7/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.7728 - class_output_loss: 1.1097 - domain_output_accuracy: 0.5779 - domain_output_loss: 6.8604 - loss: 10.0665\n",
      "Epoch 8/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 9ms/step - class_output_accuracy: 0.7898 - class_output_loss: 1.0128 - domain_output_accuracy: 0.5875 - domain_output_loss: 6.8059 - loss: 9.8871\n",
      "Epoch 9/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - class_output_accuracy: 0.7994 - class_output_loss: 0.9543 - domain_output_accuracy: 0.5910 - domain_output_loss: 6.7838 - loss: 9.7760\n",
      "Epoch 10/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - class_output_accuracy: 0.8102 - class_output_loss: 0.8946 - domain_output_accuracy: 0.5960 - domain_output_loss: 6.7545 - loss: 9.6538\n",
      "Epoch 11/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 9ms/step - class_output_accuracy: 0.8161 - class_output_loss: 0.8635 - domain_output_accuracy: 0.5987 - domain_output_loss: 6.7335 - loss: 9.5674\n",
      "Epoch 12/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.8228 - class_output_loss: 0.8332 - domain_output_accuracy: 0.5991 - domain_output_loss: 6.7309 - loss: 9.4999\n",
      "Epoch 13/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - class_output_accuracy: 0.8273 - class_output_loss: 0.8086 - domain_output_accuracy: 0.5986 - domain_output_loss: 6.7246 - loss: 9.4337\n",
      "Epoch 14/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - class_output_accuracy: 0.8319 - class_output_loss: 0.7866 - domain_output_accuracy: 0.5969 - domain_output_loss: 6.7377 - loss: 9.3893\n",
      "Epoch 15/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.8364 - class_output_loss: 0.7674 - domain_output_accuracy: 0.5986 - domain_output_loss: 6.7275 - loss: 9.3249\n",
      "Epoch 16/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.8402 - class_output_loss: 0.7480 - domain_output_accuracy: 0.5981 - domain_output_loss: 6.7360 - loss: 9.2791\n",
      "Epoch 17/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.8437 - class_output_loss: 0.7335 - domain_output_accuracy: 0.5994 - domain_output_loss: 6.7343 - loss: 9.2285\n",
      "Epoch 18/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 9ms/step - class_output_accuracy: 0.8472 - class_output_loss: 0.7164 - domain_output_accuracy: 0.5999 - domain_output_loss: 6.7251 - loss: 9.1684\n",
      "Epoch 19/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.8498 - class_output_loss: 0.7061 - domain_output_accuracy: 0.5993 - domain_output_loss: 6.7226 - loss: 9.1227\n",
      "Epoch 20/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - class_output_accuracy: 0.8536 - class_output_loss: 0.6888 - domain_output_accuracy: 0.5989 - domain_output_loss: 6.7244 - loss: 9.0752\n",
      "Epoch 21/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.8564 - class_output_loss: 0.6802 - domain_output_accuracy: 0.5990 - domain_output_loss: 6.7266 - loss: 9.0366\n",
      "Epoch 22/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - class_output_accuracy: 0.8609 - class_output_loss: 0.6642 - domain_output_accuracy: 0.5980 - domain_output_loss: 6.7358 - loss: 8.9989\n",
      "Epoch 23/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.8621 - class_output_loss: 0.6559 - domain_output_accuracy: 0.5991 - domain_output_loss: 6.7353 - loss: 8.9602\n",
      "Epoch 24/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - class_output_accuracy: 0.8651 - class_output_loss: 0.6470 - domain_output_accuracy: 0.5991 - domain_output_loss: 6.7312 - loss: 8.9178\n",
      "Epoch 25/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - class_output_accuracy: 0.8684 - class_output_loss: 0.6337 - domain_output_accuracy: 0.5995 - domain_output_loss: 6.7235 - loss: 8.8686\n",
      "Epoch 26/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.8713 - class_output_loss: 0.6240 - domain_output_accuracy: 0.6003 - domain_output_loss: 6.7232 - loss: 8.8309\n",
      "Epoch 27/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.8737 - class_output_loss: 0.6120 - domain_output_accuracy: 0.5999 - domain_output_loss: 6.7232 - loss: 8.7921\n",
      "Epoch 28/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.8761 - class_output_loss: 0.6044 - domain_output_accuracy: 0.5994 - domain_output_loss: 6.7231 - loss: 8.7584\n",
      "Epoch 29/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - class_output_accuracy: 0.8786 - class_output_loss: 0.5950 - domain_output_accuracy: 0.5994 - domain_output_loss: 6.7282 - loss: 8.7285\n",
      "Epoch 30/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.8814 - class_output_loss: 0.5817 - domain_output_accuracy: 0.5997 - domain_output_loss: 6.7212 - loss: 8.6837\n",
      "Epoch 31/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 9ms/step - class_output_accuracy: 0.8839 - class_output_loss: 0.5747 - domain_output_accuracy: 0.5994 - domain_output_loss: 6.7410 - loss: 8.6728\n",
      "Epoch 32/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.8859 - class_output_loss: 0.5677 - domain_output_accuracy: 0.5993 - domain_output_loss: 6.7321 - loss: 8.6343\n",
      "Epoch 33/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - class_output_accuracy: 0.8891 - class_output_loss: 0.5556 - domain_output_accuracy: 0.5994 - domain_output_loss: 6.7210 - loss: 8.5887\n",
      "Epoch 34/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.8917 - class_output_loss: 0.5443 - domain_output_accuracy: 0.6003 - domain_output_loss: 6.7362 - loss: 8.5710\n",
      "Epoch 35/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.8936 - class_output_loss: 0.5384 - domain_output_accuracy: 0.5994 - domain_output_loss: 6.7261 - loss: 8.5340\n",
      "Epoch 36/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.8969 - class_output_loss: 0.5247 - domain_output_accuracy: 0.6004 - domain_output_loss: 6.7337 - loss: 8.5075\n",
      "Epoch 37/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - class_output_accuracy: 0.8982 - class_output_loss: 0.5201 - domain_output_accuracy: 0.5988 - domain_output_loss: 6.7301 - loss: 8.4798\n",
      "Epoch 38/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.9012 - class_output_loss: 0.5081 - domain_output_accuracy: 0.6003 - domain_output_loss: 6.7216 - loss: 8.4407\n",
      "Epoch 39/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - class_output_accuracy: 0.9029 - class_output_loss: 0.5003 - domain_output_accuracy: 0.5998 - domain_output_loss: 6.7310 - loss: 8.4241\n",
      "Epoch 40/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.9047 - class_output_loss: 0.4931 - domain_output_accuracy: 0.5987 - domain_output_loss: 6.7293 - loss: 8.3976\n",
      "Epoch 41/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.9076 - class_output_loss: 0.4809 - domain_output_accuracy: 0.6002 - domain_output_loss: 6.7183 - loss: 8.3575\n",
      "Epoch 42/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.9101 - class_output_loss: 0.4686 - domain_output_accuracy: 0.5998 - domain_output_loss: 6.7326 - loss: 8.3430\n",
      "Epoch 43/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - class_output_accuracy: 0.9118 - class_output_loss: 0.4616 - domain_output_accuracy: 0.6005 - domain_output_loss: 6.7279 - loss: 8.3154\n",
      "Epoch 44/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.9143 - class_output_loss: 0.4480 - domain_output_accuracy: 0.6008 - domain_output_loss: 6.7182 - loss: 8.2769\n",
      "Epoch 45/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - class_output_accuracy: 0.9170 - class_output_loss: 0.4378 - domain_output_accuracy: 0.5993 - domain_output_loss: 6.7299 - loss: 8.2635\n",
      "Epoch 46/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.9194 - class_output_loss: 0.4273 - domain_output_accuracy: 0.6002 - domain_output_loss: 6.7186 - loss: 8.2274\n",
      "Epoch 47/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.9207 - class_output_loss: 0.4187 - domain_output_accuracy: 0.5998 - domain_output_loss: 6.7298 - loss: 8.2163\n",
      "Epoch 48/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.9234 - class_output_loss: 0.4056 - domain_output_accuracy: 0.5993 - domain_output_loss: 6.7276 - loss: 8.1877\n",
      "Epoch 49/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - class_output_accuracy: 0.9249 - class_output_loss: 0.3974 - domain_output_accuracy: 0.5983 - domain_output_loss: 6.7330 - loss: 8.1723\n",
      "Epoch 50/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 9ms/step - class_output_accuracy: 0.9263 - class_output_loss: 0.3896 - domain_output_accuracy: 0.5991 - domain_output_loss: 6.7258 - loss: 8.1449\n",
      "Epoch 51/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 9ms/step - class_output_accuracy: 0.9291 - class_output_loss: 0.3725 - domain_output_accuracy: 0.6003 - domain_output_loss: 6.7233 - loss: 8.1135\n",
      "Epoch 52/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - class_output_accuracy: 0.9313 - class_output_loss: 0.3626 - domain_output_accuracy: 0.5993 - domain_output_loss: 6.7240 - loss: 8.0932\n",
      "Epoch 53/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - class_output_accuracy: 0.9330 - class_output_loss: 0.3509 - domain_output_accuracy: 0.5995 - domain_output_loss: 6.7266 - loss: 8.0733\n",
      "Epoch 54/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - class_output_accuracy: 0.9351 - class_output_loss: 0.3416 - domain_output_accuracy: 0.6007 - domain_output_loss: 6.7192 - loss: 8.0463\n",
      "Epoch 55/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.9378 - class_output_loss: 0.3245 - domain_output_accuracy: 0.6009 - domain_output_loss: 6.7199 - loss: 8.0197\n",
      "Epoch 56/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 9ms/step - class_output_accuracy: 0.9388 - class_output_loss: 0.3158 - domain_output_accuracy: 0.5997 - domain_output_loss: 6.7205 - loss: 8.0016\n",
      "Epoch 57/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 9ms/step - class_output_accuracy: 0.9406 - class_output_loss: 0.3044 - domain_output_accuracy: 0.5992 - domain_output_loss: 6.7222 - loss: 7.9825\n",
      "Epoch 58/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.9431 - class_output_loss: 0.2890 - domain_output_accuracy: 0.6007 - domain_output_loss: 6.7207 - loss: 7.9568\n",
      "Epoch 59/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 9ms/step - class_output_accuracy: 0.9453 - class_output_loss: 0.2794 - domain_output_accuracy: 0.5996 - domain_output_loss: 6.7248 - loss: 7.9426\n",
      "Epoch 60/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 9ms/step - class_output_accuracy: 0.9483 - class_output_loss: 0.2631 - domain_output_accuracy: 0.6003 - domain_output_loss: 6.7216 - loss: 7.9147\n",
      "Epoch 61/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 9ms/step - class_output_accuracy: 0.9490 - class_output_loss: 0.2565 - domain_output_accuracy: 0.6002 - domain_output_loss: 6.7256 - loss: 7.9041\n",
      "Epoch 62/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 9ms/step - class_output_accuracy: 0.9510 - class_output_loss: 0.2447 - domain_output_accuracy: 0.6016 - domain_output_loss: 6.7165 - loss: 7.8753\n",
      "Epoch 63/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - class_output_accuracy: 0.9541 - class_output_loss: 0.2305 - domain_output_accuracy: 0.5997 - domain_output_loss: 6.7233 - loss: 7.8606\n",
      "Epoch 64/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 8ms/step - class_output_accuracy: 0.9554 - class_output_loss: 0.2199 - domain_output_accuracy: 0.5999 - domain_output_loss: 6.7228 - loss: 7.8419\n",
      "Epoch 65/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 9ms/step - class_output_accuracy: 0.9583 - class_output_loss: 0.2081 - domain_output_accuracy: 0.6002 - domain_output_loss: 6.7221 - loss: 7.8223\n",
      "Epoch 66/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.9603 - class_output_loss: 0.1951 - domain_output_accuracy: 0.5987 - domain_output_loss: 6.7279 - loss: 7.8081\n",
      "Epoch 67/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - class_output_accuracy: 0.9616 - class_output_loss: 0.1879 - domain_output_accuracy: 0.6003 - domain_output_loss: 6.7230 - loss: 7.7893\n",
      "Epoch 68/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 9ms/step - class_output_accuracy: 0.9644 - class_output_loss: 0.1756 - domain_output_accuracy: 0.6007 - domain_output_loss: 6.7221 - loss: 7.7694\n",
      "Epoch 69/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - class_output_accuracy: 0.9655 - class_output_loss: 0.1678 - domain_output_accuracy: 0.5998 - domain_output_loss: 6.7212 - loss: 7.7542\n",
      "Epoch 70/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 9ms/step - class_output_accuracy: 0.9679 - class_output_loss: 0.1576 - domain_output_accuracy: 0.5992 - domain_output_loss: 6.7268 - loss: 7.7431\n",
      "Epoch 71/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.9690 - class_output_loss: 0.1501 - domain_output_accuracy: 0.5997 - domain_output_loss: 6.7238 - loss: 7.7262\n",
      "Epoch 72/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.9715 - class_output_loss: 0.1416 - domain_output_accuracy: 0.6005 - domain_output_loss: 6.7236 - loss: 7.7113\n",
      "Epoch 73/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.9726 - class_output_loss: 0.1347 - domain_output_accuracy: 0.6010 - domain_output_loss: 6.7219 - loss: 7.6964\n",
      "Epoch 74/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.9740 - class_output_loss: 0.1275 - domain_output_accuracy: 0.6001 - domain_output_loss: 6.7234 - loss: 7.6845\n",
      "Epoch 75/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 8ms/step - class_output_accuracy: 0.9745 - class_output_loss: 0.1229 - domain_output_accuracy: 0.6006 - domain_output_loss: 6.7204 - loss: 7.6704\n",
      "Epoch 76/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.9760 - class_output_loss: 0.1165 - domain_output_accuracy: 0.5993 - domain_output_loss: 6.7262 - loss: 7.6636\n",
      "Epoch 77/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 9ms/step - class_output_accuracy: 0.9773 - class_output_loss: 0.1110 - domain_output_accuracy: 0.5995 - domain_output_loss: 6.7268 - loss: 7.6525\n",
      "Epoch 78/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.9782 - class_output_loss: 0.1072 - domain_output_accuracy: 0.6005 - domain_output_loss: 6.7225 - loss: 7.6381\n",
      "Epoch 79/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.9798 - class_output_loss: 0.1018 - domain_output_accuracy: 0.6011 - domain_output_loss: 6.7203 - loss: 7.6244\n",
      "Epoch 80/100\n",
      "\u001b[1m4688/4688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 10ms/step - class_output_accuracy: 0.9799 - class_output_loss: 0.0993 - domain_output_accuracy: 0.6003 - domain_output_loss: 6.7214 - loss: 7.6167\n",
      "Epoch 81/100\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 1.0       # Gradient reversal strength\n",
    "\n",
    "model = build_domain_invariant_classifier(latent_dim, feature_extractor, class_classifier, domain_classifier, lambda_)\n",
    "print(model.output_names)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.00001),\n",
    "    loss={\n",
    "        \"class_output\": keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        \"domain_output\": keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    },\n",
    "    metrics={\n",
    "        \"class_output\": \"accuracy\",\n",
    "        \"domain_output\": \"accuracy\"\n",
    "    },\n",
    "    loss_weights={\"class_output\": 1.0, \"domain_output\": 1.0}\n",
    ")    \n",
    "\n",
    "model.fit(x_train, {\"class_output\": y_train, \"domain_output\": d_train}, batch_size=128, epochs=100, shuffle=True)\n",
    "feature_extractor.save(\"../models/classification/latent_web/feature_extractor.keras\")\n",
    "class_classifier.save(\"../models/classification/latent_web/class_classifier.keras\")\n",
    "domain_classifier.save(\"../models/classification/latent_web/domain_classifier.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_classifier = tf.keras.models.load_model(\"../models/classification/latent_web/class_classifier.keras\")\n",
    "feature_extractor = tf.keras.models.load_model(\"../models/classification/latent_web/feature_extractor.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.64783335\n"
     ]
    }
   ],
   "source": [
    "# Get logits from the model\n",
    "logits = class_classifier(feature_extractor(x_test))\n",
    "\n",
    "# Convert logits to predictions\n",
    "predictions = tf.argmax(logits, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = tf.keras.metrics.Accuracy()\n",
    "accuracy.update_state(y_test, predictions)\n",
    "acc_value = accuracy.result().numpy()\n",
    "\n",
    "\n",
    "print(\"Accuracy\", acc_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# see whether sending the test data to the other side of the location hyperplane helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.64325\n"
     ]
    }
   ],
   "source": [
    "w_norm = [ 0.06975281,  0.04118532, -0.14468005, -0.05042024,  0.01549432,\n",
    "       -0.02282231,  0.12847888, -0.01408303,  0.02656271,  0.0355668 ,\n",
    "       -0.05648927, -0.14249759, -0.50905263,  0.01057976, -0.12150489,\n",
    "        0.01641565, -0.06888216, -0.03849784, -0.00694003, -0.00515264,\n",
    "        0.01254265, -0.36203945,  0.01098717, -0.21074273,  0.05474511,\n",
    "        0.00883125,  0.0365044 , -0.04460574,  0.04653245,  0.02774703,\n",
    "        0.05174151, -0.01852731, -0.00529856, -0.16091824,  0.18659344,\n",
    "       -0.04393233,  0.04611056,  0.04562994,  0.02683887, -0.02065428,\n",
    "       -0.00894934, -0.0686677 ,  0.04277077, -0.01343503, -0.0336921 ,\n",
    "        0.01214135, -0.05644768, -0.01702002, -0.04333098,  0.07054286,\n",
    "        0.0393395 , -0.12504448,  0.04985355, -0.06638476, -0.02668105,\n",
    "       -0.0404169 ,  0.0033951 ,  0.10164927,  0.02533227, -0.00456006,\n",
    "       -0.03698264,  0.01445536,  0.07971338, -0.05770196,  0.01214109,\n",
    "       -0.01428591, -0.01922118, -0.05471335,  0.06706536,  0.09856064,\n",
    "       -0.02785856, -0.06674519,  0.02121052, -0.02505254, -0.00223312,\n",
    "       -0.0595909 , -0.07537825, -0.11761025, -0.02598793, -0.05158163,\n",
    "       -0.024724  ,  0.02093302,  0.18926618, -0.00074534,  0.01370208,\n",
    "       -0.02600524, -0.03602418,  0.06587068,  0.41776927, -0.10614276,\n",
    "       -0.03901233,  0.12387   ,  0.07350189, -0.00673966,  0.1447743 ,\n",
    "        0.0108655 ]\n",
    "\n",
    "bias = [1.0903536]\n",
    "\n",
    "def jump_to_other_side(z, w, b, alpha=0.1, mirrored=True):\n",
    "    \"\"\"\n",
    "    Jump directly to the other side of the hyperplane.\n",
    "    \n",
    "    Parameters:\n",
    "    - z: The current latent vector (numpy array).\n",
    "    - w: The normal vector of the hyperplane.\n",
    "    - b: The bias term of the hyperplane.\n",
    "    - alpha: Additional step size to cross to the other side.\n",
    "    \n",
    "    Returns:\n",
    "    - z_new: The updated latent vector on the other side.\n",
    "    \"\"\"\n",
    "    # Compute the current value of the decision function\n",
    "    decision_value = np.dot(w, z) + b\n",
    "    \n",
    "    # Compute the displacement to the hyperplane\n",
    "    delta_z = -decision_value / np.dot(w, w) * w\n",
    "    \n",
    "    if mirrored:\n",
    "        # mirror image of the point\n",
    "        z_new = z + 1 * delta_z\n",
    "    else: \n",
    "        # Add an additional step to cross the hyperplane\n",
    "        z_new = z + delta_z + alpha * w\n",
    "\n",
    "    return z_new\n",
    "\n",
    "x_test_mirror = np.zeros(x_test.shape)\n",
    "for i in range(len(x_test)):\n",
    "    x_test_mirror[i] = jump_to_other_side(x_test[i], w_norm, bias, mirrored=True, alpha = 1.0) \n",
    "    \n",
    "    \n",
    "# Get logits from the model\n",
    "logits = class_classifier(feature_extractor(x_test_mirror))\n",
    "\n",
    "# Convert logits to predictions\n",
    "predictions = tf.argmax(logits, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = tf.keras.metrics.Accuracy()\n",
    "accuracy.update_state(y_test, predictions)\n",
    "acc_value = accuracy.result().numpy()\n",
    "\n",
    "\n",
    "print(\"Accuracy\", acc_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
