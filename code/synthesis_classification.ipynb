{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 13:15:57.541073: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-31 13:15:57.547918: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-31 13:15:57.555994: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-31 13:15:57.558384: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-31 13:15:57.564532: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-31 13:15:57.958474: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1730340958.377095 1812107 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1730340958.377204 1812107 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1730340958.396563 1812107 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1730340958.396675 1812107 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1730340958.396742 1812107 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1730340958.396800 1812107 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n",
      "Loading Dataset...\n",
      "Training Websites: [1309, 228, 51, 563, 501, 457, 285, 209, 1385, 1116, 178, 1209, 864, 65, 61, 191, 447, 476, 1034, 1232, 54, 1149, 407, 1466, 1330, 1436, 1490, 859, 451, 919, 1206, 569, 13, 326, 1429, 865, 696, 1468, 318, 440, 689, 1492, 189, 778, 198, 735, 704, 1236, 541, 88, 940, 1098, 255, 775, 161, 1130, 600, 1287, 1266, 740, 1182, 393, 142, 93, 1354, 466, 592, 163, 1482, 206, 1456, 1462, 928, 1301, 747, 333, 758, 727, 429, 1372, 546, 1399, 1327, 146, 1247, 1300, 350, 1093, 1495, 334, 946, 777, 552, 1310, 1140, 449, 1402, 664, 114, 469, 1486, 646, 821, 548, 135, 432, 1161, 644, 435, 1342, 1022, 810, 1316, 939, 292, 542, 1493, 505, 1478, 1103, 538, 1197, 877, 1195, 817, 741, 1404, 283, 1043, 1010, 186, 96, 224, 313, 1285, 327, 1487, 1221, 130, 788, 781, 1220, 958, 1083, 514, 1133, 23, 234, 1099, 1419, 1312, 1463, 1498, 601, 890, 323, 929, 6, 539, 1025, 365, 1039, 217, 1280, 611, 1308, 1338, 1415, 1477, 1366, 765, 330, 1104, 1086, 1, 1226, 663, 1000, 39, 229, 743, 629, 490, 118, 493, 1393, 1445, 175, 995, 141, 1090, 257, 262, 973, 1125, 338, 1384, 1080, 1242, 866, 433, 1417, 411, 638, 1375, 764, 897, 1059, 924, 247, 507, 460, 131, 692, 43, 1204, 1134, 471, 1205, 1471, 14, 145, 120, 468, 138, 64, 676, 1278, 1052, 487, 570, 994, 438, 1298, 270, 1169, 1180, 968, 497, 1262, 833, 389, 193, 1455, 882, 725, 867, 841, 956, 110, 201, 124, 824, 694, 223, 509, 392, 1258, 1448, 918, 287, 1363, 375, 1269, 947, 511, 154, 907, 1127, 200, 103, 1107, 30, 1484, 484, 340, 832, 1268, 985, 437, 1397, 1277, 337, 776, 4, 799, 543, 931, 584, 1414, 1138, 996, 317, 388, 607, 445, 119, 1186, 1110, 1248, 642, 117, 102, 1196, 976, 1029, 1087, 322, 116, 1040, 164, 380, 140, 139, 481, 826, 245, 1166, 504, 81, 167, 858, 1157, 1070, 647, 534, 418, 643, 488, 1213, 1388, 268, 614, 936, 1175, 148, 19, 938, 1153, 204, 150, 1101, 436, 1036, 1170, 271, 714, 1187, 500, 756, 583, 1344, 1293, 1112, 619, 1356, 16, 1135, 613, 212, 275, 1451, 236, 219, 1435, 1461, 557, 577, 431, 702, 416, 540, 1035, 1322, 1355, 104, 1457, 1253, 566, 90, 7, 683, 267, 536, 1328, 904, 875, 1163, 1320, 1233, 305, 73, 1150, 303, 880, 261, 85, 631, 746, 1263, 732, 430, 1234, 210, 724, 1223, 316, 1225, 332, 362, 844, 50, 367, 680, 843, 508, 1350, 1476, 221, 783, 79, 963, 455, 408, 942, 716, 625, 1434, 456, 48, 395, 816, 672, 1452, 1437, 571, 719, 1371, 818, 678, 56, 1137, 1174, 1339, 1155, 78, 222, 889, 707, 1199, 893, 1047, 1058, 1360, 1426, 521, 1120, 1049, 3, 403, 745, 883, 143, 1273, 1050, 1447, 615, 633, 836, 668, 1332, 605, 260, 1243, 861, 1216, 356, 630, 582, 308, 415, 561, 853, 0, 311, 293, 215, 1460, 804, 593, 621, 670, 329, 1431, 452, 1005, 691, 218, 523, 1092, 812, 922, 982, 815, 753, 173, 674, 86, 290, 527, 679, 648, 634, 343, 95, 838, 974, 769, 240, 688, 1207, 230, 825, 203, 1159, 25, 47, 250, 486, 1073, 870, 786, 74, 1072, 424, 1480, 1392, 589, 199, 1454, 713, 1438, 506, 409, 249, 151, 671, 1453, 5, 914, 768, 881, 1046, 906, 109, 797, 1391, 1367, 180, 823, 712, 530, 475, 1497, 1066, 1481, 868, 1200, 467, 136, 820, 937, 1118, 1055, 572, 609, 324, 773, 912, 453, 627, 834, 736, 913, 516, 1177, 850, 1018, 1071, 162, 761, 1255, 971, 1288, 265, 997, 253, 860, 652, 1420, 784, 796, 533, 496, 641, 244, 281, 450, 1079, 730, 1491, 981, 278, 986, 1364, 553, 82, 1406, 1201, 1048, 1026, 710, 156, 723, 1136, 1418, 965, 417, 1304, 555, 477, 425, 63, 211, 852, 1241, 398, 1235, 598, 1386, 20, 1302, 962, 1045, 1171, 1390, 360, 1109, 771, 399, 1421, 551, 1329, 752, 559, 819, 617, 225, 499, 1075, 279, 446, 1261, 29, 863, 344, 684, 695, 1295, 414, 1374, 169, 478, 1361, 637, 1144, 27, 1190, 606, 1132, 1060, 1449, 1380, 658, 1267, 1275, 472, 1369, 1439, 266, 1469, 335, 216, 465, 1410, 345, 779, 809, 284, 770, 1131, 258, 83, 1185, 1146, 767, 1407, 53, 358, 1111, 665, 70, 667, 41, 772, 31, 903, 1160, 1472, 636, 1377, 894, 129, 1126, 685, 1198, 1343, 1245, 1006, 1074, 1307, 377, 171, 620, 1009, 960, 774, 1179, 1283, 1250, 1433, 26, 319, 857, 693, 384, 406, 1351, 1376, 77, 1219, 1051, 1231, 248, 1124, 959, 1020, 700, 1167, 123, 579, 42, 355, 545, 1208, 677, 379, 862, 518, 1323, 349, 12, 1238, 1411, 108, 443, 370, 650, 470, 803, 1284, 941, 1057, 666, 276, 1389, 848, 495, 851, 984, 749, 274, 1115, 251, 1450, 1383, 461, 955, 1427, 1381, 624, 1259, 802, 1240, 1031, 957, 1091, 999, 1252, 1337, 363, 264, 348, 286, 610, 282, 1428, 10, 529, 195, 87, 1290, 1129, 1151, 568, 246, 1270, 661, 502, 458, 17, 1362, 301, 226, 830, 1444, 1475, 595, 949, 1024, 1121, 926, 352, 943, 1496, 871, 1017, 464, 277, 1345, 1334, 1105, 1440, 197, 1148, 122, 1396, 1123, 196, 1081, 902, 900, 603, 537, 1335, 289, 1378, 1256, 1106, 232, 369, 183, 309, 1279, 1194, 1408, 280, 46, 55, 659, 299, 699, 953, 105, 728, 587, 291, 480, 1317, 1336, 687, 188, 52, 798, 489, 1191, 66, 410, 503, 75, 590, 1479, 155, 152, 576, 1015, 989, 254, 121, 1064, 426, 231, 535, 856, 703, 920, 304, 439, 312, 1485, 101, 1405, 807, 1265, 944, 160, 1183, 177, 565, 76, 574, 2, 1173, 585, 898, 298, 33, 237, 295, 987, 901, 72, 239, 662, 202, 656, 763, 978, 596, 272, 1272, 1108, 580, 828, 314, 921, 1373, 127, 479, 594, 412, 887, 512, 1382, 448, 1038, 40, 442, 748, 256, 1423, 1474, 1352, 21, 1139, 1260, 179, 599, 1004, 801, 185, 878, 1346, 528, 522, 1023, 567, 341, 328, 886, 792, 1021, 717, 168, 1096, 737, 1178, 147, 339, 483, 205, 734, 586, 1042, 18, 1314, 45, 1313, 618, 165, 59, 1069, 1430, 532, 263, 422, 1016, 336, 1063, 651, 988, 1210, 1061, 1368, 905, 519, 909, 387, 934, 320, 800, 837, 681, 1333, 930, 896, 67, 1085, 840, 892, 357, 1158, 62, 626, 1192, 1128, 1251, 1078, 1459, 1100, 159, 698, 1119, 829, 208, 1306, 115, 1422, 58, 1488, 60, 331, 1228, 1054, 1282, 366, 149, 1027, 361, 1202, 578, 427, 1089, 241, 932, 233, 731, 967, 895, 97, 306, 1394, 382, 69, 35, 908, 855, 404, 849, 174, 822, 259, 806, 1325, 144, 371, 744, 300, 296, 1217, 972, 935, 1347, 525, 428, 176, 170, 423, 390, 1379, 1257, 873, 1189, 711, 459, 1044, 1271, 421, 1203, 1473, 22, 910, 242, 1214, 1326, 1398, 726, 1424, 750, 517, 639, 1274, 649, 302, 970, 811, 842, 364, 269, 697, 1483, 1172, 1458, 808, 891, 38, 888, 1395, 1222, 757, 751, 755, 524, 1246, 1011, 273, 194, 378, 721, 1403, 612, 1318, 1412, 1019, 1218, 645, 462, 604, 622, 1053, 1088, 923, 1499, 227, 831, 153, 911, 1353, 166, 28, 975, 628, 1324, 220, 660, 125, 1154, 1188, 560, 92, 1370, 89, 1147, 1237, 1165, 759, 564, 791, 1387, 1012]\n",
      "Training Locations: ['LOC1', 'LOC2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asil0892/doh_traffic_analysis/code/scripts/init_dataset.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df.sort_values(by=[\"Location\"], inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Website</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOC1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.086861</td>\n",
       "      <td>0.690199</td>\n",
       "      <td>0.647933</td>\n",
       "      <td>0.201517</td>\n",
       "      <td>0.207139</td>\n",
       "      <td>-0.031855</td>\n",
       "      <td>-0.316768</td>\n",
       "      <td>0.824649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055571</td>\n",
       "      <td>0.160203</td>\n",
       "      <td>0.121677</td>\n",
       "      <td>0.093533</td>\n",
       "      <td>0.09274</td>\n",
       "      <td>0.057062</td>\n",
       "      <td>0.155991</td>\n",
       "      <td>0.109839</td>\n",
       "      <td>0.086282</td>\n",
       "      <td>0.08604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LOC1</td>\n",
       "      <td>1005</td>\n",
       "      <td>1.095547</td>\n",
       "      <td>-1.585943</td>\n",
       "      <td>0.647933</td>\n",
       "      <td>0.343653</td>\n",
       "      <td>0.207139</td>\n",
       "      <td>-0.031855</td>\n",
       "      <td>-0.316768</td>\n",
       "      <td>0.824649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055571</td>\n",
       "      <td>0.160203</td>\n",
       "      <td>0.121677</td>\n",
       "      <td>0.093533</td>\n",
       "      <td>0.09274</td>\n",
       "      <td>0.057062</td>\n",
       "      <td>0.155991</td>\n",
       "      <td>0.109839</td>\n",
       "      <td>0.086282</td>\n",
       "      <td>0.08604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOC1</td>\n",
       "      <td>1005</td>\n",
       "      <td>1.095547</td>\n",
       "      <td>0.780047</td>\n",
       "      <td>-1.972790</td>\n",
       "      <td>0.343653</td>\n",
       "      <td>0.207139</td>\n",
       "      <td>-0.031855</td>\n",
       "      <td>-0.316768</td>\n",
       "      <td>0.824649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055571</td>\n",
       "      <td>0.160203</td>\n",
       "      <td>0.121677</td>\n",
       "      <td>0.093533</td>\n",
       "      <td>0.09274</td>\n",
       "      <td>0.057062</td>\n",
       "      <td>0.155991</td>\n",
       "      <td>0.109839</td>\n",
       "      <td>0.086282</td>\n",
       "      <td>0.08604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LOC1</td>\n",
       "      <td>1005</td>\n",
       "      <td>-1.086861</td>\n",
       "      <td>0.690199</td>\n",
       "      <td>0.647933</td>\n",
       "      <td>0.343653</td>\n",
       "      <td>0.207139</td>\n",
       "      <td>-0.031855</td>\n",
       "      <td>-0.316768</td>\n",
       "      <td>0.824649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055571</td>\n",
       "      <td>0.160203</td>\n",
       "      <td>0.121677</td>\n",
       "      <td>0.093533</td>\n",
       "      <td>0.09274</td>\n",
       "      <td>0.057062</td>\n",
       "      <td>0.155991</td>\n",
       "      <td>0.109839</td>\n",
       "      <td>0.086282</td>\n",
       "      <td>0.08604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LOC1</td>\n",
       "      <td>1005</td>\n",
       "      <td>1.095547</td>\n",
       "      <td>0.780047</td>\n",
       "      <td>0.183501</td>\n",
       "      <td>-0.828965</td>\n",
       "      <td>-2.083179</td>\n",
       "      <td>-0.031855</td>\n",
       "      <td>-0.316768</td>\n",
       "      <td>0.824649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055571</td>\n",
       "      <td>0.160203</td>\n",
       "      <td>0.121677</td>\n",
       "      <td>0.093533</td>\n",
       "      <td>0.09274</td>\n",
       "      <td>0.057062</td>\n",
       "      <td>0.155991</td>\n",
       "      <td>0.109839</td>\n",
       "      <td>0.086282</td>\n",
       "      <td>0.08604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Location  Website         0         1         2         3         4  \\\n",
       "0     LOC1        0 -1.086861  0.690199  0.647933  0.201517  0.207139   \n",
       "1     LOC1     1005  1.095547 -1.585943  0.647933  0.343653  0.207139   \n",
       "2     LOC1     1005  1.095547  0.780047 -1.972790  0.343653  0.207139   \n",
       "3     LOC1     1005 -1.086861  0.690199  0.647933  0.343653  0.207139   \n",
       "4     LOC1     1005  1.095547  0.780047  0.183501 -0.828965 -2.083179   \n",
       "\n",
       "          5         6         7  ...       116       117       118       119  \\\n",
       "0 -0.031855 -0.316768  0.824649  ...  0.055571  0.160203  0.121677  0.093533   \n",
       "1 -0.031855 -0.316768  0.824649  ...  0.055571  0.160203  0.121677  0.093533   \n",
       "2 -0.031855 -0.316768  0.824649  ...  0.055571  0.160203  0.121677  0.093533   \n",
       "3 -0.031855 -0.316768  0.824649  ...  0.055571  0.160203  0.121677  0.093533   \n",
       "4 -0.031855 -0.316768  0.824649  ...  0.055571  0.160203  0.121677  0.093533   \n",
       "\n",
       "       120       121       122       123       124      125  \n",
       "0  0.09274  0.057062  0.155991  0.109839  0.086282  0.08604  \n",
       "1  0.09274  0.057062  0.155991  0.109839  0.086282  0.08604  \n",
       "2  0.09274  0.057062  0.155991  0.109839  0.086282  0.08604  \n",
       "3  0.09274  0.057062  0.155991  0.109839  0.086282  0.08604  \n",
       "4  0.09274  0.057062  0.155991  0.109839  0.086282  0.08604  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scripts.init_gpu as init_gpu\n",
    "import scripts.init_dataset as init_dataset\n",
    "from scripts.triplet_functions import n_neurons\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "init_gpu.initialize_gpus()\n",
    "\n",
    "locations = ['LOC1', 'LOC2']\n",
    "\n",
    "print(\"Loading Dataset...\")\n",
    "# load the dataset\n",
    "df = pd.read_csv(\n",
    "    f\"../dataset/processed/{locations[0]}-{locations[1]}-scaled-balanced.csv\")\n",
    "\n",
    "length = len(df.columns) - 2  # subtract the two label columns\n",
    "\n",
    "# get train-test set\n",
    "train_df, test_df, train_web_samples, test_web_samples = init_dataset.get_sample(\n",
    "    df, locations, range(1500), 1200)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Columns: 128 entries, Location to 125\n",
      "dtypes: float64(126), int64(1), object(1)\n",
      "memory usage: 58.6+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1812107/3678497788.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target_df.sort_values(by=['Website'], inplace=True)\n",
      "/tmp/ipykernel_1812107/3678497788.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_location_df.sort_values(by=['Website'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# dataset for the classification\n",
    "target_location = 'LOC2'\n",
    "target_df = test_df[test_df['Location'] == target_location]\n",
    "target_df.sort_values(by=['Website'], inplace=True)\n",
    "target_df.reset_index(drop=True, inplace=True)\n",
    "target_df.head(20)\n",
    "\n",
    "\n",
    "train_location = 'LOC1'\n",
    "train_location_df = test_df[test_df['Location'] == train_location]\n",
    "train_location_df.sort_values(by=['Website'], inplace=True)\n",
    "train_location_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_location_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Website</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOC2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.203406</td>\n",
       "      <td>-0.314179</td>\n",
       "      <td>-0.139752</td>\n",
       "      <td>-0.071425</td>\n",
       "      <td>0.108881</td>\n",
       "      <td>-0.011975</td>\n",
       "      <td>0.151551</td>\n",
       "      <td>-0.222142</td>\n",
       "      <td>...</td>\n",
       "      <td>0.356577</td>\n",
       "      <td>-0.299774</td>\n",
       "      <td>-0.202030</td>\n",
       "      <td>0.014276</td>\n",
       "      <td>-0.422333</td>\n",
       "      <td>0.004943</td>\n",
       "      <td>-1.432474</td>\n",
       "      <td>-0.630595</td>\n",
       "      <td>-0.062904</td>\n",
       "      <td>0.319991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LOC2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.185225</td>\n",
       "      <td>-0.414302</td>\n",
       "      <td>-0.310309</td>\n",
       "      <td>-0.049250</td>\n",
       "      <td>0.248417</td>\n",
       "      <td>-0.017623</td>\n",
       "      <td>0.285865</td>\n",
       "      <td>-0.147767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124226</td>\n",
       "      <td>-0.364065</td>\n",
       "      <td>-0.324481</td>\n",
       "      <td>-0.268335</td>\n",
       "      <td>-0.265297</td>\n",
       "      <td>-0.017370</td>\n",
       "      <td>-0.572126</td>\n",
       "      <td>-0.489263</td>\n",
       "      <td>-0.414731</td>\n",
       "      <td>-0.254916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOC2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.300867</td>\n",
       "      <td>-0.333989</td>\n",
       "      <td>0.771254</td>\n",
       "      <td>-0.062574</td>\n",
       "      <td>-0.007563</td>\n",
       "      <td>-0.403584</td>\n",
       "      <td>-0.441415</td>\n",
       "      <td>-1.381110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705736</td>\n",
       "      <td>-3.779596</td>\n",
       "      <td>-0.267297</td>\n",
       "      <td>1.004949</td>\n",
       "      <td>1.396240</td>\n",
       "      <td>1.055891</td>\n",
       "      <td>-1.651864</td>\n",
       "      <td>0.324592</td>\n",
       "      <td>0.963504</td>\n",
       "      <td>-0.139990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LOC2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.123885</td>\n",
       "      <td>-0.662992</td>\n",
       "      <td>-0.892483</td>\n",
       "      <td>0.313389</td>\n",
       "      <td>0.683722</td>\n",
       "      <td>0.751752</td>\n",
       "      <td>0.881606</td>\n",
       "      <td>0.337842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.961339</td>\n",
       "      <td>0.112217</td>\n",
       "      <td>0.086410</td>\n",
       "      <td>-0.366637</td>\n",
       "      <td>-0.160450</td>\n",
       "      <td>-1.389893</td>\n",
       "      <td>-0.549960</td>\n",
       "      <td>-3.518910</td>\n",
       "      <td>-1.244516</td>\n",
       "      <td>-3.602888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LOC2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.506402</td>\n",
       "      <td>-0.445047</td>\n",
       "      <td>0.409191</td>\n",
       "      <td>-0.145644</td>\n",
       "      <td>0.030367</td>\n",
       "      <td>-0.303240</td>\n",
       "      <td>-0.255780</td>\n",
       "      <td>-1.473365</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.440858</td>\n",
       "      <td>-1.647118</td>\n",
       "      <td>0.016812</td>\n",
       "      <td>0.537111</td>\n",
       "      <td>0.290844</td>\n",
       "      <td>-0.359081</td>\n",
       "      <td>-0.963809</td>\n",
       "      <td>-1.706611</td>\n",
       "      <td>0.358788</td>\n",
       "      <td>-1.795628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LOC2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.529472</td>\n",
       "      <td>0.093815</td>\n",
       "      <td>0.332313</td>\n",
       "      <td>-0.293104</td>\n",
       "      <td>-0.472844</td>\n",
       "      <td>-0.203073</td>\n",
       "      <td>-0.341054</td>\n",
       "      <td>-1.357381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403145</td>\n",
       "      <td>-1.430642</td>\n",
       "      <td>-0.038243</td>\n",
       "      <td>0.299889</td>\n",
       "      <td>0.363038</td>\n",
       "      <td>0.185339</td>\n",
       "      <td>-0.732481</td>\n",
       "      <td>-0.812661</td>\n",
       "      <td>-0.044492</td>\n",
       "      <td>-1.392823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LOC2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.493913</td>\n",
       "      <td>-0.444168</td>\n",
       "      <td>0.121023</td>\n",
       "      <td>-0.146445</td>\n",
       "      <td>-0.016119</td>\n",
       "      <td>-0.116675</td>\n",
       "      <td>-0.001641</td>\n",
       "      <td>-0.752897</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011327</td>\n",
       "      <td>-0.785555</td>\n",
       "      <td>-0.789380</td>\n",
       "      <td>-0.492514</td>\n",
       "      <td>-0.723175</td>\n",
       "      <td>-0.082961</td>\n",
       "      <td>-1.035463</td>\n",
       "      <td>-0.854735</td>\n",
       "      <td>-0.391557</td>\n",
       "      <td>-0.296285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LOC2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.440434</td>\n",
       "      <td>-0.364613</td>\n",
       "      <td>-0.107900</td>\n",
       "      <td>-0.028960</td>\n",
       "      <td>-0.046870</td>\n",
       "      <td>-0.120512</td>\n",
       "      <td>0.139025</td>\n",
       "      <td>-0.584830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.354648</td>\n",
       "      <td>-0.733600</td>\n",
       "      <td>-0.210071</td>\n",
       "      <td>-0.051970</td>\n",
       "      <td>-0.100523</td>\n",
       "      <td>0.193880</td>\n",
       "      <td>-0.966895</td>\n",
       "      <td>-0.788626</td>\n",
       "      <td>-0.518967</td>\n",
       "      <td>-0.250285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LOC2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.176741</td>\n",
       "      <td>-0.400024</td>\n",
       "      <td>-0.189658</td>\n",
       "      <td>-0.064648</td>\n",
       "      <td>0.302245</td>\n",
       "      <td>-0.028929</td>\n",
       "      <td>0.222665</td>\n",
       "      <td>-0.469539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178721</td>\n",
       "      <td>-0.241689</td>\n",
       "      <td>-0.424518</td>\n",
       "      <td>-0.205740</td>\n",
       "      <td>-0.092397</td>\n",
       "      <td>0.133416</td>\n",
       "      <td>-0.541934</td>\n",
       "      <td>-0.610627</td>\n",
       "      <td>-0.351969</td>\n",
       "      <td>-0.218352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LOC2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.314194</td>\n",
       "      <td>-0.363953</td>\n",
       "      <td>-0.147444</td>\n",
       "      <td>-0.077837</td>\n",
       "      <td>0.072941</td>\n",
       "      <td>-0.075567</td>\n",
       "      <td>0.175021</td>\n",
       "      <td>-0.445487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266730</td>\n",
       "      <td>-0.662978</td>\n",
       "      <td>-0.435572</td>\n",
       "      <td>-0.241315</td>\n",
       "      <td>-0.211038</td>\n",
       "      <td>0.138796</td>\n",
       "      <td>-1.011317</td>\n",
       "      <td>-0.861465</td>\n",
       "      <td>-0.590561</td>\n",
       "      <td>-0.278451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LOC2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.260802</td>\n",
       "      <td>-0.392947</td>\n",
       "      <td>-0.248641</td>\n",
       "      <td>-0.091340</td>\n",
       "      <td>0.123045</td>\n",
       "      <td>-0.030130</td>\n",
       "      <td>0.278937</td>\n",
       "      <td>-0.176903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198264</td>\n",
       "      <td>-0.346919</td>\n",
       "      <td>-0.434779</td>\n",
       "      <td>-0.257976</td>\n",
       "      <td>-0.276354</td>\n",
       "      <td>-0.008547</td>\n",
       "      <td>-0.692448</td>\n",
       "      <td>-0.749910</td>\n",
       "      <td>-0.496515</td>\n",
       "      <td>-0.332216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LOC2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.193357</td>\n",
       "      <td>-0.474475</td>\n",
       "      <td>-0.264213</td>\n",
       "      <td>-0.025069</td>\n",
       "      <td>0.282937</td>\n",
       "      <td>0.023990</td>\n",
       "      <td>0.290673</td>\n",
       "      <td>-0.393464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.520197</td>\n",
       "      <td>-0.645753</td>\n",
       "      <td>-0.210104</td>\n",
       "      <td>0.104621</td>\n",
       "      <td>-0.703175</td>\n",
       "      <td>-0.021386</td>\n",
       "      <td>-2.317884</td>\n",
       "      <td>-0.844581</td>\n",
       "      <td>0.185255</td>\n",
       "      <td>0.713528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LOC2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.118165</td>\n",
       "      <td>-0.418229</td>\n",
       "      <td>-0.355158</td>\n",
       "      <td>-0.100645</td>\n",
       "      <td>0.279650</td>\n",
       "      <td>-0.000472</td>\n",
       "      <td>0.376931</td>\n",
       "      <td>-0.084637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106294</td>\n",
       "      <td>-0.342580</td>\n",
       "      <td>-0.335370</td>\n",
       "      <td>-0.217121</td>\n",
       "      <td>-0.204574</td>\n",
       "      <td>0.011681</td>\n",
       "      <td>-0.518129</td>\n",
       "      <td>-0.444882</td>\n",
       "      <td>-0.327061</td>\n",
       "      <td>-0.195964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LOC2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.314696</td>\n",
       "      <td>-0.499131</td>\n",
       "      <td>0.037571</td>\n",
       "      <td>0.077881</td>\n",
       "      <td>0.180270</td>\n",
       "      <td>-0.053381</td>\n",
       "      <td>0.021469</td>\n",
       "      <td>-0.952389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.790124</td>\n",
       "      <td>-1.188272</td>\n",
       "      <td>-0.225539</td>\n",
       "      <td>0.161851</td>\n",
       "      <td>-1.106138</td>\n",
       "      <td>0.082311</td>\n",
       "      <td>-3.452233</td>\n",
       "      <td>-0.955846</td>\n",
       "      <td>0.249829</td>\n",
       "      <td>1.041492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LOC2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.025527</td>\n",
       "      <td>-0.868657</td>\n",
       "      <td>-1.068934</td>\n",
       "      <td>0.703031</td>\n",
       "      <td>0.958689</td>\n",
       "      <td>1.046255</td>\n",
       "      <td>0.995383</td>\n",
       "      <td>0.357889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996866</td>\n",
       "      <td>-0.859872</td>\n",
       "      <td>-0.041704</td>\n",
       "      <td>-0.708824</td>\n",
       "      <td>-0.472412</td>\n",
       "      <td>0.624639</td>\n",
       "      <td>-1.982057</td>\n",
       "      <td>-0.827397</td>\n",
       "      <td>-1.045935</td>\n",
       "      <td>-0.731846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LOC2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.212709</td>\n",
       "      <td>-0.452808</td>\n",
       "      <td>-0.351267</td>\n",
       "      <td>-0.113829</td>\n",
       "      <td>0.225561</td>\n",
       "      <td>-0.009885</td>\n",
       "      <td>0.350517</td>\n",
       "      <td>-0.096562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169777</td>\n",
       "      <td>-0.091143</td>\n",
       "      <td>-0.295942</td>\n",
       "      <td>-0.246096</td>\n",
       "      <td>-0.192190</td>\n",
       "      <td>0.016165</td>\n",
       "      <td>-0.425080</td>\n",
       "      <td>-0.514375</td>\n",
       "      <td>-0.358499</td>\n",
       "      <td>-0.193963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LOC2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.188081</td>\n",
       "      <td>-0.409292</td>\n",
       "      <td>-0.478670</td>\n",
       "      <td>-0.125058</td>\n",
       "      <td>0.184265</td>\n",
       "      <td>0.113993</td>\n",
       "      <td>0.477646</td>\n",
       "      <td>0.279926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085395</td>\n",
       "      <td>-0.248768</td>\n",
       "      <td>-0.528833</td>\n",
       "      <td>-0.373178</td>\n",
       "      <td>-0.387865</td>\n",
       "      <td>-0.017396</td>\n",
       "      <td>-0.754230</td>\n",
       "      <td>-0.793972</td>\n",
       "      <td>-0.570614</td>\n",
       "      <td>-0.263517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LOC2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.270951</td>\n",
       "      <td>-0.274978</td>\n",
       "      <td>-0.200361</td>\n",
       "      <td>-0.104458</td>\n",
       "      <td>0.093380</td>\n",
       "      <td>0.017817</td>\n",
       "      <td>0.182080</td>\n",
       "      <td>-0.280462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.928316</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>-0.003498</td>\n",
       "      <td>-0.316841</td>\n",
       "      <td>-0.133901</td>\n",
       "      <td>-1.051424</td>\n",
       "      <td>-0.749520</td>\n",
       "      <td>-3.161317</td>\n",
       "      <td>-1.168354</td>\n",
       "      <td>-3.157675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LOC2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.095650</td>\n",
       "      <td>-0.318984</td>\n",
       "      <td>-0.321311</td>\n",
       "      <td>-0.063161</td>\n",
       "      <td>0.187254</td>\n",
       "      <td>0.027046</td>\n",
       "      <td>0.326356</td>\n",
       "      <td>0.106163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076199</td>\n",
       "      <td>-0.394517</td>\n",
       "      <td>-0.285613</td>\n",
       "      <td>-0.260574</td>\n",
       "      <td>-0.233388</td>\n",
       "      <td>-0.010927</td>\n",
       "      <td>-0.511434</td>\n",
       "      <td>-0.337164</td>\n",
       "      <td>-0.373034</td>\n",
       "      <td>-0.138999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LOC2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.433586</td>\n",
       "      <td>-0.346299</td>\n",
       "      <td>0.204555</td>\n",
       "      <td>-0.214958</td>\n",
       "      <td>-0.006033</td>\n",
       "      <td>-0.173001</td>\n",
       "      <td>-0.116465</td>\n",
       "      <td>-0.999568</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.364537</td>\n",
       "      <td>-1.480948</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.634123</td>\n",
       "      <td>0.340501</td>\n",
       "      <td>-0.479688</td>\n",
       "      <td>-0.995529</td>\n",
       "      <td>-2.125422</td>\n",
       "      <td>0.180541</td>\n",
       "      <td>-2.166861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Location  Website         0         1         2         3         4  \\\n",
       "0      LOC2        8  0.203406 -0.314179 -0.139752 -0.071425  0.108881   \n",
       "1      LOC2        8  0.185225 -0.414302 -0.310309 -0.049250  0.248417   \n",
       "2      LOC2        8  0.300867 -0.333989  0.771254 -0.062574 -0.007563   \n",
       "3      LOC2        8  0.123885 -0.662992 -0.892483  0.313389  0.683722   \n",
       "4      LOC2        8  0.506402 -0.445047  0.409191 -0.145644  0.030367   \n",
       "5      LOC2        8  0.529472  0.093815  0.332313 -0.293104 -0.472844   \n",
       "6      LOC2        8  0.493913 -0.444168  0.121023 -0.146445 -0.016119   \n",
       "7      LOC2        8  0.440434 -0.364613 -0.107900 -0.028960 -0.046870   \n",
       "8      LOC2        8  0.176741 -0.400024 -0.189658 -0.064648  0.302245   \n",
       "9      LOC2        8  0.314194 -0.363953 -0.147444 -0.077837  0.072941   \n",
       "10     LOC2        8  0.260802 -0.392947 -0.248641 -0.091340  0.123045   \n",
       "11     LOC2        8  0.193357 -0.474475 -0.264213 -0.025069  0.282937   \n",
       "12     LOC2        8  0.118165 -0.418229 -0.355158 -0.100645  0.279650   \n",
       "13     LOC2        8  0.314696 -0.499131  0.037571  0.077881  0.180270   \n",
       "14     LOC2        8  0.025527 -0.868657 -1.068934  0.703031  0.958689   \n",
       "15     LOC2        8  0.212709 -0.452808 -0.351267 -0.113829  0.225561   \n",
       "16     LOC2        8  0.188081 -0.409292 -0.478670 -0.125058  0.184265   \n",
       "17     LOC2        8  0.270951 -0.274978 -0.200361 -0.104458  0.093380   \n",
       "18     LOC2        8  0.095650 -0.318984 -0.321311 -0.063161  0.187254   \n",
       "19     LOC2        8  0.433586 -0.346299  0.204555 -0.214958 -0.006033   \n",
       "\n",
       "           5         6         7  ...       116       117       118       119  \\\n",
       "0  -0.011975  0.151551 -0.222142  ...  0.356577 -0.299774 -0.202030  0.014276   \n",
       "1  -0.017623  0.285865 -0.147767  ...  0.124226 -0.364065 -0.324481 -0.268335   \n",
       "2  -0.403584 -0.441415 -1.381110  ...  0.705736 -3.779596 -0.267297  1.004949   \n",
       "3   0.751752  0.881606  0.337842  ...  0.961339  0.112217  0.086410 -0.366637   \n",
       "4  -0.303240 -0.255780 -1.473365  ... -0.440858 -1.647118  0.016812  0.537111   \n",
       "5  -0.203073 -0.341054 -1.357381  ...  0.403145 -1.430642 -0.038243  0.299889   \n",
       "6  -0.116675 -0.001641 -0.752897  ...  0.011327 -0.785555 -0.789380 -0.492514   \n",
       "7  -0.120512  0.139025 -0.584830  ...  0.354648 -0.733600 -0.210071 -0.051970   \n",
       "8  -0.028929  0.222665 -0.469539  ...  0.178721 -0.241689 -0.424518 -0.205740   \n",
       "9  -0.075567  0.175021 -0.445487  ...  0.266730 -0.662978 -0.435572 -0.241315   \n",
       "10 -0.030130  0.278937 -0.176903  ...  0.198264 -0.346919 -0.434779 -0.257976   \n",
       "11  0.023990  0.290673 -0.393464  ...  0.520197 -0.645753 -0.210104  0.104621   \n",
       "12 -0.000472  0.376931 -0.084637  ...  0.106294 -0.342580 -0.335370 -0.217121   \n",
       "13 -0.053381  0.021469 -0.952389  ...  0.790124 -1.188272 -0.225539  0.161851   \n",
       "14  1.046255  0.995383  0.357889  ...  0.996866 -0.859872 -0.041704 -0.708824   \n",
       "15 -0.009885  0.350517 -0.096562  ...  0.169777 -0.091143 -0.295942 -0.246096   \n",
       "16  0.113993  0.477646  0.279926  ...  0.085395 -0.248768 -0.528833 -0.373178   \n",
       "17  0.017817  0.182080 -0.280462  ...  0.928316  0.001919 -0.003498 -0.316841   \n",
       "18  0.027046  0.326356  0.106163  ...  0.076199 -0.394517 -0.285613 -0.260574   \n",
       "19 -0.173001 -0.116465 -0.999568  ... -0.364537 -1.480948  0.000010  0.634123   \n",
       "\n",
       "         120       121       122       123       124       125  \n",
       "0  -0.422333  0.004943 -1.432474 -0.630595 -0.062904  0.319991  \n",
       "1  -0.265297 -0.017370 -0.572126 -0.489263 -0.414731 -0.254916  \n",
       "2   1.396240  1.055891 -1.651864  0.324592  0.963504 -0.139990  \n",
       "3  -0.160450 -1.389893 -0.549960 -3.518910 -1.244516 -3.602888  \n",
       "4   0.290844 -0.359081 -0.963809 -1.706611  0.358788 -1.795628  \n",
       "5   0.363038  0.185339 -0.732481 -0.812661 -0.044492 -1.392823  \n",
       "6  -0.723175 -0.082961 -1.035463 -0.854735 -0.391557 -0.296285  \n",
       "7  -0.100523  0.193880 -0.966895 -0.788626 -0.518967 -0.250285  \n",
       "8  -0.092397  0.133416 -0.541934 -0.610627 -0.351969 -0.218352  \n",
       "9  -0.211038  0.138796 -1.011317 -0.861465 -0.590561 -0.278451  \n",
       "10 -0.276354 -0.008547 -0.692448 -0.749910 -0.496515 -0.332216  \n",
       "11 -0.703175 -0.021386 -2.317884 -0.844581  0.185255  0.713528  \n",
       "12 -0.204574  0.011681 -0.518129 -0.444882 -0.327061 -0.195964  \n",
       "13 -1.106138  0.082311 -3.452233 -0.955846  0.249829  1.041492  \n",
       "14 -0.472412  0.624639 -1.982057 -0.827397 -1.045935 -0.731846  \n",
       "15 -0.192190  0.016165 -0.425080 -0.514375 -0.358499 -0.193963  \n",
       "16 -0.387865 -0.017396 -0.754230 -0.793972 -0.570614 -0.263517  \n",
       "17 -0.133901 -1.051424 -0.749520 -3.161317 -1.168354 -3.157675  \n",
       "18 -0.233388 -0.010927 -0.511434 -0.337164 -0.373034 -0.138999  \n",
       "19  0.340501 -0.479688 -0.995529 -2.125422  0.180541 -2.166861  \n",
       "\n",
       "[20 rows x 128 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_df = pd.read_csv('../synthesized/sampling-LOC1-target-LOC2.csv').iloc[:, 1:]\n",
    "synthetic_df = synthetic_df[synthetic_df['Location'] == target_location]\n",
    "synthetic_df.sort_values(by=['Website'], inplace=True)\n",
    "synthetic_df.reset_index(drop=True, inplace=True)\n",
    "synthetic_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available websites: [394, 951, 885, 591, 1117, 991, 187, 1013, 1416, 705, 874, 1315, 510, 347, 876, 588, 785, 884, 992, 213, 325, 126, 1425, 562, 1095, 581, 1097, 1400, 1102, 1348, 106, 558, 794, 99, 872, 307, 373, 554, 762, 1296, 814, 597, 182, 128, 927, 1114, 444, 1065, 653, 1446, 238, 1176, 556, 846, 623, 760, 945, 353, 184, 420, 1264, 847, 359, 441, 782, 288, 789, 419, 520, 780, 961, 482, 297, 111, 11, 979, 112, 813, 434, 948, 137, 933, 515, 383, 1077, 754, 1413, 1193, 544, 372, 835, 787, 385, 550, 473, 733, 602, 158, 181, 491, 1162, 573, 1442, 1003, 315, 133, 243, 1286, 37, 1041, 1084, 1358, 1294, 1037, 397, 36, 381, 190, 795, 950, 1229, 391, 1331, 690, 310, 346, 214, 1441, 1164, 494, 44, 1141, 1281, 1401, 1244, 983, 15, 91, 675, 1289, 531, 374, 84, 635, 1113, 1076, 616, 1143, 845, 964, 1341, 720, 1305, 1094, 49, 669, 1249, 1008, 1230, 654, 34, 1297, 526, 1211, 8, 706, 547, 640, 917, 682, 1319, 1349, 827, 80, 1028, 1227, 575, 351, 1409, 915, 998, 722, 1311, 405, 1299, 805, 952, 966, 1067, 1212, 632, 207, 107, 738, 498, 402, 157, 1489, 1465, 68, 673, 715, 1033, 990, 1152, 401, 342, 854, 1056, 413, 1254, 132, 742, 766, 701, 1359, 1068, 1062, 549, 1292, 729, 368, 474, 100, 94, 879, 1432, 235, 1303, 321, 709, 869, 1032, 608, 386, 1082, 1291, 57, 1030, 1239, 793, 485, 294, 252, 739, 718, 98, 1494, 134, 708]\n",
      "Missing websites: [513, 899, 1156, 9, 396, 655, 1168, 657, 400, 916, 790, 24, 1181, 925, 32, 1184, 1443, 1321, 172, 686, 1464, 954, 1467, 1340, 1470, 1215, 192, 454, 71, 1224, 839, 969, 1357, 463, 977, 980, 1365, 993, 1122, 354, 1001, 1002, 492, 1007, 113, 1142, 1014, 376, 1145, 1276]\n",
      "Trained partially on the synthesized data: \n",
      "Accuracy: 75.22, F1 Score:  71.70, Precision:  70.22, Recall:  75.22\n",
      "Trained partially on the data from another location: \n",
      "Accuracy: 82.78, F1 Score:  84.06, Precision:  88.72, Recall:  82.78\n",
      "Available websites: [1095, 952, 847, 401, 992, 37, 391, 1003, 158, 789, 1432, 948, 635, 1067, 494, 84, 294, 581, 951, 1164, 36, 207, 383, 1494, 964, 961, 1152, 1340, 1348, 754, 1358, 359, 1315, 554, 1311, 49, 794, 588, 1357, 243, 722, 1145, 1305, 575, 1141, 397, 653, 128, 1441, 793, 1321, 795, 1184, 1465, 1286, 307, 405, 1007, 1443, 1264, 492, 491, 854, 134, 869, 1068, 1467, 813, 876, 835, 1008, 1244, 550, 790, 396, 182, 213, 1168, 8, 1401, 917, 510, 1113, 1162, 1013, 1229, 1102, 1114, 1076, 1084, 547, 632, 342, 1062, 1014, 954, 739, 482, 1409, 1033, 1442, 137, 372, 1294, 927, 925, 916, 872, 1001, 884, 297, 1296, 68, 315, 701, 15, 686, 657, 1489, 608, 190, 1292, 434, 1212, 94, 1249, 98, 376, 709, 879, 1176, 760, 827, 1215, 402, 669, 1230, 733, 181, 325, 80, 1291, 874, 1122, 1041, 1227, 157, 235, 1470, 1239, 485, 386, 1425, 785, 112, 192, 214, 57, 1211, 899, 91, 966, 1446, 520, 690, 780, 675, 1299, 1289, 814, 1181, 473, 1002, 44, 1030, 573, 708, 1037, 1297, 655, 133, 1341, 729, 34, 544, 1400, 1077, 787, 381, 738, 420, 346, 915, 654, 845, 1416, 126, 513, 526, 1028, 945, 1065, 979, 113, 1331, 998, 107, 374, 531, 394, 558, 1082, 24, 805, 187, 597, 977, 288, 1117, 1032, 385, 100, 990, 111, 933, 616, 1142, 1156, 705, 238, 885, 1193, 71, 252, 498, 1254, 706, 846, 419, 1319, 99, 562, 673, 591, 172, 782, 515, 1056, 106, 400]\n",
      "Missing websites: [640, 1281, 132, 1413, 9, 11, 1303, 413, 32, 549, 682, 556, 310, 950, 184, 441, 1464, 444, 321, 1349, 1094, 839, 1224, 1097, 454, 715, 969, 718, 1359, 720, 463, 980, 1365, 983, 602, 347, 474, 351, 991, 353, 354, 993, 742, 623, 368, 373, 1143, 762, 1276, 766]\n",
      "Trained partially on the synthesized data: \n",
      "Accuracy: 74.62, F1 Score:  70.85, Precision:  69.40, Recall:  74.62\n",
      "Trained partially on the data from another location: \n",
      "Accuracy: 82.28, F1 Score:  83.48, Precision:  88.61, Recall:  82.28\n",
      "Available websites: [1358, 325, 722, 1032, 1331, 562, 1319, 1227, 597, 1305, 111, 549, 814, 473, 1315, 653, 402, 925, 657, 112, 1068, 397, 34, 766, 682, 1001, 1193, 37, 1297, 1294, 444, 346, 1425, 1013, 1494, 669, 760, 1008, 1056, 795, 742, 100, 321, 381, 347, 1348, 701, 1413, 413, 1181, 137, 1097, 720, 297, 1441, 1176, 401, 434, 1239, 1002, 351, 463, 1340, 182, 36, 187, 1291, 872, 1095, 952, 632, 132, 554, 400, 133, 441, 1443, 126, 1212, 396, 310, 184, 68, 494, 190, 993, 1400, 526, 992, 654, 782, 1211, 113, 1299, 854, 531, 547, 1311, 353, 979, 990, 214, 1014, 482, 573, 11, 1077, 780, 847, 1446, 498, 485, 544, 608, 790, 1359, 1168, 420, 243, 835, 885, 1084, 1357, 192, 933, 157, 372, 964, 454, 1076, 733, 315, 675, 640, 623, 951, 556, 520, 1442, 991, 917, 673, 1264, 558, 107, 106, 879, 1113, 1142, 718, 80, 1067, 1007, 1467, 373, 1094, 927, 1489, 1152, 1164, 32, 739, 1143, 1145, 846, 754, 376, 213, 1114, 252, 1276, 491, 874, 793, 789, 1030, 1416, 354, 1281, 969, 94, 977, 715, 391, 1401, 359, 15, 998, 1321, 591, 655, 729, 98, 1349, 84, 1464, 686, 1224, 1033, 705, 383, 1409, 207, 510, 708, 1162, 813, 1244, 1184, 706, 235, 948, 550, 884, 602, 1122, 845, 24, 794, 288, 1082, 915, 386, 575, 44, 1432, 368, 839, 876, 950, 1062, 1230, 419, 128, 1289, 134, 307, 57, 785, 99, 1465, 827, 374, 869, 1003, 1215, 980, 1141, 1028, 515]\n",
      "Missing websites: [513, 385, 899, 1156, 1286, 8, 9, 394, 1292, 1037, 1296, 1041, 787, 916, 405, 1303, 158, 805, 294, 1065, 172, 49, 690, 945, 181, 954, 1341, 1470, 961, 581, 709, 71, 966, 588, 1229, 1102, 1365, 342, 983, 474, 91, 1117, 1249, 738, 1254, 616, 492, 238, 762, 635]\n",
      "Trained partially on the synthesized data: \n",
      "Accuracy: 74.33, F1 Score:  70.86, Precision:  69.54, Recall:  74.33\n",
      "Trained partially on the data from another location: \n",
      "Accuracy: 83.31, F1 Score:  84.76, Precision:  89.53, Recall:  83.31\n",
      "Available websites: [575, 550, 1008, 733, 181, 1003, 879, 24, 945, 473, 252, 1291, 1441, 718, 354, 1286, 394, 57, 558, 640, 385, 876, 998, 402, 172, 805, 1142, 785, 1299, 1319, 347, 106, 835, 444, 1113, 969, 1227, 915, 99, 1249, 391, 207, 1141, 1102, 1062, 1143, 1425, 1181, 1400, 782, 1013, 100, 927, 933, 952, 573, 869, 133, 916, 762, 315, 474, 588, 846, 413, 720, 214, 708, 397, 491, 126, 1416, 9, 1014, 1184, 722, 706, 98, 602, 1276, 654, 1037, 884, 950, 951, 993, 793, 288, 709, 977, 738, 1028, 1065, 187, 562, 107, 1340, 657, 401, 682, 854, 15, 1193, 1041, 243, 1264, 948, 1168, 235, 872, 34, 1443, 383, 192, 1254, 623, 1296, 1489, 1292, 990, 1068, 1056, 925, 157, 137, 238, 1359, 1077, 591, 1001, 213, 1357, 1230, 526, 1358, 1033, 1076, 1156, 1224, 1239, 705, 1082, 780, 396, 1465, 1446, 1152, 510, 297, 1145, 294, 1341, 795, 441, 485, 754, 766, 1297, 8, 1176, 134, 44, 690, 1413, 520, 1331, 454, 1117, 847, 715, 581, 669, 991, 686, 1162, 1067, 325, 1348, 1244, 653, 531, 1321, 1432, 91, 400, 597, 547, 84, 1164, 554, 701, 1122, 673, 760, 608, 827, 1315, 11, 376, 544, 964, 353, 917, 368, 742, 1401, 729, 980, 346, 549, 463, 111, 1002, 112, 71, 132, 49, 359, 979, 1215, 1229, 420, 494, 1294, 739, 845, 983, 321, 954, 655, 1289, 1094, 966, 405, 1007, 1349, 1311, 184, 675, 839, 80, 789, 961, 381, 1114, 787, 899, 885, 632, 158]\n",
      "Missing websites: [128, 513, 1281, 515, 386, 1409, 1030, 1032, 790, 1303, 1305, 794, 32, 1442, 419, 36, 37, 556, 813, 814, 434, 307, 182, 310, 1464, 1211, 1084, 1212, 190, 1467, 1470, 68, 1095, 1097, 1365, 342, 1494, 94, 351, 992, 482, 616, 874, 492, 113, 498, 372, 373, 374, 635]\n",
      "Trained partially on the synthesized data: \n",
      "Accuracy: 74.96, F1 Score:  71.37, Precision:  69.74, Recall:  74.96\n",
      "Trained partially on the data from another location: \n",
      "Accuracy: 82.78, F1 Score:  84.04, Precision:  88.82, Recall:  82.78\n",
      "Available websites: [214, 925, 184, 682, 979, 739, 998, 742, 113, 602, 715, 1229, 1062, 969, 192, 708, 554, 80, 874, 795, 917, 991, 492, 1028, 640, 869, 1432, 413, 1193, 762, 789, 760, 558, 71, 1215, 547, 653, 879, 654, 34, 346, 1446, 157, 252, 1465, 980, 1145, 243, 1291, 1065, 402, 1156, 1141, 705, 454, 1003, 1244, 207, 899, 405, 562, 374, 722, 993, 112, 1102, 354, 44, 134, 575, 1341, 706, 790, 616, 310, 1097, 1030, 321, 137, 854, 632, 1443, 288, 839, 397, 1094, 1292, 1299, 498, 1013, 510, 133, 549, 485, 1311, 1056, 383, 1076, 1002, 132, 100, 515, 106, 1181, 608, 1212, 1349, 111, 1416, 635, 1276, 827, 964, 945, 1357, 793, 1168, 297, 846, 24, 1305, 961, 401, 126, 573, 990, 1230, 128, 1321, 876, 376, 1176, 782, 91, 1303, 916, 342, 1084, 927, 780, 950, 690, 1254, 709, 738, 491, 1359, 1286, 36, 977, 845, 733, 385, 434, 754, 394, 814, 315, 419, 983, 785, 1082, 68, 1489, 729, 190, 556, 84, 718, 550, 835, 531, 1297, 32, 386, 597, 1117, 1441, 11, 623, 94, 1400, 805, 787, 966, 1340, 591, 1068, 172, 513, 1296, 420, 1143, 1033, 9, 1113, 368, 444, 1032, 400, 1224, 675, 307, 8, 1077, 1239, 213, 544, 1365, 655, 1001, 952, 588, 1114, 1162, 1442, 526, 1184, 482, 1409, 1264, 396, 373, 766, 107, 1425, 884, 187, 235, 1007, 1331, 1211, 325, 948, 1067, 1227, 1008, 1294, 1152, 1037, 954, 473, 1095, 474, 673, 1464, 238, 463, 992, 720]\n",
      "Missing websites: [1281, 1413, 391, 520, 1289, 1164, 15, 1041, 657, 915, 794, 669, 158, 1315, 37, 294, 1319, 933, 813, 686, 49, 181, 182, 951, 57, 441, 1467, 701, 1470, 1348, 581, 1358, 847, 1494, 347, 351, 1249, 98, 99, 1122, 353, 359, 872, 494, 372, 885, 1142, 1014, 1401, 381]\n",
      "Trained partially on the synthesized data: \n",
      "Accuracy: 73.88, F1 Score:  70.08, Precision:  68.30, Recall:  73.88\n",
      "Trained partially on the data from another location: \n",
      "Accuracy: 82.90, F1 Score:  84.46, Precision:  89.14, Recall:  82.90\n",
      "Available websites: [1032, 1227, 686, 562, 80, 544, 1409, 1056, 718, 549, 351, 383, 708, 980, 1211, 44, 623, 950, 252, 998, 1299, 789, 1224, 1291, 1142, 1470, 190, 874, 1289, 1254, 1067, 653, 884, 1446, 1003, 1319, 846, 473, 879, 359, 1145, 969, 1349, 1176, 1425, 98, 374, 1114, 952, 376, 307, 1077, 353, 1215, 397, 531, 1264, 1013, 690, 238, 733, 498, 673, 99, 616, 84, 134, 790, 36, 192, 373, 948, 550, 738, 827, 876, 588, 1041, 111, 158, 288, 485, 961, 526, 34, 381, 1002, 845, 766, 385, 1311, 1432, 602, 715, 597, 1156, 635, 1030, 1113, 368, 835, 558, 325, 839, 1441, 401, 1321, 1489, 814, 925, 675, 945, 235, 132, 1141, 669, 474, 1249, 1303, 655, 126, 106, 1168, 214, 720, 310, 1062, 709, 1152, 1348, 494, 640, 434, 15, 754, 91, 57, 8, 847, 1341, 1276, 990, 917, 632, 1084, 591, 396, 1117, 315, 243, 1001, 294, 966, 137, 400, 492, 394, 706, 581, 654, 785, 1076, 869, 1097, 24, 1244, 510, 547, 1037, 515, 793, 739, 94, 729, 762, 1239, 213, 1065, 1122, 1286, 1331, 11, 128, 441, 347, 813, 321, 951, 1296, 1028, 513, 1400, 113, 1305, 1014, 187, 107, 297, 1230, 182, 1442, 172, 1494, 964, 760, 354, 491, 1401, 1465, 682, 1229, 1358, 1467, 405, 386, 1068, 71, 915, 991, 1413, 805, 1008, 420, 872, 454, 993, 1082, 794, 787, 419, 1212, 1365, 1184, 608, 927, 722, 37, 9, 1292, 342, 444, 575, 705, 100, 402, 1297, 520, 49, 1181, 413]\n",
      "Missing websites: [1281, 899, 133, 391, 1416, 1033, 1162, 1164, 780, 782, 1294, 657, 916, 795, 157, 32, 1315, 1443, 933, 1193, 554, 556, 181, 184, 1464, 954, 1340, 573, 701, 68, 1094, 1095, 1357, 1102, 207, 1359, 463, 977, 979, 854, 983, 346, 992, 482, 742, 1007, 112, 372, 885, 1143]\n",
      "Trained partially on the synthesized data: \n",
      "Accuracy: 74.08, F1 Score:  70.29, Precision:  68.49, Recall:  74.08\n",
      "Trained partially on the data from another location: \n",
      "Accuracy: 82.44, F1 Score:  83.76, Precision:  88.94, Recall:  82.44\n",
      "Available websites: [182, 491, 1432, 739, 550, 1082, 1264, 983, 1001, 32, 655, 1297, 1032, 690, 1142, 346, 1084, 1289, 133, 1227, 1359, 980, 795, 847, 623, 573, 948, 297, 1065, 591, 381, 1215, 1007, 805, 993, 793, 24, 742, 391, 933, 705, 1331, 1349, 1400, 157, 1470, 1276, 1494, 669, 37, 1357, 1294, 632, 347, 100, 951, 34, 420, 111, 992, 441, 413, 1365, 787, 405, 835, 673, 1152, 1168, 969, 1008, 515, 126, 190, 1416, 845, 112, 554, 49, 549, 1097, 187, 1212, 238, 373, 214, 172, 1401, 520, 885, 654, 235, 1413, 1286, 991, 402, 990, 1181, 1030, 1143, 686, 396, 1425, 827, 494, 99, 760, 729, 581, 294, 916, 325, 1076, 917, 635, 1340, 207, 57, 1014, 68, 1211, 113, 181, 1145, 474, 547, 385, 1122, 925, 785, 310, 353, 782, 966, 706, 954, 562, 1003, 1409, 675, 1156, 1442, 397, 134, 701, 1296, 884, 794, 1299, 1319, 132, 80, 715, 640, 71, 1305, 394, 927, 718, 383, 1467, 158, 979, 1184, 657, 556, 252, 1095, 874, 964, 597, 780, 372, 485, 1164, 998, 879, 510, 754, 1224, 84, 137, 1311, 544, 846, 961, 558, 1068, 1292, 945, 1002, 1113, 709, 588, 1033, 1114, 766, 98, 720, 950, 315, 288, 869, 722, 454, 8, 1239, 653, 1358, 682, 192, 526, 1041, 1067, 813, 243, 419, 1446, 899, 9, 1141, 1249, 814, 602, 1162, 492, 1037, 977, 44, 616, 106, 738, 1348, 376, 1303, 94, 351, 872, 463, 386, 368, 1013, 876, 839, 11, 498, 733, 473, 1254, 790]\n",
      "Missing websites: [128, 513, 1281, 1028, 1291, 15, 400, 401, 531, 915, 789, 1176, 1056, 1441, 1315, 36, 1443, 1062, 1193, 1321, 434, 307, 1077, 184, 952, 1464, 1465, 444, 1341, 575, 321, 708, 1094, 1229, 1102, 1230, 1489, 213, 342, 854, 91, 1244, 1117, 608, 354, 482, 359, 107, 374, 762]\n",
      "Trained partially on the synthesized data: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained partially on the synthesized data: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m accuracy, precision, recall, f1_score, cm \u001b[38;5;241m=\u001b[39m \u001b[43mclassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_classification_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# classification.show_confusion_matrix_heatmap(cm, le, f\"Synthetic Data: missing classes: {missing_websites}\")\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# using real data from X\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Prepare the data\u001b[39;00m\n\u001b[1;32m     57\u001b[0m X_train, y_train, X_test, y_test, le \u001b[38;5;241m=\u001b[39m prepare_data(target_df, train_location_df, available_websites, missing_websites)\n",
      "File \u001b[0;32m~/doh_traffic_analysis/code/scripts/classification.py:10\u001b[0m, in \u001b[0;36mevaluate_classification_model\u001b[0;34m(X_train, y_train, X_test, y_test, model)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_classification_model\u001b[39m(X_train, y_train, X_test, y_test, model) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, np\u001b[38;5;241m.\u001b[39marray]:\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mscore(X_test, y_test)\n\u001b[1;32m     12\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/doh_traffic_analysis/.venv/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/doh_traffic_analysis/.venv/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    481\u001b[0m ]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/doh_traffic_analysis/.venv/lib/python3.10/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/doh_traffic_analysis/.venv/lib/python3.10/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/doh_traffic_analysis/.venv/lib/python3.10/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/doh_traffic_analysis/.venv/lib/python3.10/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/doh_traffic_analysis/.venv/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    201\u001b[0m         X,\n\u001b[1;32m    202\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m~/doh_traffic_analysis/.venv/lib/python3.10/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scripts.classification as classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import random\n",
    "\n",
    "def prepare_data(target_df, synthetic_df, available_websites, missing_websites, test_size=0.2, random_state=42):\n",
    "    # Split the target data into initial train and test sets\n",
    "    target_train, target_test = train_test_split(target_df, test_size=test_size, random_state=random_state, stratify=target_df['Website'])\n",
    "    \n",
    "    # Filter target data for available websites (for training)\n",
    "    target_train_data = target_train[target_train['Website'].isin(available_websites)]\n",
    "    \n",
    "    # Get synthetic data for missing websites (for training)\n",
    "    synthetic_train_data = synthetic_df[synthetic_df['Website'].isin(missing_websites)]\n",
    "    \n",
    "    # Combine real and synthetic data for training\n",
    "    train_data = pd.concat([target_train_data, synthetic_train_data], axis=0)\n",
    "    \n",
    "    # Prepare features and labels for training\n",
    "    X_train = train_data.iloc[:, 2:]  # All columns except 'Location' and 'Website'\n",
    "    y_train = train_data['Website']\n",
    "    \n",
    "    # Prepare test data (only real data from the target test set)\n",
    "    X_test = target_test.iloc[:, 2:]\n",
    "    y_test = target_test['Website']\n",
    "    \n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y_train_encoded = le.fit_transform(y_train)\n",
    "    y_test_encoded = le.transform(y_test)\n",
    "    \n",
    "    return X_train, y_train_encoded, X_test, y_test_encoded, le\n",
    "\n",
    "for i in range(10):\n",
    "    available_websites = random.sample(test_web_samples, 250)\n",
    "    missing_websites = list(set(test_web_samples) - set(available_websites))\n",
    "\n",
    "    print(\"Available websites:\", available_websites)\n",
    "    print(\"Missing websites:\", missing_websites)\n",
    "\n",
    "    # using synthetic\n",
    "    # Prepare the data\n",
    "    X_train, y_train, X_test, y_test, le = prepare_data(target_df, synthetic_df, available_websites, missing_websites)\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    model = RandomForestClassifier()\n",
    "    print(\"Trained partially on the synthesized data: \")\n",
    "    accuracy, precision, recall, f1_score, cm = classification.evaluate_classification_model(X_train, y_train, X_test, y_test, model)\n",
    "    # classification.show_confusion_matrix_heatmap(cm, le, f\"Synthetic Data: missing classes: {missing_websites}\")\n",
    "    \n",
    "    # using real data from X\n",
    "    # Prepare the data\n",
    "    X_train, y_train, X_test, y_test, le = prepare_data(target_df, train_location_df, available_websites, missing_websites)\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    model = RandomForestClassifier()\n",
    "    print(\"Trained partially on the data from another location: \")\n",
    "    accuracy, precision, recall, f1_score, confusion_matrix = classification.evaluate_classification_model(X_train, y_train, X_test, y_test, model)\n",
    "    # classification.show_confusion_matrix_heatmap(cm, le, f\"From LOC1: missing classes: {missing_websites}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handpicked best 50 synthesized according to the Euclidean Distance Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available websites: [1028, 1030, 8, 9, 1032, 11, 1033, 1037, 15, 1041, 24, 32, 1056, 34, 36, 37, 1062, 1065, 1067, 44, 1068, 49, 1076, 1077, 57, 1082, 1084, 68, 1094, 71, 1097, 1102, 80, 84, 1113, 91, 1117, 94, 98, 99, 100, 1122, 106, 107, 111, 112, 113, 1141, 1142, 1145, 126, 128, 1152, 132, 133, 134, 1156, 137, 1162, 1164, 1176, 157, 158, 1181, 1184, 1193, 172, 181, 182, 184, 187, 1211, 1212, 190, 1215, 192, 1224, 1227, 1229, 1230, 207, 213, 214, 1239, 1244, 1249, 1254, 235, 238, 1264, 243, 252, 1276, 1281, 1286, 1289, 1291, 1294, 1296, 1297, 1299, 1303, 1305, 1311, 1315, 294, 1319, 297, 1321, 307, 1331, 310, 315, 1340, 1341, 321, 1348, 325, 1349, 1357, 1358, 1359, 342, 346, 347, 351, 353, 354, 359, 368, 372, 373, 374, 376, 1400, 1401, 381, 383, 385, 386, 1409, 1413, 391, 1416, 394, 396, 397, 400, 401, 402, 1425, 405, 413, 1441, 1442, 419, 420, 1443, 1446, 434, 441, 1465, 1467, 444, 1470, 454, 463, 1489, 1494, 473, 474, 482, 485, 491, 492, 494, 498, 510, 513, 515, 520, 526, 531, 544, 547, 549, 550, 554, 556, 558, 562, 573, 575, 581, 588, 591, 597, 602, 608, 616, 623, 632, 635, 640, 653, 654, 655, 657, 669, 673, 675, 682, 686, 690, 701, 705, 706, 708, 709, 715, 718, 720, 722, 729, 733, 738, 739, 742, 754, 760, 762, 766, 780, 782, 785, 787, 789, 790, 793, 794, 795, 805, 813, 814, 827, 835, 839, 845, 846, 847, 854, 869, 872, 874, 876, 879, 884, 885, 899, 915, 916, 917, 925, 927, 933, 945, 948, 950, 951, 952, 954, 961, 964, 966, 969, 977, 979, 980, 983, 990, 991, 993, 998, 1001, 1002, 1003, 1007, 1008, 1013, 1014]\n",
      "Missing websites: [992, 1168, 1432, 1464, 288, 1143, 1365, 1095, 1114, 1292]\n",
      "Trained partially on the synthesized data: \n",
      "Accuracy: 86.16, F1 Score:  87.55, Precision:  89.92, Recall:  86.16\n",
      "Trained partially on the data from another location: \n",
      "Accuracy: 87.88, F1 Score:  90.33, Precision:  93.48, Recall:  87.88\n"
     ]
    }
   ],
   "source": [
    "missing_websites = [992, 1168, 1432, 1464, 288, 1143, 1365, 1095, 1114, 1292]\n",
    "available_websites = list(set(test_web_samples) - set(missing_websites))\n",
    "\n",
    "print(\"Available websites:\", available_websites)\n",
    "print(\"Missing websites:\", missing_websites)\n",
    "\n",
    "# using synthetic\n",
    "# Prepare the data\n",
    "X_train, y_train, X_test, y_test, le = prepare_data(\n",
    "    target_df, synthetic_df, available_websites, missing_websites)\n",
    "\n",
    "# Train and evaluate the model\n",
    "model = RandomForestClassifier()\n",
    "print(\"Trained partially on the synthesized data: \")\n",
    "accuracy, precision, recall, f1_score, cm = classification.evaluate_classification_model(\n",
    "    X_train, y_train, X_test, y_test, model)\n",
    "# classification.show_confusion_matrix_heatmap(cm, le, f\"Synthetic Data: missing classes: {missing_websites}\")\n",
    "\n",
    "# using real data from X\n",
    "# Prepare the data\n",
    "X_train, y_train, X_test, y_test, le = prepare_data(\n",
    "    target_df, train_location_df, available_websites, missing_websites)\n",
    "\n",
    "# Train and evaluate the model\n",
    "model = RandomForestClassifier()\n",
    "print(\"Trained partially on the data from another location: \")\n",
    "accuracy, precision, recall, f1_score, confusion_matrix = classification.evaluate_classification_model(\n",
    "    X_train, y_train, X_test, y_test, model)\n",
    "# classification.show_confusion_matrix_heatmap(cm, le, f\"From LOC1: missing classes: {missing_websites}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Synthetic, Test on Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_for_web_classification(train_location_df, target_location_df, train_location, test_location):\n",
    "    le = LabelEncoder()\n",
    "    X_train = train_location_df[train_location_df['Location'] == train_location].drop(\n",
    "        ['Location', 'Website'], axis=1)\n",
    "    X_test = target_location_df[target_location_df['Location'] == test_location].drop(\n",
    "        ['Location', 'Website'], axis=1)\n",
    "    \n",
    "    y_train = train_location_df[train_location_df['Location'] == train_location]['Website']\n",
    "    y_test = target_location_df[target_location_df['Location'] == test_location]['Website']\n",
    "\n",
    "    y_test = le.fit_transform(y_test)\n",
    "    y_train = le.fit_transform(y_train)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, le\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 11.76, F1 Score:  8.87, Precision:  11.01, Recall:  11.76\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import scripts.classification as classification\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "X_train, y_train, X_test, y_test, le = preprocess_data_for_web_classification(synthetic_df, target_df, 'LOC2', 'LOC2')\n",
    "accuracy, precision, recall, f1_score, cm = classification.evaluate_classification_model(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00551038, 0.00865423, 0.00499711, 0.00930905, 0.00657184,\n",
       "       0.00582409, 0.00491639, 0.00445065, 0.00935057, 0.00445735,\n",
       "       0.00599664, 0.01181599, 0.01009915, 0.00987499, 0.00678659,\n",
       "       0.00784897, 0.01096159, 0.0110471 , 0.01190982, 0.0083488 ,\n",
       "       0.00934716, 0.01057891, 0.01077117, 0.01087088, 0.00809168,\n",
       "       0.00940118, 0.01053337, 0.01014823, 0.0098367 , 0.00756151,\n",
       "       0.00939466, 0.00902372, 0.0096759 , 0.00971414, 0.00686099,\n",
       "       0.00902069, 0.00925922, 0.0091321 , 0.00925508, 0.0060042 ,\n",
       "       0.00922879, 0.00883642, 0.00873171, 0.00921564, 0.00595471,\n",
       "       0.00871605, 0.00949108, 0.00927673, 0.00763584, 0.00633038,\n",
       "       0.00829786, 0.00863335, 0.00873818, 0.00842418, 0.00566738,\n",
       "       0.00906406, 0.00794059, 0.0088508 , 0.00759698, 0.0056253 ,\n",
       "       0.00730147, 0.00795783, 0.00725209, 0.00767712, 0.00515663,\n",
       "       0.00846798, 0.00887356, 0.00750193, 0.00831806, 0.00588214,\n",
       "       0.00823078, 0.00964374, 0.00782754, 0.00814804, 0.00582354,\n",
       "       0.0065054 , 0.00862982, 0.00603134, 0.00735013, 0.00570132,\n",
       "       0.00925641, 0.00770682, 0.00739323, 0.00770152, 0.00565784,\n",
       "       0.00812132, 0.00841453, 0.00714387, 0.00716799, 0.00684014,\n",
       "       0.00665847, 0.00857032, 0.00656238, 0.00747011, 0.00671006,\n",
       "       0.00911702, 0.00811987, 0.00731467, 0.00745658, 0.00684052,\n",
       "       0.00759739, 0.00816898, 0.00695531, 0.00720066, 0.00742335,\n",
       "       0.00730196, 0.00775889, 0.00723026, 0.00782974, 0.0064841 ,\n",
       "       0.00764416, 0.00810519, 0.00722743, 0.00742434, 0.00644235,\n",
       "       0.00765935, 0.0078748 , 0.00734825, 0.00879323, 0.00626123,\n",
       "       0.00837392, 0.00861986, 0.00869581, 0.00742984, 0.0068987 ,\n",
       "       0.00727802])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 35.  33.  16. 137.   0.   0.   0.  41.   4.   4. 104.  49.   0.   7.\n",
      "   7.   0.  19.  17.   0.   0.  41. 127.  88.   0.   0.   0.   2.  24.\n",
      "   2.   3.   8.   0.   0.  47.   1.  14.   0.   2.   0.   0.   0.   0.\n",
      "   0.   6.   7.  42.   0.   0.   0.   0. 111.   0.   0.  21.   0.   0.\n",
      "  54.   0.   3.  42.   0.   2.   0.  11.  23.   0.  67.   0.   0.   1.\n",
      "   0.   0.   0.   0.  62.  56.  19.   0.   0.   4.  15.   0.  61.   0.\n",
      "   0.   0.  12.  99.   4. 159.  65.   0.   2.   0.  51.   2.   5.  17.\n",
      "  61.  88.   4. 131.  14.   5.   0. 118.   0.   2.   0.   0.  28.   0.\n",
      "  71.   0. 142.   0. 124.   0.   9.   0.   0.  36.   0.  18.  77.   0.\n",
      "   0.   0.  29.  53.  50.  31.   0.  12.   0.   2. 145.  61.  35.   0.\n",
      "   1.   2.   0.  56.  10.  84.   0. 113.  83. 182.   0. 118.   0.  52.\n",
      "   0. 183.   2.   4.   1.  66.  50.   0.  39.   0.   0.  28. 101.   0.\n",
      "   0.   8.   0.  54.  24.  38.  69.   0.   0.   3. 105.   0.  81.   0.\n",
      "   1.   0.  83.   0.   3.  94.   0.   0.  46.   0. 125.  29.  62.   7.\n",
      "   0.   0. 138.   2.   0.   0.   1.   0.   0.   0. 137.   0.   0.  65.\n",
      "   0.   0.   0.   0.   2.  66.  17.  38.   0.  25.  12.   0.  62.   0.\n",
      "   0.   1.   0.   8.   1.   0.  77.   0.   0.   0.   0.   0.   0.  58.\n",
      " 100.   7.  51.   0. 108.   0.   0. 129.   0.   1.   0.   0.   0.   0.\n",
      "  10.   0.   0.   1.  54.   4.  89.   8.  35.   0.   3.   0.   0.   0.\n",
      "   0. 114.   0.   0.   0.   0.  52.   0.   7.  14.  68.   0. 103.   0.\n",
      "   0.   0.   0. 105.   6.   0.   0.   0.  98.   1.   0.   0.  10.   0.\n",
      "   0.  69.   0.   0.   0. 142.]\n"
     ]
    }
   ],
   "source": [
    "true_pos_synths = np.zeros(shape=(len(test_web_samples)))\n",
    "for i in range(len(test_web_samples)):\n",
    "    true_pos_synths[i] = cm[i][i]\n",
    "    \n",
    "print(true_pos_synths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train of another Location, test of Target location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.81, F1 Score:  65.51, Precision:  72.42, Recall:  63.81\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "X_train, y_train, X_test, y_test, le = preprocess_data_for_web_classification(train_location_df, target_df, 'LOC1', 'LOC2')\n",
    "accuracy, precision, recall, f1_score, confusion_matrix = classification.evaluate_classification_model(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(model.feature_importances_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.5\n",
      "0.785\n",
      "0.695\n",
      "0.78\n",
      "0.66\n",
      "0.79\n",
      "0.83\n",
      "0.825\n",
      "0.82\n",
      "0.81\n",
      "0.825\n",
      "0.8\n",
      "0.795\n",
      "0.815\n"
     ]
    }
   ],
   "source": [
    "step = 10\n",
    "# Loop through each step and set elements to zero\n",
    "print(\"0.815\")\n",
    "for i in range(0, X_train.shape[1], step):\n",
    "    _, _, X_test, y_test, le = preprocess_data_for_web_classification(train_location_df[train_location_df['Website'] == 8], target_df[target_df['Website'] == 8], 'LOC1', 'LOC2')\n",
    "    # print(X_test.shape, y_test.shape)\n",
    "    X_test_copy = X_test.copy()\n",
    "    X_test_copy.iloc[:, i:i + step] = 0\n",
    "    accuracy = model.score(X_test_copy, y_test)\n",
    "    print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[154. 145. 136. 175.  73. 146. 162. 139. 139. 141. 179. 111. 139.  40.\n",
      "  12. 134. 146. 103. 153. 100.  87. 121. 126. 144. 178. 167.  55. 174.\n",
      " 117.  83. 126.  50.  77. 124. 134. 141.  95. 118. 143. 130.  27.  75.\n",
      " 114. 162. 106. 115.  99. 185. 121. 126. 156. 108. 122.  90. 175. 115.\n",
      " 152. 175. 173. 183. 153.  98. 117.  92. 155. 153. 148.   0. 154. 130.\n",
      " 186. 145. 185.   4. 125. 159.  37.   4. 146.  92. 181. 159. 159. 134.\n",
      " 126. 130. 125.  77. 175. 169.  96.  99.  96. 107. 139.  72. 139. 118.\n",
      "  54. 124. 176. 188. 140. 103. 137. 132. 134.  87. 135. 126. 169. 185.\n",
      " 173. 137. 175. 141. 139. 146. 116. 149. 126. 169. 112. 100. 138. 149.\n",
      " 150.  91. 152. 175. 116. 142. 146. 144.  62. 129. 181. 187. 181. 168.\n",
      " 131. 155. 178.  99. 166. 143. 119. 142.  28. 130.   0. 144. 129. 150.\n",
      " 181. 175.  98. 168. 105. 158. 110. 145. 141. 122. 110. 163. 171.  92.\n",
      " 142. 164. 110. 147. 154. 103. 123. 171. 116. 141. 182. 110. 126. 138.\n",
      " 179. 142. 117. 138. 113. 171. 143. 177. 144. 149. 180. 135. 127. 118.\n",
      "  53. 127. 140. 139. 132. 148. 143. 130. 151. 174. 120. 183. 183. 179.\n",
      " 188. 137.   0. 118. 132. 119. 147. 109. 130. 149. 112. 141. 116. 140.\n",
      "   0. 172. 107. 103.  53. 116. 131. 174. 144. 113. 145. 101.  32. 109.\n",
      " 130. 133. 150. 170. 171.  77. 133. 135. 155. 183. 111. 132. 167. 144.\n",
      " 119. 150.   0. 131. 122. 107.  37. 136.  89. 144.  22.  49. 152. 116.\n",
      "  80. 129.  74. 118. 116. 114. 187. 146. 123. 175. 115. 151. 134. 145.\n",
      " 110. 120.  81. 153. 173.  89. 113. 142. 113. 111. 131. 146. 117. 165.\n",
      "  44. 123. 121. 127.  98. 117.]\n"
     ]
    }
   ],
   "source": [
    "true_pos_dif_location = np.zeros(shape=(len(test_web_samples)))\n",
    "for i in range(len(test_web_samples)):\n",
    "    true_pos_dif_location[i] = confusion_matrix[i][i]\n",
    "    \n",
    "print(true_pos_dif_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False,  True,  True, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False,  True])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_pos_synths > true_pos_dif_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on both synthetic and other location's data and test on the actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_df = pd.concat((synthetic_df, train_location_df), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_df.sort_values(by=['Website'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mixed_df.iloc[:, 2:]\n",
    "y_train = mixed_df.Website\n",
    "X_test = target_df.iloc[:, 2:]\n",
    "y_test = target_df.Website\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_test = le.fit_transform(y_test)\n",
    "y_train = le.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.15, F1 Score:  60.53, Precision:  65.77, Recall:  60.15\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "accuracy, precision, recall, f1_score, confusion_matrix = classification.evaluate_classification_model(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location Classification using Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The location classification had an high accuracy with just a XGBoostClassifier model. If our synthetic data is good, it should also be easily distinguishable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_df = pd.concat((pd.read_csv('../synthesized/sampling-LOC1-target-LOC2.csv').iloc[:, 1:], \n",
    "                         pd.read_csv('../synthesized/sampling-LOC2-target-LOC1.csv').iloc[:, 1:] \n",
    "                          ), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Websites: [1309, 228, 51, 563, 501, 457, 285, 209, 1385, 1116, 178, 1209, 864, 65, 61, 191, 447, 476, 1034, 1232, 54, 1149, 407, 1466, 1330, 1436, 1490, 859, 451, 919, 1206, 569, 13, 326, 1429, 865, 696, 1468, 318, 440, 689, 1492, 189, 778, 198, 735, 704, 1236, 541, 88, 940, 1098, 255, 775, 161, 1130, 600, 1287, 1266, 740, 1182, 393, 142, 93, 1354, 466, 592, 163, 1482, 206, 1456, 1462, 928, 1301, 747, 333, 758, 727, 429, 1372, 546, 1399, 1327, 146, 1247, 1300, 350, 1093, 1495, 334, 946, 777, 552, 1310, 1140, 449, 1402, 664, 114, 469, 1486, 646, 821, 548, 135, 432, 1161, 644, 435, 1342, 1022, 810, 1316, 939, 292, 542, 1493, 505, 1478, 1103, 538, 1197, 877, 1195, 817, 741, 1404, 283, 1043, 1010, 186, 96, 224, 313, 1285, 327, 1487, 1221, 130, 788, 781, 1220, 958, 1083, 514, 1133, 23, 234, 1099, 1419, 1312, 1463, 1498, 601, 890, 323, 929, 6, 539, 1025, 365, 1039, 217, 1280, 611, 1308, 1338, 1415, 1477, 1366, 765, 330, 1104, 1086, 1, 1226, 663, 1000, 39, 229, 743, 629, 490, 118, 493, 1393, 1445, 175, 995, 141, 1090, 257, 262, 973, 1125, 338, 1384, 1080, 1242, 866, 433, 1417, 411, 638, 1375, 764, 897, 1059, 924, 247, 507, 460, 131, 692, 43, 1204, 1134, 471, 1205, 1471, 14, 145, 120, 468, 138, 64, 676, 1278, 1052, 487, 570, 994, 438, 1298, 270, 1169, 1180, 968, 497, 1262, 833, 389, 193, 1455, 882, 725, 867, 841, 956, 110, 201, 124, 824, 694, 223, 509, 392, 1258, 1448, 918, 287, 1363, 375, 1269, 947, 511, 154, 907, 1127, 200, 103, 1107, 30, 1484, 484, 340, 832, 1268, 985, 437, 1397, 1277, 337, 776, 4, 799, 543, 931, 584, 1414, 1138, 996, 317, 388, 607, 445, 119, 1186, 1110, 1248, 642, 117, 102, 1196, 976, 1029, 1087, 322, 116, 1040, 164, 380, 140, 139, 481, 826, 245, 1166, 504, 81, 167, 858, 1157, 1070, 647, 534, 418, 643, 488, 1213, 1388, 268, 614, 936, 1175, 148, 19, 938, 1153, 204, 150, 1101, 436, 1036, 1170, 271, 714, 1187, 500, 756, 583, 1344, 1293, 1112, 619, 1356, 16, 1135, 613, 212, 275, 1451, 236, 219, 1435, 1461, 557, 577, 431, 702, 416, 540, 1035, 1322, 1355, 104, 1457, 1253, 566, 90, 7, 683, 267, 536, 1328, 904, 875, 1163, 1320, 1233, 305, 73, 1150, 303, 880, 261, 85, 631, 746, 1263, 732, 430, 1234, 210, 724, 1223, 316, 1225, 332, 362, 844, 50, 367, 680, 843, 508, 1350, 1476, 221, 783, 79, 963, 455, 408, 942, 716, 625, 1434, 456, 48, 395, 816, 672, 1452, 1437, 571, 719, 1371, 818, 678, 56, 1137, 1174, 1339, 1155, 78, 222, 889, 707, 1199, 893, 1047, 1058, 1360, 1426, 521, 1120, 1049, 3, 403, 745, 883, 143, 1273, 1050, 1447, 615, 633, 836, 668, 1332, 605, 260, 1243, 861, 1216, 356, 630, 582, 308, 415, 561, 853, 0, 311, 293, 215, 1460, 804, 593, 621, 670, 329, 1431, 452, 1005, 691, 218, 523, 1092, 812, 922, 982, 815, 753, 173, 674, 86, 290, 527, 679, 648, 634, 343, 95, 838, 974, 769, 240, 688, 1207, 230, 825, 203, 1159, 25, 47, 250, 486, 1073, 870, 786, 74, 1072, 424, 1480, 1392, 589, 199, 1454, 713, 1438, 506, 409, 249, 151, 671, 1453, 5, 914, 768, 881, 1046, 906, 109, 797, 1391, 1367, 180, 823, 712, 530, 475, 1497, 1066, 1481, 868, 1200, 467, 136, 820, 937, 1118, 1055, 572, 609, 324, 773, 912, 453, 627, 834, 736, 913, 516, 1177, 850, 1018, 1071, 162, 761, 1255, 971, 1288, 265, 997, 253, 860, 652, 1420, 784, 796, 533, 496, 641, 244, 281, 450, 1079, 730, 1491, 981, 278, 986, 1364, 553, 82, 1406, 1201, 1048, 1026, 710, 156, 723, 1136, 1418, 965, 417, 1304, 555, 477, 425, 63, 211, 852, 1241, 398, 1235, 598, 1386, 20, 1302, 962, 1045, 1171, 1390, 360, 1109, 771, 399, 1421, 551, 1329, 752, 559, 819, 617, 225, 499, 1075, 279, 446, 1261, 29, 863, 344, 684, 695, 1295, 414, 1374, 169, 478, 1361, 637, 1144, 27, 1190, 606, 1132, 1060, 1449, 1380, 658, 1267, 1275, 472, 1369, 1439, 266, 1469, 335, 216, 465, 1410, 345, 779, 809, 284, 770, 1131, 258, 83, 1185, 1146, 767, 1407, 53, 358, 1111, 665, 70, 667, 41, 772, 31, 903, 1160, 1472, 636, 1377, 894, 129, 1126, 685, 1198, 1343, 1245, 1006, 1074, 1307, 377, 171, 620, 1009, 960, 774, 1179, 1283, 1250, 1433, 26, 319, 857, 693, 384, 406, 1351, 1376, 77, 1219, 1051, 1231, 248, 1124, 959, 1020, 700, 1167, 123, 579, 42, 355, 545, 1208, 677, 379, 862, 518, 1323, 349, 12, 1238, 1411, 108, 443, 370, 650, 470, 803, 1284, 941, 1057, 666, 276, 1389, 848, 495, 851, 984, 749, 274, 1115, 251, 1450, 1383, 461, 955, 1427, 1381, 624, 1259, 802, 1240, 1031, 957, 1091, 999, 1252, 1337, 363, 264, 348, 286, 610, 282, 1428, 10, 529, 195, 87, 1290, 1129, 1151, 568, 246, 1270, 661, 502, 458, 17, 1362, 301, 226, 830, 1444, 1475, 595, 949, 1024, 1121, 926, 352, 943, 1496, 871, 1017, 464, 277, 1345, 1334, 1105, 1440, 197, 1148, 122, 1396, 1123, 196, 1081, 902, 900, 603, 537, 1335, 289, 1378, 1256, 1106, 232, 369, 183, 309, 1279, 1194, 1408, 280, 46, 55, 659, 299, 699, 953, 105, 728, 587, 291, 480, 1317, 1336, 687, 188, 52, 798, 489, 1191, 66, 410, 503, 75, 590, 1479, 155, 152, 576, 1015, 989, 254, 121, 1064, 426, 231, 535, 856, 703, 920, 304, 439, 312, 1485, 101, 1405, 807, 1265, 944, 160, 1183, 177, 565, 76, 574, 2, 1173, 585, 898, 298, 33, 237, 295, 987, 901, 72, 239, 662, 202, 656, 763, 978, 596, 272, 1272, 1108, 580, 828, 314, 921, 1373, 127, 479, 594, 412, 887, 512, 1382, 448, 1038, 40, 442, 748, 256, 1423, 1474, 1352, 21, 1139, 1260, 179, 599, 1004, 801, 185, 878, 1346, 528, 522, 1023, 567, 341, 328, 886, 792, 1021, 717, 168, 1096, 737, 1178, 147, 339, 483, 205, 734, 586, 1042, 18, 1314, 45, 1313, 618, 165, 59, 1069, 1430, 532, 263, 422, 1016, 336, 1063, 651, 988, 1210, 1061, 1368, 905, 519, 909, 387, 934, 320, 800, 837, 681, 1333, 930, 896, 67, 1085, 840, 892, 357, 1158, 62, 626, 1192, 1128, 1251, 1078, 1459, 1100, 159, 698, 1119, 829, 208, 1306, 115, 1422, 58, 1488, 60, 331, 1228, 1054, 1282, 366, 149, 1027, 361, 1202, 578, 427, 1089, 241, 932, 233, 731, 967, 895, 97, 306, 1394, 382, 69, 35, 908, 855, 404, 849, 174, 822, 259, 806, 1325, 144, 371, 744, 300, 296, 1217, 972, 935, 1347, 525, 428, 176, 170, 423, 390, 1379, 1257, 873, 1189, 711, 459, 1044, 1271, 421, 1203, 1473, 22, 910, 242, 1214, 1326, 1398, 726, 1424, 750, 517, 639, 1274, 649, 302, 970, 811, 842, 364, 269, 697, 1483, 1172, 1458, 808, 891, 38, 888, 1395, 1222, 757, 751, 755, 524, 1246, 1011, 273, 194, 378, 721, 1403, 612, 1318, 1412, 1019, 1218, 645, 462, 604, 622, 1053, 1088, 923, 1499, 227, 831, 153, 911, 1353, 166, 28, 975, 628, 1324, 220, 660, 125, 1154, 1188, 560, 92, 1370, 89, 1147, 1237, 1165, 759, 564, 791, 1387, 1012]\n",
      "Training Locations: ['LOC1', 'LOC2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asil0892/doh_traffic_analysis/code/scripts/init_dataset.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df.sort_values(by=[\"Location\"], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "from scripts.init_dataset import get_sample\n",
    "\n",
    "train_synthetic_df, test_synthetic_df, _, _ = get_sample(synthetic_df, ['LOC1', 'LOC2'], test_web_samples, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.85, F1 Score:  90.83, Precision:  91.21, Recall:  90.85\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9085166666666666,\n",
       " 0.9120678790950376,\n",
       " 0.9085166666666666,\n",
       " 0.9083191396162973,\n",
       " array([[51726,  8274],\n",
       "        [ 2704, 57296]]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_data(train_df: pd.DataFrame, test_df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, LabelEncoder]:\n",
    "    le = LabelEncoder()\n",
    "    X_train = train_df.iloc[:, 2:]\n",
    "    X_test = test_df.iloc[:, 2:]\n",
    "\n",
    "    y_train = le.fit_transform(train_df['Location'])\n",
    "    y_test = le.transform(test_df['Location'])\n",
    "\n",
    "    return (X_train, X_test, y_train, y_test, le)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "X_train, X_test, y_train, y_test, le = preprocess_data(train_synthetic_df, test_synthetic_df)\n",
    "classification.evaluate_classification_model(X_train, y_train, X_test, y_test, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
