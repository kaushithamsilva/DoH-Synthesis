{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Loading Dataset...\n",
      "Training Websites: [1309, 228, 51, 563, 501, 457, 285, 209, 1385, 1116, 178, 1209, 864, 65, 61, 191, 447, 476, 1034, 1232, 54, 1149, 407, 1466, 1330, 1436, 1490, 859, 451, 919, 1206, 569, 13, 326, 1429, 865, 696, 1468, 318, 440, 689, 1492, 189, 778, 198, 735, 704, 1236, 541, 88, 940, 1098, 255, 775, 161, 1130, 600, 1287, 1266, 740, 1182, 393, 142, 93, 1354, 466, 592, 163, 1482, 206, 1456, 1462, 928, 1301, 747, 333, 758, 727, 429, 1372, 546, 1399, 1327, 146, 1247, 1300, 350, 1093, 1495, 334, 946, 777, 552, 1310, 1140, 449, 1402, 664, 114, 469, 1486, 646, 821, 548, 135, 432, 1161, 644, 435, 1342, 1022, 810, 1316, 939, 292, 542, 1493, 505, 1478, 1103, 538, 1197, 877, 1195, 817, 741, 1404, 283, 1043, 1010, 186, 96, 224, 313, 1285, 327, 1487, 1221, 130, 788, 781, 1220, 958, 1083, 514, 1133, 23, 234, 1099, 1419, 1312, 1463, 1498, 601, 890, 323, 929, 6, 539, 1025, 365, 1039, 217, 1280, 611, 1308, 1338, 1415, 1477, 1366, 765, 330, 1104, 1086, 1, 1226, 663, 1000, 39, 229, 743, 629, 490, 118, 493, 1393, 1445, 175, 995, 141, 1090, 257, 262, 973, 1125, 338, 1384, 1080, 1242, 866, 433, 1417, 411, 638, 1375, 764, 897, 1059, 924, 247, 507, 460, 131, 692, 43, 1204, 1134, 471, 1205, 1471, 14, 145, 120, 468, 138, 64, 676, 1278, 1052, 487, 570, 994, 438, 1298, 270, 1169, 1180, 968, 497, 1262, 833, 389, 193, 1455, 882, 725, 867, 841, 956, 110, 201, 124, 824, 694, 223, 509, 392, 1258, 1448, 918, 287, 1363, 375, 1269, 947, 511, 154, 907, 1127, 200, 103, 1107, 30, 1484, 484, 340, 832, 1268, 985, 437, 1397, 1277, 337, 776, 4, 799, 543, 931, 584, 1414, 1138, 996, 317, 388, 607, 445, 119, 1186, 1110, 1248, 642, 117, 102, 1196, 976, 1029, 1087, 322, 116, 1040, 164, 380, 140, 139, 481, 826, 245, 1166, 504, 81, 167, 858, 1157, 1070, 647, 534, 418, 643, 488, 1213, 1388, 268, 614, 936, 1175, 148, 19, 938, 1153, 204, 150, 1101, 436, 1036, 1170, 271, 714, 1187, 500, 756, 583, 1344, 1293, 1112, 619, 1356, 16, 1135, 613, 212, 275, 1451, 236, 219, 1435, 1461, 557, 577, 431, 702, 416, 540, 1035, 1322, 1355, 104, 1457, 1253, 566, 90, 7, 683, 267, 536, 1328, 904, 875, 1163, 1320, 1233, 305, 73, 1150, 303, 880, 261, 85, 631, 746, 1263, 732, 430, 1234, 210, 724, 1223, 316, 1225, 332, 362, 844, 50, 367, 680, 843, 508, 1350, 1476, 221, 783, 79, 963, 455, 408, 942, 716, 625, 1434, 456, 48, 395, 816, 672, 1452, 1437, 571, 719, 1371, 818, 678, 56, 1137, 1174, 1339, 1155, 78, 222, 889, 707, 1199, 893, 1047, 1058, 1360, 1426, 521, 1120, 1049, 3, 403, 745, 883, 143, 1273, 1050, 1447, 615, 633, 836, 668, 1332, 605, 260, 1243, 861, 1216, 356, 630, 582, 308, 415, 561, 853, 0, 311, 293, 215, 1460, 804, 593, 621, 670, 329, 1431, 452, 1005, 691, 218, 523, 1092, 812, 922, 982, 815, 753, 173, 674, 86, 290, 527, 679, 648, 634, 343, 95, 838, 974, 769, 240, 688, 1207, 230, 825, 203, 1159, 25, 47, 250, 486, 1073, 870, 786, 74, 1072, 424, 1480, 1392, 589, 199, 1454, 713, 1438, 506, 409, 249, 151, 671, 1453, 5, 914, 768, 881, 1046, 906, 109, 797, 1391, 1367, 180, 823, 712, 530, 475, 1497, 1066, 1481, 868, 1200, 467, 136, 820, 937, 1118, 1055, 572, 609, 324, 773, 912, 453, 627, 834, 736, 913, 516, 1177, 850, 1018, 1071, 162, 761, 1255, 971, 1288, 265, 997, 253, 860, 652, 1420, 784, 796, 533, 496, 641, 244, 281, 450, 1079, 730, 1491, 981, 278, 986, 1364, 553, 82, 1406, 1201, 1048, 1026, 710, 156, 723, 1136, 1418, 965, 417, 1304, 555, 477, 425, 63, 211, 852, 1241, 398, 1235, 598, 1386, 20, 1302, 962, 1045, 1171, 1390, 360, 1109, 771, 399, 1421, 551, 1329, 752, 559, 819, 617, 225, 499, 1075, 279, 446, 1261, 29, 863, 344, 684, 695, 1295, 414, 1374, 169, 478, 1361, 637, 1144, 27, 1190, 606, 1132, 1060, 1449, 1380, 658, 1267, 1275, 472, 1369, 1439, 266, 1469, 335, 216, 465, 1410, 345, 779, 809, 284, 770, 1131, 258, 83, 1185, 1146, 767, 1407, 53, 358, 1111, 665, 70, 667, 41, 772, 31, 903, 1160, 1472, 636, 1377, 894, 129, 1126, 685, 1198, 1343, 1245, 1006, 1074, 1307, 377, 171, 620, 1009, 960, 774, 1179, 1283, 1250, 1433, 26, 319, 857, 693, 384, 406, 1351, 1376, 77, 1219, 1051, 1231, 248, 1124, 959, 1020, 700, 1167, 123, 579, 42, 355, 545, 1208, 677, 379, 862, 518, 1323, 349, 12, 1238, 1411, 108, 443, 370, 650, 470, 803, 1284, 941, 1057, 666, 276, 1389, 848, 495, 851, 984, 749, 274, 1115, 251, 1450, 1383, 461, 955, 1427, 1381, 624, 1259, 802, 1240, 1031, 957, 1091, 999, 1252, 1337, 363, 264, 348, 286, 610, 282, 1428, 10, 529, 195, 87, 1290, 1129, 1151, 568, 246, 1270, 661, 502, 458, 17, 1362, 301, 226, 830, 1444, 1475, 595, 949, 1024, 1121, 926, 352, 943, 1496, 871, 1017, 464, 277, 1345, 1334, 1105, 1440, 197, 1148, 122, 1396, 1123, 196, 1081, 902, 900, 603, 537, 1335, 289, 1378, 1256, 1106, 232, 369, 183, 309, 1279, 1194, 1408, 280, 46, 55, 659, 299, 699, 953, 105, 728, 587, 291, 480, 1317, 1336, 687, 188, 52, 798, 489, 1191, 66, 410, 503, 75, 590, 1479, 155, 152, 576, 1015, 989, 254, 121, 1064, 426, 231, 535, 856, 703, 920, 304, 439, 312, 1485, 101, 1405, 807, 1265, 944, 160, 1183, 177, 565, 76, 574, 2, 1173, 585, 898, 298, 33, 237, 295, 987, 901, 72, 239, 662, 202, 656, 763, 978, 596, 272, 1272, 1108, 580, 828, 314, 921, 1373, 127, 479, 594, 412, 887, 512, 1382, 448, 1038, 40, 442, 748, 256, 1423, 1474, 1352, 21, 1139, 1260, 179, 599, 1004, 801, 185, 878, 1346, 528, 522, 1023, 567, 341, 328, 886, 792, 1021, 717, 168, 1096, 737, 1178, 147, 339, 483, 205, 734, 586, 1042, 18, 1314, 45, 1313, 618, 165, 59, 1069, 1430, 532, 263, 422, 1016, 336, 1063, 651, 988, 1210, 1061, 1368, 905, 519, 909, 387, 934, 320, 800, 837, 681, 1333, 930, 896, 67, 1085, 840, 892, 357, 1158, 62, 626, 1192, 1128, 1251, 1078, 1459, 1100, 159, 698, 1119, 829, 208, 1306, 115, 1422, 58, 1488, 60, 331, 1228, 1054, 1282, 366, 149, 1027, 361, 1202, 578, 427, 1089, 241, 932, 233, 731, 967, 895, 97, 306, 1394, 382, 69, 35, 908, 855, 404, 849, 174, 822, 259, 806, 1325, 144, 371, 744, 300, 296, 1217, 972, 935, 1347, 525, 428, 176, 170, 423, 390, 1379, 1257, 873, 1189, 711, 459, 1044, 1271, 421, 1203, 1473, 22, 910, 242, 1214, 1326, 1398, 726, 1424, 750, 517, 639, 1274, 649, 302, 970, 811, 842, 364, 269, 697, 1483, 1172, 1458, 808, 891, 38, 888, 1395, 1222, 757, 751, 755, 524, 1246, 1011, 273, 194, 378, 721, 1403, 612, 1318, 1412, 1019, 1218, 645, 462, 604, 622, 1053, 1088, 923, 1499, 227, 831, 153, 911, 1353, 166, 28, 975, 628, 1324, 220, 660, 125, 1154, 1188, 560, 92, 1370, 89, 1147, 1237, 1165, 759, 564, 791, 1387, 1012]\n",
      "Training Locations: ['LOC2', 'LOC3']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e19372/DoH-Synthesis/code/scripts/init_dataset.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df.sort_values(by=[\"Location\"], inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Website</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>...</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOC2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.714089</td>\n",
       "      <td>0.541197</td>\n",
       "      <td>0.391921</td>\n",
       "      <td>-0.400778</td>\n",
       "      <td>-0.266345</td>\n",
       "      <td>-0.522526</td>\n",
       "      <td>0.023889</td>\n",
       "      <td>-0.261817</td>\n",
       "      <td>0.293742</td>\n",
       "      <td>0.328137</td>\n",
       "      <td>0.126488</td>\n",
       "      <td>-0.549498</td>\n",
       "      <td>-1.544878</td>\n",
       "      <td>-0.066524</td>\n",
       "      <td>0.358774</td>\n",
       "      <td>0.462756</td>\n",
       "      <td>-0.006084</td>\n",
       "      <td>0.732340</td>\n",
       "      <td>0.795686</td>\n",
       "      <td>0.209927</td>\n",
       "      <td>-1.926165</td>\n",
       "      <td>-0.553122</td>\n",
       "      <td>-1.038216</td>\n",
       "      <td>0.053991</td>\n",
       "      <td>0.465841</td>\n",
       "      <td>0.693164</td>\n",
       "      <td>0.348125</td>\n",
       "      <td>0.819507</td>\n",
       "      <td>0.991114</td>\n",
       "      <td>0.426657</td>\n",
       "      <td>0.434773</td>\n",
       "      <td>0.842753</td>\n",
       "      <td>0.724091</td>\n",
       "      <td>-1.489046</td>\n",
       "      <td>-0.168640</td>\n",
       "      <td>-1.504361</td>\n",
       "      <td>-0.460175</td>\n",
       "      <td>-1.170200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164343</td>\n",
       "      <td>0.119994</td>\n",
       "      <td>0.093444</td>\n",
       "      <td>0.038966</td>\n",
       "      <td>0.21022</td>\n",
       "      <td>0.162792</td>\n",
       "      <td>0.119427</td>\n",
       "      <td>0.092497</td>\n",
       "      <td>0.035366</td>\n",
       "      <td>0.202943</td>\n",
       "      <td>0.150631</td>\n",
       "      <td>0.114271</td>\n",
       "      <td>0.08326</td>\n",
       "      <td>0.031206</td>\n",
       "      <td>0.182456</td>\n",
       "      <td>0.136367</td>\n",
       "      <td>0.108917</td>\n",
       "      <td>0.085165</td>\n",
       "      <td>0.032035</td>\n",
       "      <td>0.174922</td>\n",
       "      <td>0.133561</td>\n",
       "      <td>0.110018</td>\n",
       "      <td>0.087227</td>\n",
       "      <td>0.034741</td>\n",
       "      <td>0.159972</td>\n",
       "      <td>0.122191</td>\n",
       "      <td>0.097487</td>\n",
       "      <td>0.084637</td>\n",
       "      <td>0.039947</td>\n",
       "      <td>0.160207</td>\n",
       "      <td>0.125986</td>\n",
       "      <td>0.10277</td>\n",
       "      <td>0.092495</td>\n",
       "      <td>0.043004</td>\n",
       "      <td>0.158904</td>\n",
       "      <td>0.119506</td>\n",
       "      <td>0.099445</td>\n",
       "      <td>0.083843</td>\n",
       "      <td>0.037226</td>\n",
       "      <td>0.152216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LOC2</td>\n",
       "      <td>1006</td>\n",
       "      <td>0.093792</td>\n",
       "      <td>-1.273349</td>\n",
       "      <td>-2.142801</td>\n",
       "      <td>1.205098</td>\n",
       "      <td>1.879002</td>\n",
       "      <td>2.139593</td>\n",
       "      <td>1.482513</td>\n",
       "      <td>2.382939</td>\n",
       "      <td>0.110383</td>\n",
       "      <td>-2.678266</td>\n",
       "      <td>-2.454618</td>\n",
       "      <td>0.740625</td>\n",
       "      <td>1.503410</td>\n",
       "      <td>0.802837</td>\n",
       "      <td>-2.707920</td>\n",
       "      <td>-0.723518</td>\n",
       "      <td>0.333578</td>\n",
       "      <td>1.030483</td>\n",
       "      <td>0.523433</td>\n",
       "      <td>0.487220</td>\n",
       "      <td>0.663673</td>\n",
       "      <td>0.324062</td>\n",
       "      <td>0.841249</td>\n",
       "      <td>1.105190</td>\n",
       "      <td>0.439766</td>\n",
       "      <td>-1.321745</td>\n",
       "      <td>-0.423351</td>\n",
       "      <td>-1.206079</td>\n",
       "      <td>0.021729</td>\n",
       "      <td>-1.593981</td>\n",
       "      <td>-0.294538</td>\n",
       "      <td>0.572230</td>\n",
       "      <td>1.094341</td>\n",
       "      <td>0.751568</td>\n",
       "      <td>-1.610703</td>\n",
       "      <td>-0.333367</td>\n",
       "      <td>0.571780</td>\n",
       "      <td>1.125553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164343</td>\n",
       "      <td>0.119994</td>\n",
       "      <td>0.093444</td>\n",
       "      <td>0.038966</td>\n",
       "      <td>0.21022</td>\n",
       "      <td>0.162792</td>\n",
       "      <td>0.119427</td>\n",
       "      <td>0.092497</td>\n",
       "      <td>0.035366</td>\n",
       "      <td>0.202943</td>\n",
       "      <td>0.150631</td>\n",
       "      <td>0.114271</td>\n",
       "      <td>0.08326</td>\n",
       "      <td>0.031206</td>\n",
       "      <td>0.182456</td>\n",
       "      <td>0.136367</td>\n",
       "      <td>0.108917</td>\n",
       "      <td>0.085165</td>\n",
       "      <td>0.032035</td>\n",
       "      <td>0.174922</td>\n",
       "      <td>0.133561</td>\n",
       "      <td>0.110018</td>\n",
       "      <td>0.087227</td>\n",
       "      <td>0.034741</td>\n",
       "      <td>0.159972</td>\n",
       "      <td>0.122191</td>\n",
       "      <td>0.097487</td>\n",
       "      <td>0.084637</td>\n",
       "      <td>0.039947</td>\n",
       "      <td>0.160207</td>\n",
       "      <td>0.125986</td>\n",
       "      <td>0.10277</td>\n",
       "      <td>0.092495</td>\n",
       "      <td>0.043004</td>\n",
       "      <td>0.158904</td>\n",
       "      <td>0.119506</td>\n",
       "      <td>0.099445</td>\n",
       "      <td>0.083843</td>\n",
       "      <td>0.037226</td>\n",
       "      <td>0.152216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOC2</td>\n",
       "      <td>1006</td>\n",
       "      <td>1.401091</td>\n",
       "      <td>-1.636258</td>\n",
       "      <td>0.527106</td>\n",
       "      <td>0.354928</td>\n",
       "      <td>-0.017609</td>\n",
       "      <td>-0.049260</td>\n",
       "      <td>-0.177300</td>\n",
       "      <td>-0.412692</td>\n",
       "      <td>0.110383</td>\n",
       "      <td>0.404009</td>\n",
       "      <td>0.608673</td>\n",
       "      <td>0.023890</td>\n",
       "      <td>-0.730847</td>\n",
       "      <td>-0.186436</td>\n",
       "      <td>0.476159</td>\n",
       "      <td>0.487470</td>\n",
       "      <td>0.175069</td>\n",
       "      <td>0.800487</td>\n",
       "      <td>1.079776</td>\n",
       "      <td>0.333168</td>\n",
       "      <td>0.445372</td>\n",
       "      <td>0.898768</td>\n",
       "      <td>0.755035</td>\n",
       "      <td>0.790914</td>\n",
       "      <td>0.948232</td>\n",
       "      <td>0.419784</td>\n",
       "      <td>-2.139220</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>-1.627338</td>\n",
       "      <td>-0.296781</td>\n",
       "      <td>-1.409956</td>\n",
       "      <td>-0.509861</td>\n",
       "      <td>-1.244553</td>\n",
       "      <td>-0.014652</td>\n",
       "      <td>0.684058</td>\n",
       "      <td>0.814889</td>\n",
       "      <td>0.420022</td>\n",
       "      <td>0.817586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164343</td>\n",
       "      <td>0.119994</td>\n",
       "      <td>0.093444</td>\n",
       "      <td>0.038966</td>\n",
       "      <td>0.21022</td>\n",
       "      <td>0.162792</td>\n",
       "      <td>0.119427</td>\n",
       "      <td>0.092497</td>\n",
       "      <td>0.035366</td>\n",
       "      <td>0.202943</td>\n",
       "      <td>0.150631</td>\n",
       "      <td>0.114271</td>\n",
       "      <td>0.08326</td>\n",
       "      <td>0.031206</td>\n",
       "      <td>0.182456</td>\n",
       "      <td>0.136367</td>\n",
       "      <td>0.108917</td>\n",
       "      <td>0.085165</td>\n",
       "      <td>0.032035</td>\n",
       "      <td>0.174922</td>\n",
       "      <td>0.133561</td>\n",
       "      <td>0.110018</td>\n",
       "      <td>0.087227</td>\n",
       "      <td>0.034741</td>\n",
       "      <td>0.159972</td>\n",
       "      <td>0.122191</td>\n",
       "      <td>0.097487</td>\n",
       "      <td>0.084637</td>\n",
       "      <td>0.039947</td>\n",
       "      <td>0.160207</td>\n",
       "      <td>0.125986</td>\n",
       "      <td>0.10277</td>\n",
       "      <td>0.092495</td>\n",
       "      <td>0.043004</td>\n",
       "      <td>0.158904</td>\n",
       "      <td>0.119506</td>\n",
       "      <td>0.099445</td>\n",
       "      <td>0.083843</td>\n",
       "      <td>0.037226</td>\n",
       "      <td>0.152216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LOC2</td>\n",
       "      <td>1006</td>\n",
       "      <td>-0.831599</td>\n",
       "      <td>0.662167</td>\n",
       "      <td>0.527106</td>\n",
       "      <td>0.354928</td>\n",
       "      <td>-0.017609</td>\n",
       "      <td>-0.049260</td>\n",
       "      <td>-0.177300</td>\n",
       "      <td>-0.412692</td>\n",
       "      <td>0.110383</td>\n",
       "      <td>0.404009</td>\n",
       "      <td>0.608673</td>\n",
       "      <td>0.023890</td>\n",
       "      <td>-0.739507</td>\n",
       "      <td>-0.186436</td>\n",
       "      <td>0.476159</td>\n",
       "      <td>0.487470</td>\n",
       "      <td>0.175069</td>\n",
       "      <td>0.800487</td>\n",
       "      <td>1.079776</td>\n",
       "      <td>0.333168</td>\n",
       "      <td>0.445372</td>\n",
       "      <td>0.898768</td>\n",
       "      <td>0.755035</td>\n",
       "      <td>0.790914</td>\n",
       "      <td>0.948232</td>\n",
       "      <td>0.419784</td>\n",
       "      <td>-2.857490</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>-1.627338</td>\n",
       "      <td>-0.296781</td>\n",
       "      <td>-1.409956</td>\n",
       "      <td>-0.509861</td>\n",
       "      <td>-0.756907</td>\n",
       "      <td>-0.014652</td>\n",
       "      <td>0.684058</td>\n",
       "      <td>0.814889</td>\n",
       "      <td>0.420022</td>\n",
       "      <td>0.817586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164343</td>\n",
       "      <td>0.119994</td>\n",
       "      <td>0.093444</td>\n",
       "      <td>0.038966</td>\n",
       "      <td>0.21022</td>\n",
       "      <td>0.162792</td>\n",
       "      <td>0.119427</td>\n",
       "      <td>0.092497</td>\n",
       "      <td>0.035366</td>\n",
       "      <td>0.202943</td>\n",
       "      <td>0.150631</td>\n",
       "      <td>0.114271</td>\n",
       "      <td>0.08326</td>\n",
       "      <td>0.031206</td>\n",
       "      <td>0.182456</td>\n",
       "      <td>0.136367</td>\n",
       "      <td>0.108917</td>\n",
       "      <td>0.085165</td>\n",
       "      <td>0.032035</td>\n",
       "      <td>0.174922</td>\n",
       "      <td>0.133561</td>\n",
       "      <td>0.110018</td>\n",
       "      <td>0.087227</td>\n",
       "      <td>0.034741</td>\n",
       "      <td>0.159972</td>\n",
       "      <td>0.122191</td>\n",
       "      <td>0.097487</td>\n",
       "      <td>0.084637</td>\n",
       "      <td>0.039947</td>\n",
       "      <td>0.160207</td>\n",
       "      <td>0.125986</td>\n",
       "      <td>0.10277</td>\n",
       "      <td>0.092495</td>\n",
       "      <td>0.043004</td>\n",
       "      <td>0.158904</td>\n",
       "      <td>0.119506</td>\n",
       "      <td>0.099445</td>\n",
       "      <td>0.083843</td>\n",
       "      <td>0.037226</td>\n",
       "      <td>0.152216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LOC2</td>\n",
       "      <td>1006</td>\n",
       "      <td>0.093792</td>\n",
       "      <td>-1.152379</td>\n",
       "      <td>-2.007616</td>\n",
       "      <td>0.827245</td>\n",
       "      <td>1.630266</td>\n",
       "      <td>1.666328</td>\n",
       "      <td>1.281324</td>\n",
       "      <td>2.311938</td>\n",
       "      <td>0.293742</td>\n",
       "      <td>-2.554975</td>\n",
       "      <td>-2.227708</td>\n",
       "      <td>0.167237</td>\n",
       "      <td>1.434131</td>\n",
       "      <td>0.682925</td>\n",
       "      <td>-2.605208</td>\n",
       "      <td>-0.624662</td>\n",
       "      <td>0.220357</td>\n",
       "      <td>0.919744</td>\n",
       "      <td>0.428736</td>\n",
       "      <td>-2.177875</td>\n",
       "      <td>-0.229375</td>\n",
       "      <td>0.354309</td>\n",
       "      <td>1.013677</td>\n",
       "      <td>0.595846</td>\n",
       "      <td>0.465841</td>\n",
       "      <td>0.733664</td>\n",
       "      <td>0.348125</td>\n",
       "      <td>-0.800961</td>\n",
       "      <td>0.110868</td>\n",
       "      <td>-2.043011</td>\n",
       "      <td>-0.208737</td>\n",
       "      <td>0.458326</td>\n",
       "      <td>1.103371</td>\n",
       "      <td>0.658693</td>\n",
       "      <td>0.583740</td>\n",
       "      <td>0.701200</td>\n",
       "      <td>0.298616</td>\n",
       "      <td>-1.216862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164343</td>\n",
       "      <td>0.119994</td>\n",
       "      <td>0.093444</td>\n",
       "      <td>0.038966</td>\n",
       "      <td>0.21022</td>\n",
       "      <td>0.162792</td>\n",
       "      <td>0.119427</td>\n",
       "      <td>0.092497</td>\n",
       "      <td>0.035366</td>\n",
       "      <td>0.202943</td>\n",
       "      <td>0.150631</td>\n",
       "      <td>0.114271</td>\n",
       "      <td>0.08326</td>\n",
       "      <td>0.031206</td>\n",
       "      <td>0.182456</td>\n",
       "      <td>0.136367</td>\n",
       "      <td>0.108917</td>\n",
       "      <td>0.085165</td>\n",
       "      <td>0.032035</td>\n",
       "      <td>0.174922</td>\n",
       "      <td>0.133561</td>\n",
       "      <td>0.110018</td>\n",
       "      <td>0.087227</td>\n",
       "      <td>0.034741</td>\n",
       "      <td>0.159972</td>\n",
       "      <td>0.122191</td>\n",
       "      <td>0.097487</td>\n",
       "      <td>0.084637</td>\n",
       "      <td>0.039947</td>\n",
       "      <td>0.160207</td>\n",
       "      <td>0.125986</td>\n",
       "      <td>0.10277</td>\n",
       "      <td>0.092495</td>\n",
       "      <td>0.043004</td>\n",
       "      <td>0.158904</td>\n",
       "      <td>0.119506</td>\n",
       "      <td>0.099445</td>\n",
       "      <td>0.083843</td>\n",
       "      <td>0.037226</td>\n",
       "      <td>0.152216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Location  Website         0         1  ...       124       125       126       127\n",
       "0     LOC2        0 -0.714089  0.541197  ...  0.099445  0.083843  0.037226  0.152216\n",
       "1     LOC2     1006  0.093792 -1.273349  ...  0.099445  0.083843  0.037226  0.152216\n",
       "2     LOC2     1006  1.401091 -1.636258  ...  0.099445  0.083843  0.037226  0.152216\n",
       "3     LOC2     1006 -0.831599  0.662167  ...  0.099445  0.083843  0.037226  0.152216\n",
       "4     LOC2     1006  0.093792 -1.152379  ...  0.099445  0.083843  0.037226  0.152216\n",
       "\n",
       "[5 rows x 130 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scripts.init_gpu as init_gpu\n",
    "import scripts.init_dataset as init_dataset\n",
    "from scripts.triplet_functions import n_neurons\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "init_gpu.initialize_gpus()\n",
    "\n",
    "locations = ['LOC2', 'LOC3']\n",
    "\n",
    "print(\"Loading Dataset...\")\n",
    "# load the dataset\n",
    "df = pd.read_csv(\n",
    "    f\"../dataset/processed/{locations[0]}-{locations[1]}-scaled-balanced.csv\")\n",
    "\n",
    "length = len(df.columns) - 2  # subtract the two label columns\n",
    "\n",
    "# get train-test set\n",
    "train_df, test_df, train_web_samples, test_web_samples = init_dataset.get_sample(\n",
    "    df, locations, range(1500), 1200)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29700 entries, 0 to 29699\n",
      "Columns: 130 entries, Location to 127\n",
      "dtypes: float64(128), int64(1), object(1)\n",
      "memory usage: 29.5+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_200022/2431998659.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target_df.sort_values(by=['Website'], inplace=True)\n",
      "/tmp/ipykernel_200022/2431998659.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  source_df.sort_values(by=['Website'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# dataset for the classification\n",
    "source_location, target_location = locations\n",
    "\n",
    "target_df = test_df[test_df['Location'] == target_location]\n",
    "target_df.sort_values(by=['Website'], inplace=True)\n",
    "target_df.reset_index(drop=True, inplace=True)\n",
    "target_df.head(20)\n",
    "\n",
    "source_df = test_df[test_df['Location'] == source_location]\n",
    "source_df.sort_values(by=['Website'], inplace=True)\n",
    "source_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "source_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained completely on source data, tested on real target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scripts.classification as classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import random\n",
    "import xgboost\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "# synthetic data\n",
    "X_train = source_df.iloc[:, 2:]\n",
    "y_train = le.fit_transform(source_df.Website)\n",
    "X_test = target_df.iloc[:, 2:]\n",
    "y_test = le.transform(target_df.Website)\n",
    "\n",
    "#  # Train and evaluate the model\n",
    "# model = xgboost.XGBClassifier(max_depth=10, n_estimators=30)\n",
    "# print(\"Trained completely on the source data: \")\n",
    "# accuracy, precision, recall, f1_score, cm = classification.evaluate_classification_model(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained in a CNN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e19372/anaconda3/envs/doh_synth_env/lib/python3.11/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1737570672.636096  203215 service.cc:148] XLA service 0x79bf70017a20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1737570672.646686  203215 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3090 Ti, Compute Capability 8.6\n",
      "2025-01-23 00:01:13.156206: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1737570673.783955  203215 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-01-23 00:01:14.111294: W external/local_xla/xla/service/gpu/nvptx_compiler.cc:930] The NVIDIA driver's CUDA version is 12.2 which is older than the PTX compiler version 12.5.82. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "2025-01-23 00:01:14.653497: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1883', 620 bytes spill stores, 520 bytes spill loads\n",
      "\n",
      "2025-01-23 00:01:14.657822: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2268', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-01-23 00:01:14.667578: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2268', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-01-23 00:01:16.813817: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng0{} for conv (f32[96,1,1,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,1,1,129]{3,2,1,0}, f32[32,96,1,64]{3,2,1,0}), window={size=1x7 stride=1x2 pad=0_0x2_2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n",
      "2025-01-23 00:01:19.428332: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 3.614784147s\n",
      "Trying algorithm eng0{} for conv (f32[96,1,1,7]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,1,1,129]{3,2,1,0}, f32[32,96,1,64]{3,2,1,0}), window={size=1x7 stride=1x2 pad=0_0x2_2}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardFilter\", backend_config={\"cudnn_conv_backend_config\":{\"activation_mode\":\"kNone\",\"conv_result_scale\":1,\"leakyrelu_alpha\":0,\"side_input_scale\":0},\"force_earliest_schedule\":false,\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[]} is taking a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 43/929\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.0158 - loss: 6.0710 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1737570680.935545  203215 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m923/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3524 - loss: 3.3303"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 00:01:25.538052: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1883', 232 bytes spill stores, 236 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.3539 - loss: 3.3205\n",
      "Epoch 2/20\n",
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7980 - loss: 0.7074\n",
      "Epoch 3/20\n",
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8419 - loss: 0.5072\n",
      "Epoch 4/20\n",
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8762 - loss: 0.3850\n",
      "Epoch 5/20\n",
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8894 - loss: 0.3202\n",
      "Epoch 6/20\n",
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8923 - loss: 0.3128\n",
      "Epoch 7/20\n",
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9006 - loss: 0.2807\n",
      "Epoch 8/20\n",
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9094 - loss: 0.2532\n",
      "Epoch 9/20\n",
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9162 - loss: 0.2345\n",
      "Epoch 10/20\n",
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9167 - loss: 0.2329\n",
      "Epoch 11/20\n",
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9220 - loss: 0.2122\n",
      "Epoch 12/20\n",
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9145 - loss: 0.2375\n",
      "Epoch 13/20\n",
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9310 - loss: 0.1909\n",
      "Epoch 14/20\n",
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9272 - loss: 0.1926\n",
      "Epoch 15/20\n",
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9360 - loss: 0.1738\n",
      "Epoch 16/20\n",
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9381 - loss: 0.1676\n",
      "Epoch 17/20\n",
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9350 - loss: 0.1784\n",
      "Epoch 18/20\n",
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9412 - loss: 0.1590\n",
      "Epoch 19/20\n",
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9379 - loss: 0.1660\n",
      "Epoch 20/20\n",
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9367 - loss: 0.1648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x79c1b03a08d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, **kwargs):\n",
    "        super(ResidualBlock, self).__init__(**kwargs)\n",
    "        self.conv1 = keras.layers.Conv1D(\n",
    "            filters, kernel_size, activation=\"relu\", padding=\"same\")\n",
    "        self.conv2 = keras.layers.Conv1D(\n",
    "            filters, kernel_size, activation=None, padding=\"same\")\n",
    "        self.activation = keras.layers.Activation(\"relu\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.conv2(x)\n",
    "        return self.activation(x + inputs)  # Adding the residual connection\n",
    "\n",
    "def build_classifier(input_dim, hidden_dim, num_classes):\n",
    "    base_model = keras.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(input_dim, 1)),\n",
    "        keras.layers.Conv1D(hidden_dim, kernel_size=7,\n",
    "                            strides=2, activation=\"relu\", padding=\"same\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        ResidualBlock(hidden_dim, kernel_size=5),\n",
    "        keras.layers.Conv1D(hidden_dim * 2, kernel_size=3,\n",
    "                            strides=2, activation=\"relu\", padding=\"same\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        ResidualBlock(hidden_dim * 2, kernel_size=3),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Flatten(),\n",
    "    ])\n",
    "\n",
    "    classifier = keras.Sequential([\n",
    "        base_model,\n",
    "        keras.layers.Dense(hidden_dim * 2, activation=\"relu\"),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(num_classes)  # No softmax for logits\n",
    "    ])\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/929\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 10ms/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Accuracy: 0.2626\n",
      "F1 Score: 0.2441\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "model = build_classifier(input_dim=length, hidden_dim=96, num_classes=len(test_web_samples))\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=20, shuffle=True)\n",
    "# Get logits from model prediction\n",
    "logits = model.predict(X_test)\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probabilities = tf.nn.softmax(logits).numpy()\n",
    "\n",
    "# Get predicted class by selecting the class with highest probability\n",
    "y_pred = np.argmax(probabilities, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')  # Use weighted average for imbalanced data\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained completely on Synthetic Data, test on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Website</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>...</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LOC3</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.801097</td>\n",
       "      <td>0.615181</td>\n",
       "      <td>0.486813</td>\n",
       "      <td>-0.093054</td>\n",
       "      <td>-0.036950</td>\n",
       "      <td>-0.038655</td>\n",
       "      <td>-0.154893</td>\n",
       "      <td>-0.410852</td>\n",
       "      <td>0.125459</td>\n",
       "      <td>0.413805</td>\n",
       "      <td>0.366333</td>\n",
       "      <td>0.025169</td>\n",
       "      <td>-0.745871</td>\n",
       "      <td>-0.205958</td>\n",
       "      <td>0.432189</td>\n",
       "      <td>0.613172</td>\n",
       "      <td>0.255371</td>\n",
       "      <td>0.795360</td>\n",
       "      <td>0.954577</td>\n",
       "      <td>0.324315</td>\n",
       "      <td>-0.535657</td>\n",
       "      <td>-2.739595</td>\n",
       "      <td>0.269144</td>\n",
       "      <td>-3.159148</td>\n",
       "      <td>-0.418931</td>\n",
       "      <td>0.390319</td>\n",
       "      <td>0.867225</td>\n",
       "      <td>0.730581</td>\n",
       "      <td>-1.609323</td>\n",
       "      <td>-0.250928</td>\n",
       "      <td>0.505421</td>\n",
       "      <td>0.908549</td>\n",
       "      <td>0.842472</td>\n",
       "      <td>-1.371934</td>\n",
       "      <td>-0.220339</td>\n",
       "      <td>0.501074</td>\n",
       "      <td>0.929169</td>\n",
       "      <td>0.728819</td>\n",
       "      <td>...</td>\n",
       "      <td>0.706278</td>\n",
       "      <td>-2.653882</td>\n",
       "      <td>-0.255431</td>\n",
       "      <td>0.813479</td>\n",
       "      <td>0.968307</td>\n",
       "      <td>0.784570</td>\n",
       "      <td>-0.073740</td>\n",
       "      <td>0.461185</td>\n",
       "      <td>0.303371</td>\n",
       "      <td>0.202163</td>\n",
       "      <td>-3.176988</td>\n",
       "      <td>-0.389997</td>\n",
       "      <td>0.679713</td>\n",
       "      <td>1.206371</td>\n",
       "      <td>0.516113</td>\n",
       "      <td>0.880956</td>\n",
       "      <td>1.340733</td>\n",
       "      <td>0.629781</td>\n",
       "      <td>-3.600099</td>\n",
       "      <td>-0.181180</td>\n",
       "      <td>-0.153915</td>\n",
       "      <td>0.433763</td>\n",
       "      <td>0.520938</td>\n",
       "      <td>1.030614</td>\n",
       "      <td>0.627094</td>\n",
       "      <td>-3.284864</td>\n",
       "      <td>-0.301891</td>\n",
       "      <td>-0.245115</td>\n",
       "      <td>-3.342043</td>\n",
       "      <td>0.098388</td>\n",
       "      <td>1.010737</td>\n",
       "      <td>1.381215</td>\n",
       "      <td>0.774437</td>\n",
       "      <td>-4.274537</td>\n",
       "      <td>-0.397408</td>\n",
       "      <td>-3.771736</td>\n",
       "      <td>-0.462209</td>\n",
       "      <td>-0.678699</td>\n",
       "      <td>0.533888</td>\n",
       "      <td>0.704908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LOC3</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.815983</td>\n",
       "      <td>0.624921</td>\n",
       "      <td>0.492457</td>\n",
       "      <td>-0.097545</td>\n",
       "      <td>-0.031203</td>\n",
       "      <td>-0.048908</td>\n",
       "      <td>-0.150839</td>\n",
       "      <td>-0.393234</td>\n",
       "      <td>0.115055</td>\n",
       "      <td>0.401166</td>\n",
       "      <td>0.354536</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>-0.756619</td>\n",
       "      <td>-0.200891</td>\n",
       "      <td>0.440968</td>\n",
       "      <td>0.609265</td>\n",
       "      <td>0.248064</td>\n",
       "      <td>0.794166</td>\n",
       "      <td>0.953027</td>\n",
       "      <td>0.316028</td>\n",
       "      <td>-0.517619</td>\n",
       "      <td>-2.766729</td>\n",
       "      <td>0.254143</td>\n",
       "      <td>-3.154434</td>\n",
       "      <td>-0.414109</td>\n",
       "      <td>0.388955</td>\n",
       "      <td>0.866262</td>\n",
       "      <td>0.743698</td>\n",
       "      <td>-1.649488</td>\n",
       "      <td>-0.244881</td>\n",
       "      <td>0.485870</td>\n",
       "      <td>0.926196</td>\n",
       "      <td>0.841446</td>\n",
       "      <td>-1.406608</td>\n",
       "      <td>-0.218507</td>\n",
       "      <td>0.488831</td>\n",
       "      <td>0.953089</td>\n",
       "      <td>0.740573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606091</td>\n",
       "      <td>-2.611491</td>\n",
       "      <td>-0.289277</td>\n",
       "      <td>0.672803</td>\n",
       "      <td>0.910531</td>\n",
       "      <td>0.799682</td>\n",
       "      <td>0.516571</td>\n",
       "      <td>0.756064</td>\n",
       "      <td>0.951081</td>\n",
       "      <td>0.410307</td>\n",
       "      <td>-3.072308</td>\n",
       "      <td>-0.439858</td>\n",
       "      <td>0.682965</td>\n",
       "      <td>1.236678</td>\n",
       "      <td>0.550735</td>\n",
       "      <td>0.940887</td>\n",
       "      <td>1.197089</td>\n",
       "      <td>0.538074</td>\n",
       "      <td>-3.391568</td>\n",
       "      <td>-0.270888</td>\n",
       "      <td>0.776706</td>\n",
       "      <td>-0.609679</td>\n",
       "      <td>0.175245</td>\n",
       "      <td>0.904013</td>\n",
       "      <td>0.669747</td>\n",
       "      <td>-2.956333</td>\n",
       "      <td>-0.358175</td>\n",
       "      <td>-0.177188</td>\n",
       "      <td>-3.501562</td>\n",
       "      <td>-0.114892</td>\n",
       "      <td>0.978817</td>\n",
       "      <td>1.281078</td>\n",
       "      <td>0.795818</td>\n",
       "      <td>-4.073562</td>\n",
       "      <td>-0.394830</td>\n",
       "      <td>-3.503101</td>\n",
       "      <td>-0.515851</td>\n",
       "      <td>-0.562493</td>\n",
       "      <td>0.485425</td>\n",
       "      <td>0.847019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LOC3</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.766306</td>\n",
       "      <td>0.587604</td>\n",
       "      <td>0.450566</td>\n",
       "      <td>-0.191988</td>\n",
       "      <td>0.139999</td>\n",
       "      <td>0.059652</td>\n",
       "      <td>-0.088374</td>\n",
       "      <td>-0.502006</td>\n",
       "      <td>0.202204</td>\n",
       "      <td>0.409344</td>\n",
       "      <td>0.136938</td>\n",
       "      <td>-0.056980</td>\n",
       "      <td>-1.027741</td>\n",
       "      <td>-0.168223</td>\n",
       "      <td>0.373684</td>\n",
       "      <td>0.635023</td>\n",
       "      <td>0.209960</td>\n",
       "      <td>-1.100657</td>\n",
       "      <td>-0.280673</td>\n",
       "      <td>0.418120</td>\n",
       "      <td>0.678447</td>\n",
       "      <td>0.130366</td>\n",
       "      <td>-1.242744</td>\n",
       "      <td>-0.040002</td>\n",
       "      <td>0.338053</td>\n",
       "      <td>0.711234</td>\n",
       "      <td>0.404372</td>\n",
       "      <td>-1.133363</td>\n",
       "      <td>-0.015426</td>\n",
       "      <td>0.662883</td>\n",
       "      <td>0.665334</td>\n",
       "      <td>0.357631</td>\n",
       "      <td>-0.652759</td>\n",
       "      <td>0.061923</td>\n",
       "      <td>0.613452</td>\n",
       "      <td>0.908701</td>\n",
       "      <td>0.185938</td>\n",
       "      <td>-0.950471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067669</td>\n",
       "      <td>0.054917</td>\n",
       "      <td>0.481284</td>\n",
       "      <td>0.937721</td>\n",
       "      <td>1.429395</td>\n",
       "      <td>-1.173187</td>\n",
       "      <td>-0.799979</td>\n",
       "      <td>-0.089083</td>\n",
       "      <td>0.640041</td>\n",
       "      <td>0.250329</td>\n",
       "      <td>0.180261</td>\n",
       "      <td>0.256576</td>\n",
       "      <td>0.256388</td>\n",
       "      <td>1.025422</td>\n",
       "      <td>0.113975</td>\n",
       "      <td>0.122620</td>\n",
       "      <td>0.271398</td>\n",
       "      <td>0.287275</td>\n",
       "      <td>-0.206279</td>\n",
       "      <td>0.380618</td>\n",
       "      <td>0.868151</td>\n",
       "      <td>0.339152</td>\n",
       "      <td>0.614189</td>\n",
       "      <td>0.436115</td>\n",
       "      <td>0.373071</td>\n",
       "      <td>-0.088430</td>\n",
       "      <td>0.077884</td>\n",
       "      <td>0.269179</td>\n",
       "      <td>-3.149128</td>\n",
       "      <td>-0.430595</td>\n",
       "      <td>-0.060551</td>\n",
       "      <td>0.106505</td>\n",
       "      <td>0.349427</td>\n",
       "      <td>-0.139806</td>\n",
       "      <td>0.444249</td>\n",
       "      <td>-0.175151</td>\n",
       "      <td>0.478087</td>\n",
       "      <td>-0.527670</td>\n",
       "      <td>0.664492</td>\n",
       "      <td>0.641329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LOC3</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.798408</td>\n",
       "      <td>0.625453</td>\n",
       "      <td>0.548506</td>\n",
       "      <td>-0.216487</td>\n",
       "      <td>-0.008591</td>\n",
       "      <td>-0.085385</td>\n",
       "      <td>-0.188093</td>\n",
       "      <td>-0.392103</td>\n",
       "      <td>0.010027</td>\n",
       "      <td>0.478694</td>\n",
       "      <td>0.395822</td>\n",
       "      <td>0.024090</td>\n",
       "      <td>-1.052350</td>\n",
       "      <td>-0.219742</td>\n",
       "      <td>0.457356</td>\n",
       "      <td>0.586737</td>\n",
       "      <td>0.115925</td>\n",
       "      <td>-1.170633</td>\n",
       "      <td>-0.259916</td>\n",
       "      <td>0.451706</td>\n",
       "      <td>0.735578</td>\n",
       "      <td>0.280956</td>\n",
       "      <td>-1.288967</td>\n",
       "      <td>0.042407</td>\n",
       "      <td>0.586214</td>\n",
       "      <td>0.688393</td>\n",
       "      <td>0.390481</td>\n",
       "      <td>-1.152426</td>\n",
       "      <td>-0.007588</td>\n",
       "      <td>0.619316</td>\n",
       "      <td>0.784210</td>\n",
       "      <td>0.408216</td>\n",
       "      <td>-0.785375</td>\n",
       "      <td>0.049644</td>\n",
       "      <td>0.676895</td>\n",
       "      <td>0.895801</td>\n",
       "      <td>0.048266</td>\n",
       "      <td>-1.113967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162957</td>\n",
       "      <td>0.292017</td>\n",
       "      <td>0.668924</td>\n",
       "      <td>0.857247</td>\n",
       "      <td>0.497192</td>\n",
       "      <td>-0.313997</td>\n",
       "      <td>-0.270565</td>\n",
       "      <td>0.688754</td>\n",
       "      <td>1.077942</td>\n",
       "      <td>0.070908</td>\n",
       "      <td>0.183445</td>\n",
       "      <td>0.593851</td>\n",
       "      <td>0.332978</td>\n",
       "      <td>0.872942</td>\n",
       "      <td>0.609984</td>\n",
       "      <td>1.209190</td>\n",
       "      <td>0.866615</td>\n",
       "      <td>-1.260185</td>\n",
       "      <td>-0.691021</td>\n",
       "      <td>-0.205251</td>\n",
       "      <td>1.054963</td>\n",
       "      <td>0.940703</td>\n",
       "      <td>1.355417</td>\n",
       "      <td>-0.046931</td>\n",
       "      <td>0.262621</td>\n",
       "      <td>-0.976918</td>\n",
       "      <td>-0.031926</td>\n",
       "      <td>0.168713</td>\n",
       "      <td>-2.707478</td>\n",
       "      <td>-0.303959</td>\n",
       "      <td>-3.496777</td>\n",
       "      <td>0.008254</td>\n",
       "      <td>1.124100</td>\n",
       "      <td>-2.136851</td>\n",
       "      <td>-0.222633</td>\n",
       "      <td>-5.190058</td>\n",
       "      <td>-0.725888</td>\n",
       "      <td>-2.748366</td>\n",
       "      <td>-0.404062</td>\n",
       "      <td>0.763907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LOC3</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.737540</td>\n",
       "      <td>0.593112</td>\n",
       "      <td>0.341806</td>\n",
       "      <td>-0.155582</td>\n",
       "      <td>0.113799</td>\n",
       "      <td>0.060061</td>\n",
       "      <td>-0.101381</td>\n",
       "      <td>-0.403733</td>\n",
       "      <td>0.278271</td>\n",
       "      <td>0.379632</td>\n",
       "      <td>0.149379</td>\n",
       "      <td>-0.155203</td>\n",
       "      <td>-1.147017</td>\n",
       "      <td>-0.160847</td>\n",
       "      <td>0.406057</td>\n",
       "      <td>0.592908</td>\n",
       "      <td>0.210356</td>\n",
       "      <td>-1.225592</td>\n",
       "      <td>-0.235866</td>\n",
       "      <td>0.450849</td>\n",
       "      <td>0.608286</td>\n",
       "      <td>0.219798</td>\n",
       "      <td>-1.273395</td>\n",
       "      <td>-0.023650</td>\n",
       "      <td>0.398593</td>\n",
       "      <td>0.538564</td>\n",
       "      <td>0.457176</td>\n",
       "      <td>-1.041238</td>\n",
       "      <td>0.028675</td>\n",
       "      <td>0.561961</td>\n",
       "      <td>0.508286</td>\n",
       "      <td>0.051039</td>\n",
       "      <td>-0.700248</td>\n",
       "      <td>-0.048575</td>\n",
       "      <td>0.562713</td>\n",
       "      <td>0.566067</td>\n",
       "      <td>-0.050249</td>\n",
       "      <td>-1.090472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095939</td>\n",
       "      <td>0.076513</td>\n",
       "      <td>0.756069</td>\n",
       "      <td>0.616763</td>\n",
       "      <td>-0.004957</td>\n",
       "      <td>0.349419</td>\n",
       "      <td>0.102758</td>\n",
       "      <td>0.139234</td>\n",
       "      <td>0.167838</td>\n",
       "      <td>0.219118</td>\n",
       "      <td>0.240403</td>\n",
       "      <td>0.307171</td>\n",
       "      <td>0.160281</td>\n",
       "      <td>0.910829</td>\n",
       "      <td>0.344169</td>\n",
       "      <td>-0.018260</td>\n",
       "      <td>0.315400</td>\n",
       "      <td>-0.916335</td>\n",
       "      <td>-1.215678</td>\n",
       "      <td>0.417142</td>\n",
       "      <td>1.545992</td>\n",
       "      <td>0.568815</td>\n",
       "      <td>1.047525</td>\n",
       "      <td>0.392830</td>\n",
       "      <td>0.384393</td>\n",
       "      <td>0.028626</td>\n",
       "      <td>0.522895</td>\n",
       "      <td>0.520082</td>\n",
       "      <td>-3.075598</td>\n",
       "      <td>-0.487053</td>\n",
       "      <td>-0.112318</td>\n",
       "      <td>-0.100554</td>\n",
       "      <td>0.341356</td>\n",
       "      <td>-0.177396</td>\n",
       "      <td>0.352202</td>\n",
       "      <td>-0.238037</td>\n",
       "      <td>0.549598</td>\n",
       "      <td>-0.412714</td>\n",
       "      <td>0.670108</td>\n",
       "      <td>0.754958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LOC3</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.855277</td>\n",
       "      <td>0.675455</td>\n",
       "      <td>0.550887</td>\n",
       "      <td>-0.197418</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>-0.064831</td>\n",
       "      <td>-0.176556</td>\n",
       "      <td>-0.460890</td>\n",
       "      <td>0.172599</td>\n",
       "      <td>0.490847</td>\n",
       "      <td>0.330103</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>-1.085279</td>\n",
       "      <td>-0.217756</td>\n",
       "      <td>0.448733</td>\n",
       "      <td>0.606618</td>\n",
       "      <td>0.150438</td>\n",
       "      <td>-1.180803</td>\n",
       "      <td>-0.260345</td>\n",
       "      <td>0.465111</td>\n",
       "      <td>0.743883</td>\n",
       "      <td>0.257581</td>\n",
       "      <td>-1.254645</td>\n",
       "      <td>0.007607</td>\n",
       "      <td>0.553963</td>\n",
       "      <td>0.714393</td>\n",
       "      <td>0.410616</td>\n",
       "      <td>-1.202720</td>\n",
       "      <td>0.011333</td>\n",
       "      <td>0.628684</td>\n",
       "      <td>0.789626</td>\n",
       "      <td>0.433628</td>\n",
       "      <td>-0.854592</td>\n",
       "      <td>0.065184</td>\n",
       "      <td>0.673243</td>\n",
       "      <td>0.810441</td>\n",
       "      <td>0.132105</td>\n",
       "      <td>-1.062091</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013956</td>\n",
       "      <td>0.631792</td>\n",
       "      <td>0.669950</td>\n",
       "      <td>0.725456</td>\n",
       "      <td>0.530238</td>\n",
       "      <td>-0.114090</td>\n",
       "      <td>-0.025319</td>\n",
       "      <td>0.675771</td>\n",
       "      <td>0.709476</td>\n",
       "      <td>0.864298</td>\n",
       "      <td>1.036674</td>\n",
       "      <td>0.764475</td>\n",
       "      <td>0.342937</td>\n",
       "      <td>0.956494</td>\n",
       "      <td>0.543002</td>\n",
       "      <td>1.190438</td>\n",
       "      <td>0.873393</td>\n",
       "      <td>-1.061156</td>\n",
       "      <td>-1.189302</td>\n",
       "      <td>-0.383010</td>\n",
       "      <td>0.990566</td>\n",
       "      <td>0.920183</td>\n",
       "      <td>1.390804</td>\n",
       "      <td>0.333972</td>\n",
       "      <td>0.318681</td>\n",
       "      <td>-0.837075</td>\n",
       "      <td>0.075865</td>\n",
       "      <td>0.172705</td>\n",
       "      <td>-2.770092</td>\n",
       "      <td>-0.326505</td>\n",
       "      <td>-3.503678</td>\n",
       "      <td>0.051265</td>\n",
       "      <td>1.227194</td>\n",
       "      <td>-2.724601</td>\n",
       "      <td>-0.282898</td>\n",
       "      <td>-4.742453</td>\n",
       "      <td>-0.611562</td>\n",
       "      <td>-1.247955</td>\n",
       "      <td>0.387049</td>\n",
       "      <td>0.504077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LOC3</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.829935</td>\n",
       "      <td>0.628898</td>\n",
       "      <td>0.513886</td>\n",
       "      <td>-0.170691</td>\n",
       "      <td>0.006760</td>\n",
       "      <td>-0.067119</td>\n",
       "      <td>-0.164122</td>\n",
       "      <td>-0.383406</td>\n",
       "      <td>0.159794</td>\n",
       "      <td>0.447543</td>\n",
       "      <td>0.230367</td>\n",
       "      <td>0.018135</td>\n",
       "      <td>-1.027939</td>\n",
       "      <td>-0.189370</td>\n",
       "      <td>0.449231</td>\n",
       "      <td>0.553208</td>\n",
       "      <td>0.147651</td>\n",
       "      <td>-1.163404</td>\n",
       "      <td>-0.269337</td>\n",
       "      <td>0.461344</td>\n",
       "      <td>0.782107</td>\n",
       "      <td>0.129431</td>\n",
       "      <td>-1.173894</td>\n",
       "      <td>0.120176</td>\n",
       "      <td>0.595800</td>\n",
       "      <td>-1.520283</td>\n",
       "      <td>0.337679</td>\n",
       "      <td>-0.800160</td>\n",
       "      <td>-0.053638</td>\n",
       "      <td>0.533236</td>\n",
       "      <td>0.238177</td>\n",
       "      <td>0.004751</td>\n",
       "      <td>0.144198</td>\n",
       "      <td>0.203855</td>\n",
       "      <td>0.349798</td>\n",
       "      <td>-0.001641</td>\n",
       "      <td>-0.142392</td>\n",
       "      <td>-0.714649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159981</td>\n",
       "      <td>0.142303</td>\n",
       "      <td>0.082559</td>\n",
       "      <td>0.170494</td>\n",
       "      <td>0.063372</td>\n",
       "      <td>0.414470</td>\n",
       "      <td>0.282960</td>\n",
       "      <td>0.177743</td>\n",
       "      <td>0.100792</td>\n",
       "      <td>0.182722</td>\n",
       "      <td>0.157604</td>\n",
       "      <td>0.506384</td>\n",
       "      <td>0.198751</td>\n",
       "      <td>0.258800</td>\n",
       "      <td>0.140033</td>\n",
       "      <td>0.233748</td>\n",
       "      <td>0.653874</td>\n",
       "      <td>0.163026</td>\n",
       "      <td>-0.098195</td>\n",
       "      <td>0.400366</td>\n",
       "      <td>0.195282</td>\n",
       "      <td>0.460285</td>\n",
       "      <td>0.305948</td>\n",
       "      <td>0.003438</td>\n",
       "      <td>0.252506</td>\n",
       "      <td>-0.521492</td>\n",
       "      <td>0.186988</td>\n",
       "      <td>1.582721</td>\n",
       "      <td>-2.153129</td>\n",
       "      <td>-0.036084</td>\n",
       "      <td>0.068780</td>\n",
       "      <td>0.120734</td>\n",
       "      <td>1.734805</td>\n",
       "      <td>-2.069713</td>\n",
       "      <td>-0.001011</td>\n",
       "      <td>-0.122843</td>\n",
       "      <td>0.540268</td>\n",
       "      <td>-0.378211</td>\n",
       "      <td>0.604950</td>\n",
       "      <td>0.895409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LOC3</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.845961</td>\n",
       "      <td>0.666350</td>\n",
       "      <td>0.568376</td>\n",
       "      <td>-0.234621</td>\n",
       "      <td>0.004336</td>\n",
       "      <td>-0.098541</td>\n",
       "      <td>-0.176265</td>\n",
       "      <td>-0.452528</td>\n",
       "      <td>0.124684</td>\n",
       "      <td>0.515640</td>\n",
       "      <td>0.341084</td>\n",
       "      <td>0.029134</td>\n",
       "      <td>-1.062338</td>\n",
       "      <td>-0.218497</td>\n",
       "      <td>0.446740</td>\n",
       "      <td>0.570391</td>\n",
       "      <td>0.136750</td>\n",
       "      <td>-1.173686</td>\n",
       "      <td>-0.261330</td>\n",
       "      <td>0.456785</td>\n",
       "      <td>0.729846</td>\n",
       "      <td>0.294462</td>\n",
       "      <td>-1.261893</td>\n",
       "      <td>0.064716</td>\n",
       "      <td>0.602620</td>\n",
       "      <td>0.155930</td>\n",
       "      <td>0.357693</td>\n",
       "      <td>-0.937634</td>\n",
       "      <td>0.055495</td>\n",
       "      <td>0.462178</td>\n",
       "      <td>0.692026</td>\n",
       "      <td>0.428970</td>\n",
       "      <td>-0.654646</td>\n",
       "      <td>0.017727</td>\n",
       "      <td>0.654203</td>\n",
       "      <td>0.715431</td>\n",
       "      <td>-0.002544</td>\n",
       "      <td>-1.133998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191589</td>\n",
       "      <td>0.142967</td>\n",
       "      <td>0.611634</td>\n",
       "      <td>0.672311</td>\n",
       "      <td>0.347061</td>\n",
       "      <td>0.313783</td>\n",
       "      <td>0.402349</td>\n",
       "      <td>1.004715</td>\n",
       "      <td>0.788931</td>\n",
       "      <td>0.194019</td>\n",
       "      <td>0.174279</td>\n",
       "      <td>0.560433</td>\n",
       "      <td>0.345416</td>\n",
       "      <td>0.847962</td>\n",
       "      <td>-0.065219</td>\n",
       "      <td>0.721858</td>\n",
       "      <td>0.755356</td>\n",
       "      <td>-1.672557</td>\n",
       "      <td>-0.710290</td>\n",
       "      <td>0.661009</td>\n",
       "      <td>1.570547</td>\n",
       "      <td>0.772667</td>\n",
       "      <td>1.113802</td>\n",
       "      <td>0.470409</td>\n",
       "      <td>0.277520</td>\n",
       "      <td>-0.703118</td>\n",
       "      <td>0.325715</td>\n",
       "      <td>0.239554</td>\n",
       "      <td>-3.066325</td>\n",
       "      <td>-0.296897</td>\n",
       "      <td>-3.538377</td>\n",
       "      <td>-0.056002</td>\n",
       "      <td>0.977799</td>\n",
       "      <td>-1.491523</td>\n",
       "      <td>-0.051373</td>\n",
       "      <td>-4.632878</td>\n",
       "      <td>-0.606302</td>\n",
       "      <td>-2.722920</td>\n",
       "      <td>-0.421760</td>\n",
       "      <td>0.863308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LOC3</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.813898</td>\n",
       "      <td>0.612754</td>\n",
       "      <td>0.549751</td>\n",
       "      <td>-0.224825</td>\n",
       "      <td>0.014141</td>\n",
       "      <td>-0.053709</td>\n",
       "      <td>-0.155570</td>\n",
       "      <td>-0.407310</td>\n",
       "      <td>0.132471</td>\n",
       "      <td>0.467545</td>\n",
       "      <td>0.341571</td>\n",
       "      <td>0.006590</td>\n",
       "      <td>-1.040124</td>\n",
       "      <td>-0.194008</td>\n",
       "      <td>0.443037</td>\n",
       "      <td>0.593387</td>\n",
       "      <td>0.156483</td>\n",
       "      <td>-1.242509</td>\n",
       "      <td>-0.254757</td>\n",
       "      <td>0.472027</td>\n",
       "      <td>0.744180</td>\n",
       "      <td>0.223713</td>\n",
       "      <td>-1.388140</td>\n",
       "      <td>0.078359</td>\n",
       "      <td>0.570538</td>\n",
       "      <td>0.761254</td>\n",
       "      <td>0.415412</td>\n",
       "      <td>-1.213055</td>\n",
       "      <td>0.010969</td>\n",
       "      <td>0.630139</td>\n",
       "      <td>0.836805</td>\n",
       "      <td>0.445221</td>\n",
       "      <td>-0.804084</td>\n",
       "      <td>-0.008156</td>\n",
       "      <td>0.677372</td>\n",
       "      <td>0.653128</td>\n",
       "      <td>-0.053105</td>\n",
       "      <td>-1.055372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300462</td>\n",
       "      <td>0.178234</td>\n",
       "      <td>0.382086</td>\n",
       "      <td>0.549813</td>\n",
       "      <td>0.474523</td>\n",
       "      <td>0.477658</td>\n",
       "      <td>0.229826</td>\n",
       "      <td>0.859759</td>\n",
       "      <td>0.681915</td>\n",
       "      <td>0.325435</td>\n",
       "      <td>0.106025</td>\n",
       "      <td>0.499054</td>\n",
       "      <td>0.302874</td>\n",
       "      <td>0.934996</td>\n",
       "      <td>0.083288</td>\n",
       "      <td>0.648147</td>\n",
       "      <td>1.078757</td>\n",
       "      <td>0.149025</td>\n",
       "      <td>-0.917410</td>\n",
       "      <td>0.189455</td>\n",
       "      <td>1.311337</td>\n",
       "      <td>1.088739</td>\n",
       "      <td>0.680674</td>\n",
       "      <td>-1.752662</td>\n",
       "      <td>-0.018499</td>\n",
       "      <td>-0.670022</td>\n",
       "      <td>-0.211789</td>\n",
       "      <td>0.069373</td>\n",
       "      <td>-2.794219</td>\n",
       "      <td>-0.298904</td>\n",
       "      <td>-3.427058</td>\n",
       "      <td>-0.139064</td>\n",
       "      <td>1.028138</td>\n",
       "      <td>-1.387626</td>\n",
       "      <td>0.059915</td>\n",
       "      <td>-4.374053</td>\n",
       "      <td>-0.721706</td>\n",
       "      <td>-2.420316</td>\n",
       "      <td>-0.338516</td>\n",
       "      <td>0.792655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LOC3</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.813913</td>\n",
       "      <td>0.625432</td>\n",
       "      <td>0.548978</td>\n",
       "      <td>-0.200414</td>\n",
       "      <td>0.002776</td>\n",
       "      <td>-0.075892</td>\n",
       "      <td>-0.166270</td>\n",
       "      <td>-0.419664</td>\n",
       "      <td>0.096511</td>\n",
       "      <td>0.481509</td>\n",
       "      <td>0.326671</td>\n",
       "      <td>0.012867</td>\n",
       "      <td>-0.991755</td>\n",
       "      <td>-0.198910</td>\n",
       "      <td>0.437891</td>\n",
       "      <td>0.579154</td>\n",
       "      <td>0.159572</td>\n",
       "      <td>-1.177583</td>\n",
       "      <td>-0.260532</td>\n",
       "      <td>0.458563</td>\n",
       "      <td>0.719095</td>\n",
       "      <td>0.258184</td>\n",
       "      <td>-1.290498</td>\n",
       "      <td>0.069164</td>\n",
       "      <td>0.567205</td>\n",
       "      <td>0.314315</td>\n",
       "      <td>0.401546</td>\n",
       "      <td>-0.942643</td>\n",
       "      <td>0.032414</td>\n",
       "      <td>0.519347</td>\n",
       "      <td>0.725153</td>\n",
       "      <td>0.456174</td>\n",
       "      <td>-0.737347</td>\n",
       "      <td>0.059172</td>\n",
       "      <td>0.671116</td>\n",
       "      <td>0.702439</td>\n",
       "      <td>-0.054579</td>\n",
       "      <td>-1.123610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242279</td>\n",
       "      <td>0.174131</td>\n",
       "      <td>0.514426</td>\n",
       "      <td>0.712074</td>\n",
       "      <td>0.518010</td>\n",
       "      <td>0.564168</td>\n",
       "      <td>0.357348</td>\n",
       "      <td>0.852854</td>\n",
       "      <td>0.779544</td>\n",
       "      <td>0.321974</td>\n",
       "      <td>0.157014</td>\n",
       "      <td>0.450186</td>\n",
       "      <td>0.350827</td>\n",
       "      <td>1.043940</td>\n",
       "      <td>0.123460</td>\n",
       "      <td>0.820830</td>\n",
       "      <td>0.865931</td>\n",
       "      <td>-1.034976</td>\n",
       "      <td>-1.039819</td>\n",
       "      <td>-0.634455</td>\n",
       "      <td>0.825533</td>\n",
       "      <td>0.921585</td>\n",
       "      <td>1.358876</td>\n",
       "      <td>-0.173066</td>\n",
       "      <td>0.240710</td>\n",
       "      <td>-0.341695</td>\n",
       "      <td>0.195130</td>\n",
       "      <td>0.207626</td>\n",
       "      <td>-3.105893</td>\n",
       "      <td>-0.293228</td>\n",
       "      <td>-3.184176</td>\n",
       "      <td>-0.004520</td>\n",
       "      <td>1.165438</td>\n",
       "      <td>-2.389772</td>\n",
       "      <td>-0.205024</td>\n",
       "      <td>-4.917170</td>\n",
       "      <td>-0.662264</td>\n",
       "      <td>-2.789055</td>\n",
       "      <td>-0.489679</td>\n",
       "      <td>0.831497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LOC3</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.788128</td>\n",
       "      <td>0.608026</td>\n",
       "      <td>0.349360</td>\n",
       "      <td>-0.243803</td>\n",
       "      <td>0.041652</td>\n",
       "      <td>0.087600</td>\n",
       "      <td>0.007880</td>\n",
       "      <td>-0.257272</td>\n",
       "      <td>0.299879</td>\n",
       "      <td>0.165549</td>\n",
       "      <td>0.215007</td>\n",
       "      <td>-0.109452</td>\n",
       "      <td>0.647066</td>\n",
       "      <td>-2.312023</td>\n",
       "      <td>-0.902812</td>\n",
       "      <td>0.084588</td>\n",
       "      <td>0.748112</td>\n",
       "      <td>0.717386</td>\n",
       "      <td>0.506156</td>\n",
       "      <td>0.888167</td>\n",
       "      <td>0.281224</td>\n",
       "      <td>0.410316</td>\n",
       "      <td>0.832535</td>\n",
       "      <td>-0.136839</td>\n",
       "      <td>0.078407</td>\n",
       "      <td>-2.525258</td>\n",
       "      <td>-0.341504</td>\n",
       "      <td>0.387149</td>\n",
       "      <td>-2.030670</td>\n",
       "      <td>-0.280455</td>\n",
       "      <td>0.490394</td>\n",
       "      <td>0.846568</td>\n",
       "      <td>0.911747</td>\n",
       "      <td>-1.359836</td>\n",
       "      <td>-0.122578</td>\n",
       "      <td>0.596579</td>\n",
       "      <td>1.082062</td>\n",
       "      <td>0.785243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.481305</td>\n",
       "      <td>-1.071405</td>\n",
       "      <td>0.227809</td>\n",
       "      <td>0.957144</td>\n",
       "      <td>0.627185</td>\n",
       "      <td>0.229134</td>\n",
       "      <td>-0.811090</td>\n",
       "      <td>-0.216147</td>\n",
       "      <td>0.303099</td>\n",
       "      <td>0.447063</td>\n",
       "      <td>0.440721</td>\n",
       "      <td>0.555125</td>\n",
       "      <td>0.392524</td>\n",
       "      <td>0.727397</td>\n",
       "      <td>0.752214</td>\n",
       "      <td>0.553600</td>\n",
       "      <td>0.366821</td>\n",
       "      <td>0.572026</td>\n",
       "      <td>-2.233917</td>\n",
       "      <td>-0.661754</td>\n",
       "      <td>-2.824585</td>\n",
       "      <td>-0.832876</td>\n",
       "      <td>0.852657</td>\n",
       "      <td>1.081814</td>\n",
       "      <td>0.618992</td>\n",
       "      <td>0.158730</td>\n",
       "      <td>0.665249</td>\n",
       "      <td>1.385068</td>\n",
       "      <td>0.672060</td>\n",
       "      <td>-0.382909</td>\n",
       "      <td>-3.610699</td>\n",
       "      <td>-0.653535</td>\n",
       "      <td>1.012915</td>\n",
       "      <td>0.730188</td>\n",
       "      <td>-0.146338</td>\n",
       "      <td>0.050121</td>\n",
       "      <td>0.619095</td>\n",
       "      <td>0.220491</td>\n",
       "      <td>-0.022146</td>\n",
       "      <td>0.171690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LOC3</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.786940</td>\n",
       "      <td>0.596901</td>\n",
       "      <td>0.353160</td>\n",
       "      <td>-0.184259</td>\n",
       "      <td>0.102155</td>\n",
       "      <td>0.017045</td>\n",
       "      <td>-0.086427</td>\n",
       "      <td>-0.250292</td>\n",
       "      <td>0.202273</td>\n",
       "      <td>0.189406</td>\n",
       "      <td>0.168191</td>\n",
       "      <td>-0.089263</td>\n",
       "      <td>0.608823</td>\n",
       "      <td>-2.434456</td>\n",
       "      <td>-0.949490</td>\n",
       "      <td>-0.002451</td>\n",
       "      <td>0.848820</td>\n",
       "      <td>0.868472</td>\n",
       "      <td>0.594385</td>\n",
       "      <td>0.885350</td>\n",
       "      <td>0.331337</td>\n",
       "      <td>0.393036</td>\n",
       "      <td>0.857941</td>\n",
       "      <td>-0.044433</td>\n",
       "      <td>0.003317</td>\n",
       "      <td>-2.600870</td>\n",
       "      <td>-0.546944</td>\n",
       "      <td>0.294504</td>\n",
       "      <td>-1.853384</td>\n",
       "      <td>-0.185062</td>\n",
       "      <td>0.559557</td>\n",
       "      <td>0.925394</td>\n",
       "      <td>0.815358</td>\n",
       "      <td>-1.460897</td>\n",
       "      <td>-0.273814</td>\n",
       "      <td>0.418385</td>\n",
       "      <td>0.315471</td>\n",
       "      <td>0.631040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287699</td>\n",
       "      <td>-0.379101</td>\n",
       "      <td>0.322022</td>\n",
       "      <td>0.568119</td>\n",
       "      <td>0.275564</td>\n",
       "      <td>-0.185995</td>\n",
       "      <td>-2.341211</td>\n",
       "      <td>-0.025397</td>\n",
       "      <td>0.602713</td>\n",
       "      <td>0.220523</td>\n",
       "      <td>-0.206321</td>\n",
       "      <td>0.674646</td>\n",
       "      <td>0.811370</td>\n",
       "      <td>0.921400</td>\n",
       "      <td>0.429948</td>\n",
       "      <td>0.296189</td>\n",
       "      <td>0.129583</td>\n",
       "      <td>0.485754</td>\n",
       "      <td>-0.360125</td>\n",
       "      <td>-0.065809</td>\n",
       "      <td>-2.714014</td>\n",
       "      <td>-0.776269</td>\n",
       "      <td>0.762661</td>\n",
       "      <td>1.017664</td>\n",
       "      <td>0.842774</td>\n",
       "      <td>0.180651</td>\n",
       "      <td>0.172477</td>\n",
       "      <td>1.265431</td>\n",
       "      <td>0.746759</td>\n",
       "      <td>0.145118</td>\n",
       "      <td>-3.870807</td>\n",
       "      <td>-0.654025</td>\n",
       "      <td>0.961528</td>\n",
       "      <td>0.746370</td>\n",
       "      <td>0.569115</td>\n",
       "      <td>0.058441</td>\n",
       "      <td>0.446666</td>\n",
       "      <td>0.224796</td>\n",
       "      <td>0.279014</td>\n",
       "      <td>0.136608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LOC3</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.832698</td>\n",
       "      <td>0.638638</td>\n",
       "      <td>0.444528</td>\n",
       "      <td>-0.225370</td>\n",
       "      <td>0.052481</td>\n",
       "      <td>-0.036353</td>\n",
       "      <td>-0.129036</td>\n",
       "      <td>-0.252373</td>\n",
       "      <td>0.145485</td>\n",
       "      <td>0.257044</td>\n",
       "      <td>0.231667</td>\n",
       "      <td>-0.045810</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>-2.563305</td>\n",
       "      <td>-0.795112</td>\n",
       "      <td>0.155046</td>\n",
       "      <td>0.892122</td>\n",
       "      <td>0.736306</td>\n",
       "      <td>0.580086</td>\n",
       "      <td>0.894158</td>\n",
       "      <td>0.361421</td>\n",
       "      <td>0.380379</td>\n",
       "      <td>0.932863</td>\n",
       "      <td>-0.025946</td>\n",
       "      <td>-0.038522</td>\n",
       "      <td>-2.747205</td>\n",
       "      <td>-0.488486</td>\n",
       "      <td>0.315399</td>\n",
       "      <td>-2.016879</td>\n",
       "      <td>-0.237884</td>\n",
       "      <td>0.481015</td>\n",
       "      <td>0.885455</td>\n",
       "      <td>0.896239</td>\n",
       "      <td>-1.538070</td>\n",
       "      <td>-0.237204</td>\n",
       "      <td>0.527321</td>\n",
       "      <td>0.916244</td>\n",
       "      <td>0.677279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.819567</td>\n",
       "      <td>-0.526908</td>\n",
       "      <td>0.279543</td>\n",
       "      <td>0.866546</td>\n",
       "      <td>0.623857</td>\n",
       "      <td>0.156843</td>\n",
       "      <td>-2.357652</td>\n",
       "      <td>-0.382473</td>\n",
       "      <td>0.587230</td>\n",
       "      <td>0.944788</td>\n",
       "      <td>0.778702</td>\n",
       "      <td>0.857260</td>\n",
       "      <td>0.613766</td>\n",
       "      <td>0.833283</td>\n",
       "      <td>0.818534</td>\n",
       "      <td>0.954982</td>\n",
       "      <td>0.949268</td>\n",
       "      <td>0.599999</td>\n",
       "      <td>-2.708225</td>\n",
       "      <td>-0.468929</td>\n",
       "      <td>-3.014726</td>\n",
       "      <td>-0.570433</td>\n",
       "      <td>0.742570</td>\n",
       "      <td>1.080022</td>\n",
       "      <td>0.908569</td>\n",
       "      <td>-0.876647</td>\n",
       "      <td>-0.020666</td>\n",
       "      <td>1.165726</td>\n",
       "      <td>0.999879</td>\n",
       "      <td>0.226668</td>\n",
       "      <td>-3.436049</td>\n",
       "      <td>-0.435884</td>\n",
       "      <td>1.234133</td>\n",
       "      <td>1.045510</td>\n",
       "      <td>0.343615</td>\n",
       "      <td>-2.708252</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>0.953034</td>\n",
       "      <td>0.872468</td>\n",
       "      <td>0.863654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LOC3</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.807148</td>\n",
       "      <td>0.602333</td>\n",
       "      <td>0.379407</td>\n",
       "      <td>-0.177343</td>\n",
       "      <td>0.101338</td>\n",
       "      <td>-0.005519</td>\n",
       "      <td>-0.098709</td>\n",
       "      <td>-0.286785</td>\n",
       "      <td>0.256629</td>\n",
       "      <td>0.234226</td>\n",
       "      <td>0.207752</td>\n",
       "      <td>-0.069308</td>\n",
       "      <td>0.606633</td>\n",
       "      <td>-2.431191</td>\n",
       "      <td>-0.910074</td>\n",
       "      <td>0.090746</td>\n",
       "      <td>0.929188</td>\n",
       "      <td>0.800548</td>\n",
       "      <td>0.572647</td>\n",
       "      <td>0.870370</td>\n",
       "      <td>0.324873</td>\n",
       "      <td>0.407426</td>\n",
       "      <td>0.775001</td>\n",
       "      <td>-0.353810</td>\n",
       "      <td>-0.071174</td>\n",
       "      <td>-2.703899</td>\n",
       "      <td>-0.548183</td>\n",
       "      <td>0.327513</td>\n",
       "      <td>-1.981863</td>\n",
       "      <td>-0.209902</td>\n",
       "      <td>0.483023</td>\n",
       "      <td>0.951415</td>\n",
       "      <td>0.827104</td>\n",
       "      <td>-1.521109</td>\n",
       "      <td>-0.171382</td>\n",
       "      <td>0.589604</td>\n",
       "      <td>1.037294</td>\n",
       "      <td>0.690593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.659321</td>\n",
       "      <td>-1.180924</td>\n",
       "      <td>0.023788</td>\n",
       "      <td>0.872276</td>\n",
       "      <td>0.668988</td>\n",
       "      <td>0.281183</td>\n",
       "      <td>-2.282796</td>\n",
       "      <td>-0.307697</td>\n",
       "      <td>0.320010</td>\n",
       "      <td>0.206803</td>\n",
       "      <td>0.260941</td>\n",
       "      <td>0.737542</td>\n",
       "      <td>0.596943</td>\n",
       "      <td>0.701733</td>\n",
       "      <td>0.353865</td>\n",
       "      <td>0.539782</td>\n",
       "      <td>0.862241</td>\n",
       "      <td>0.574130</td>\n",
       "      <td>-2.551682</td>\n",
       "      <td>-0.555292</td>\n",
       "      <td>-3.258885</td>\n",
       "      <td>-0.649086</td>\n",
       "      <td>0.717792</td>\n",
       "      <td>1.112817</td>\n",
       "      <td>0.559210</td>\n",
       "      <td>-0.560750</td>\n",
       "      <td>0.769358</td>\n",
       "      <td>1.487916</td>\n",
       "      <td>0.819916</td>\n",
       "      <td>-0.366899</td>\n",
       "      <td>-4.109038</td>\n",
       "      <td>-0.409773</td>\n",
       "      <td>1.341380</td>\n",
       "      <td>0.890924</td>\n",
       "      <td>-1.327834</td>\n",
       "      <td>-0.197297</td>\n",
       "      <td>0.936905</td>\n",
       "      <td>0.615582</td>\n",
       "      <td>0.411099</td>\n",
       "      <td>0.812939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LOC3</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.792488</td>\n",
       "      <td>0.602065</td>\n",
       "      <td>0.370266</td>\n",
       "      <td>-0.200292</td>\n",
       "      <td>0.096943</td>\n",
       "      <td>-0.016764</td>\n",
       "      <td>-0.065824</td>\n",
       "      <td>-0.266014</td>\n",
       "      <td>0.207184</td>\n",
       "      <td>0.231714</td>\n",
       "      <td>0.208182</td>\n",
       "      <td>-0.089812</td>\n",
       "      <td>0.636491</td>\n",
       "      <td>-2.482465</td>\n",
       "      <td>-1.020826</td>\n",
       "      <td>0.007378</td>\n",
       "      <td>0.910182</td>\n",
       "      <td>0.760576</td>\n",
       "      <td>0.574061</td>\n",
       "      <td>0.895254</td>\n",
       "      <td>0.399697</td>\n",
       "      <td>0.491253</td>\n",
       "      <td>0.868666</td>\n",
       "      <td>-0.241759</td>\n",
       "      <td>-0.087392</td>\n",
       "      <td>-2.778738</td>\n",
       "      <td>-0.660980</td>\n",
       "      <td>0.292612</td>\n",
       "      <td>-1.943095</td>\n",
       "      <td>-0.223396</td>\n",
       "      <td>0.534326</td>\n",
       "      <td>0.901364</td>\n",
       "      <td>0.861466</td>\n",
       "      <td>-1.549832</td>\n",
       "      <td>-0.304628</td>\n",
       "      <td>0.534906</td>\n",
       "      <td>0.754055</td>\n",
       "      <td>0.679509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102890</td>\n",
       "      <td>-0.731044</td>\n",
       "      <td>0.279568</td>\n",
       "      <td>0.902269</td>\n",
       "      <td>0.605683</td>\n",
       "      <td>0.202912</td>\n",
       "      <td>-2.001293</td>\n",
       "      <td>-0.168707</td>\n",
       "      <td>0.567786</td>\n",
       "      <td>0.177209</td>\n",
       "      <td>0.109762</td>\n",
       "      <td>0.657543</td>\n",
       "      <td>0.822053</td>\n",
       "      <td>1.089965</td>\n",
       "      <td>0.520555</td>\n",
       "      <td>0.677983</td>\n",
       "      <td>0.802772</td>\n",
       "      <td>0.516557</td>\n",
       "      <td>-2.532003</td>\n",
       "      <td>-0.541349</td>\n",
       "      <td>-2.995020</td>\n",
       "      <td>-0.634218</td>\n",
       "      <td>0.722986</td>\n",
       "      <td>1.176560</td>\n",
       "      <td>0.739616</td>\n",
       "      <td>-0.741450</td>\n",
       "      <td>0.378544</td>\n",
       "      <td>1.426022</td>\n",
       "      <td>0.794610</td>\n",
       "      <td>-0.421776</td>\n",
       "      <td>-3.873676</td>\n",
       "      <td>-0.489366</td>\n",
       "      <td>1.245437</td>\n",
       "      <td>0.781204</td>\n",
       "      <td>0.076189</td>\n",
       "      <td>-0.087398</td>\n",
       "      <td>0.767137</td>\n",
       "      <td>0.354285</td>\n",
       "      <td>0.176951</td>\n",
       "      <td>0.150381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LOC3</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.829061</td>\n",
       "      <td>0.615938</td>\n",
       "      <td>0.426570</td>\n",
       "      <td>-0.192707</td>\n",
       "      <td>0.112201</td>\n",
       "      <td>-0.027643</td>\n",
       "      <td>-0.094267</td>\n",
       "      <td>-0.235544</td>\n",
       "      <td>0.077420</td>\n",
       "      <td>0.216634</td>\n",
       "      <td>0.201444</td>\n",
       "      <td>-0.074977</td>\n",
       "      <td>0.691550</td>\n",
       "      <td>-2.468813</td>\n",
       "      <td>-0.842430</td>\n",
       "      <td>0.097463</td>\n",
       "      <td>0.852748</td>\n",
       "      <td>0.737130</td>\n",
       "      <td>0.549529</td>\n",
       "      <td>0.904400</td>\n",
       "      <td>0.390800</td>\n",
       "      <td>0.431556</td>\n",
       "      <td>0.921118</td>\n",
       "      <td>0.026238</td>\n",
       "      <td>-0.023709</td>\n",
       "      <td>-2.630208</td>\n",
       "      <td>-0.461258</td>\n",
       "      <td>0.324460</td>\n",
       "      <td>-2.075092</td>\n",
       "      <td>-0.223877</td>\n",
       "      <td>0.550504</td>\n",
       "      <td>0.907003</td>\n",
       "      <td>0.955240</td>\n",
       "      <td>-1.479793</td>\n",
       "      <td>-0.190128</td>\n",
       "      <td>0.501874</td>\n",
       "      <td>0.542158</td>\n",
       "      <td>0.619900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.431274</td>\n",
       "      <td>-0.858366</td>\n",
       "      <td>0.128230</td>\n",
       "      <td>0.798061</td>\n",
       "      <td>0.595324</td>\n",
       "      <td>0.035203</td>\n",
       "      <td>-2.369323</td>\n",
       "      <td>-0.110060</td>\n",
       "      <td>0.291661</td>\n",
       "      <td>0.387144</td>\n",
       "      <td>-0.038148</td>\n",
       "      <td>0.648379</td>\n",
       "      <td>0.899788</td>\n",
       "      <td>0.910094</td>\n",
       "      <td>0.623845</td>\n",
       "      <td>0.579253</td>\n",
       "      <td>0.407232</td>\n",
       "      <td>0.676753</td>\n",
       "      <td>-0.295742</td>\n",
       "      <td>-0.367001</td>\n",
       "      <td>-2.035144</td>\n",
       "      <td>-0.393998</td>\n",
       "      <td>0.608490</td>\n",
       "      <td>1.005285</td>\n",
       "      <td>0.790941</td>\n",
       "      <td>-0.549252</td>\n",
       "      <td>0.160519</td>\n",
       "      <td>1.430402</td>\n",
       "      <td>0.805294</td>\n",
       "      <td>-0.603774</td>\n",
       "      <td>-3.725061</td>\n",
       "      <td>-0.608328</td>\n",
       "      <td>1.209455</td>\n",
       "      <td>0.877441</td>\n",
       "      <td>-0.202621</td>\n",
       "      <td>-0.032839</td>\n",
       "      <td>0.831547</td>\n",
       "      <td>0.106992</td>\n",
       "      <td>0.277957</td>\n",
       "      <td>0.549932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LOC3</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.873238</td>\n",
       "      <td>0.639100</td>\n",
       "      <td>0.521119</td>\n",
       "      <td>-0.049280</td>\n",
       "      <td>-0.011286</td>\n",
       "      <td>-0.104956</td>\n",
       "      <td>-0.181260</td>\n",
       "      <td>-0.346750</td>\n",
       "      <td>0.147180</td>\n",
       "      <td>0.445688</td>\n",
       "      <td>0.380669</td>\n",
       "      <td>0.069161</td>\n",
       "      <td>0.591313</td>\n",
       "      <td>-4.013765</td>\n",
       "      <td>-0.776891</td>\n",
       "      <td>0.257734</td>\n",
       "      <td>0.853943</td>\n",
       "      <td>0.734606</td>\n",
       "      <td>0.108009</td>\n",
       "      <td>0.691945</td>\n",
       "      <td>0.436905</td>\n",
       "      <td>0.498829</td>\n",
       "      <td>0.970265</td>\n",
       "      <td>-3.638098</td>\n",
       "      <td>-0.340891</td>\n",
       "      <td>-2.897728</td>\n",
       "      <td>-0.564532</td>\n",
       "      <td>0.188168</td>\n",
       "      <td>-3.108080</td>\n",
       "      <td>-0.240490</td>\n",
       "      <td>0.517368</td>\n",
       "      <td>0.899669</td>\n",
       "      <td>0.774175</td>\n",
       "      <td>-1.380008</td>\n",
       "      <td>-0.270807</td>\n",
       "      <td>0.547485</td>\n",
       "      <td>0.763486</td>\n",
       "      <td>0.635019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650095</td>\n",
       "      <td>-2.693246</td>\n",
       "      <td>-0.366251</td>\n",
       "      <td>0.664368</td>\n",
       "      <td>0.842871</td>\n",
       "      <td>0.580590</td>\n",
       "      <td>0.093693</td>\n",
       "      <td>0.744366</td>\n",
       "      <td>0.981908</td>\n",
       "      <td>0.852955</td>\n",
       "      <td>1.074404</td>\n",
       "      <td>1.104948</td>\n",
       "      <td>0.709826</td>\n",
       "      <td>-1.743230</td>\n",
       "      <td>0.023359</td>\n",
       "      <td>-0.072283</td>\n",
       "      <td>-0.817986</td>\n",
       "      <td>0.046291</td>\n",
       "      <td>-2.736810</td>\n",
       "      <td>-0.183499</td>\n",
       "      <td>1.167706</td>\n",
       "      <td>1.036043</td>\n",
       "      <td>0.640457</td>\n",
       "      <td>0.396851</td>\n",
       "      <td>0.332171</td>\n",
       "      <td>-1.952726</td>\n",
       "      <td>-0.343161</td>\n",
       "      <td>-0.064141</td>\n",
       "      <td>-2.729557</td>\n",
       "      <td>-0.163449</td>\n",
       "      <td>0.586472</td>\n",
       "      <td>1.094964</td>\n",
       "      <td>0.795141</td>\n",
       "      <td>-2.991162</td>\n",
       "      <td>-0.246742</td>\n",
       "      <td>-0.808050</td>\n",
       "      <td>0.709221</td>\n",
       "      <td>0.159547</td>\n",
       "      <td>-2.791585</td>\n",
       "      <td>0.127529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LOC3</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.873589</td>\n",
       "      <td>0.630943</td>\n",
       "      <td>0.513067</td>\n",
       "      <td>-0.071782</td>\n",
       "      <td>-0.011669</td>\n",
       "      <td>-0.090011</td>\n",
       "      <td>-0.179756</td>\n",
       "      <td>-0.325742</td>\n",
       "      <td>0.141412</td>\n",
       "      <td>0.423009</td>\n",
       "      <td>0.326017</td>\n",
       "      <td>0.075690</td>\n",
       "      <td>0.609731</td>\n",
       "      <td>-4.063999</td>\n",
       "      <td>-0.778452</td>\n",
       "      <td>0.245658</td>\n",
       "      <td>0.809624</td>\n",
       "      <td>0.759487</td>\n",
       "      <td>0.394856</td>\n",
       "      <td>0.767103</td>\n",
       "      <td>0.389768</td>\n",
       "      <td>0.462919</td>\n",
       "      <td>0.943000</td>\n",
       "      <td>-3.622888</td>\n",
       "      <td>-0.360389</td>\n",
       "      <td>-2.906832</td>\n",
       "      <td>-0.582539</td>\n",
       "      <td>0.236246</td>\n",
       "      <td>-3.005048</td>\n",
       "      <td>-0.261255</td>\n",
       "      <td>0.489927</td>\n",
       "      <td>0.903629</td>\n",
       "      <td>0.768499</td>\n",
       "      <td>-1.358549</td>\n",
       "      <td>-0.269450</td>\n",
       "      <td>0.557231</td>\n",
       "      <td>0.757950</td>\n",
       "      <td>0.648880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.611152</td>\n",
       "      <td>-2.778039</td>\n",
       "      <td>-0.367801</td>\n",
       "      <td>0.693818</td>\n",
       "      <td>0.877570</td>\n",
       "      <td>0.616377</td>\n",
       "      <td>0.144939</td>\n",
       "      <td>0.721472</td>\n",
       "      <td>1.036902</td>\n",
       "      <td>0.898715</td>\n",
       "      <td>1.131287</td>\n",
       "      <td>1.117717</td>\n",
       "      <td>0.683935</td>\n",
       "      <td>-1.598710</td>\n",
       "      <td>-0.054914</td>\n",
       "      <td>0.310173</td>\n",
       "      <td>-1.341179</td>\n",
       "      <td>-0.195828</td>\n",
       "      <td>-2.802448</td>\n",
       "      <td>-0.219844</td>\n",
       "      <td>1.123521</td>\n",
       "      <td>1.138498</td>\n",
       "      <td>0.730137</td>\n",
       "      <td>-0.456697</td>\n",
       "      <td>0.008380</td>\n",
       "      <td>-1.075248</td>\n",
       "      <td>-0.253057</td>\n",
       "      <td>0.042549</td>\n",
       "      <td>-3.056819</td>\n",
       "      <td>-0.319622</td>\n",
       "      <td>0.827292</td>\n",
       "      <td>1.127988</td>\n",
       "      <td>0.786830</td>\n",
       "      <td>-3.251450</td>\n",
       "      <td>-0.315667</td>\n",
       "      <td>-1.493590</td>\n",
       "      <td>0.437032</td>\n",
       "      <td>0.223944</td>\n",
       "      <td>-2.742046</td>\n",
       "      <td>0.183590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LOC3</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.868966</td>\n",
       "      <td>0.635162</td>\n",
       "      <td>0.494384</td>\n",
       "      <td>-0.058120</td>\n",
       "      <td>-0.005537</td>\n",
       "      <td>-0.077494</td>\n",
       "      <td>-0.178588</td>\n",
       "      <td>-0.310715</td>\n",
       "      <td>0.142485</td>\n",
       "      <td>0.415495</td>\n",
       "      <td>0.331012</td>\n",
       "      <td>0.079079</td>\n",
       "      <td>0.577429</td>\n",
       "      <td>-3.908619</td>\n",
       "      <td>-0.758279</td>\n",
       "      <td>0.217044</td>\n",
       "      <td>0.853563</td>\n",
       "      <td>0.753817</td>\n",
       "      <td>0.467231</td>\n",
       "      <td>0.786380</td>\n",
       "      <td>0.393111</td>\n",
       "      <td>0.459832</td>\n",
       "      <td>0.900340</td>\n",
       "      <td>-3.593221</td>\n",
       "      <td>-0.354707</td>\n",
       "      <td>-2.913905</td>\n",
       "      <td>-0.551601</td>\n",
       "      <td>0.176602</td>\n",
       "      <td>-3.195712</td>\n",
       "      <td>-0.260161</td>\n",
       "      <td>0.526282</td>\n",
       "      <td>0.936620</td>\n",
       "      <td>0.799184</td>\n",
       "      <td>-1.487397</td>\n",
       "      <td>-0.277996</td>\n",
       "      <td>0.551603</td>\n",
       "      <td>0.784582</td>\n",
       "      <td>0.697553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.633767</td>\n",
       "      <td>-2.622817</td>\n",
       "      <td>-0.368441</td>\n",
       "      <td>0.684060</td>\n",
       "      <td>0.850775</td>\n",
       "      <td>0.573756</td>\n",
       "      <td>0.286825</td>\n",
       "      <td>0.821968</td>\n",
       "      <td>0.845210</td>\n",
       "      <td>0.867671</td>\n",
       "      <td>1.028403</td>\n",
       "      <td>1.011698</td>\n",
       "      <td>0.648217</td>\n",
       "      <td>-1.127576</td>\n",
       "      <td>0.113491</td>\n",
       "      <td>0.196029</td>\n",
       "      <td>-1.423338</td>\n",
       "      <td>-0.146718</td>\n",
       "      <td>-2.769518</td>\n",
       "      <td>-0.239737</td>\n",
       "      <td>1.110076</td>\n",
       "      <td>1.097098</td>\n",
       "      <td>0.674365</td>\n",
       "      <td>0.007884</td>\n",
       "      <td>0.218181</td>\n",
       "      <td>-0.816630</td>\n",
       "      <td>-0.217657</td>\n",
       "      <td>-0.041692</td>\n",
       "      <td>-2.760434</td>\n",
       "      <td>-0.184402</td>\n",
       "      <td>0.917876</td>\n",
       "      <td>1.174355</td>\n",
       "      <td>0.851003</td>\n",
       "      <td>-3.214676</td>\n",
       "      <td>-0.295107</td>\n",
       "      <td>-1.055385</td>\n",
       "      <td>0.623507</td>\n",
       "      <td>-0.034327</td>\n",
       "      <td>-2.396649</td>\n",
       "      <td>0.240292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LOC3</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.864637</td>\n",
       "      <td>0.628949</td>\n",
       "      <td>0.506256</td>\n",
       "      <td>-0.067598</td>\n",
       "      <td>-0.013379</td>\n",
       "      <td>-0.083403</td>\n",
       "      <td>-0.174330</td>\n",
       "      <td>-0.309997</td>\n",
       "      <td>0.139799</td>\n",
       "      <td>0.414676</td>\n",
       "      <td>0.330790</td>\n",
       "      <td>0.079267</td>\n",
       "      <td>0.604877</td>\n",
       "      <td>-3.923458</td>\n",
       "      <td>-0.763149</td>\n",
       "      <td>0.208665</td>\n",
       "      <td>0.822774</td>\n",
       "      <td>0.746456</td>\n",
       "      <td>0.467243</td>\n",
       "      <td>0.787616</td>\n",
       "      <td>0.393839</td>\n",
       "      <td>0.445873</td>\n",
       "      <td>0.951972</td>\n",
       "      <td>-3.490092</td>\n",
       "      <td>-0.324635</td>\n",
       "      <td>-2.975912</td>\n",
       "      <td>-0.557053</td>\n",
       "      <td>0.191765</td>\n",
       "      <td>-3.111463</td>\n",
       "      <td>-0.257329</td>\n",
       "      <td>0.514749</td>\n",
       "      <td>0.921200</td>\n",
       "      <td>0.784205</td>\n",
       "      <td>-1.409909</td>\n",
       "      <td>-0.268707</td>\n",
       "      <td>0.537385</td>\n",
       "      <td>0.770050</td>\n",
       "      <td>0.669798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.657783</td>\n",
       "      <td>-2.707720</td>\n",
       "      <td>-0.350557</td>\n",
       "      <td>0.676131</td>\n",
       "      <td>0.833116</td>\n",
       "      <td>0.577027</td>\n",
       "      <td>0.013979</td>\n",
       "      <td>0.758757</td>\n",
       "      <td>1.091085</td>\n",
       "      <td>0.862487</td>\n",
       "      <td>0.985842</td>\n",
       "      <td>1.059721</td>\n",
       "      <td>0.472500</td>\n",
       "      <td>-2.021714</td>\n",
       "      <td>-0.234541</td>\n",
       "      <td>0.759292</td>\n",
       "      <td>-0.027739</td>\n",
       "      <td>0.179993</td>\n",
       "      <td>-2.788856</td>\n",
       "      <td>-0.214913</td>\n",
       "      <td>1.160623</td>\n",
       "      <td>1.011381</td>\n",
       "      <td>0.642176</td>\n",
       "      <td>0.588503</td>\n",
       "      <td>0.307611</td>\n",
       "      <td>-1.479895</td>\n",
       "      <td>-0.312370</td>\n",
       "      <td>0.052814</td>\n",
       "      <td>-2.796246</td>\n",
       "      <td>-0.309741</td>\n",
       "      <td>0.585196</td>\n",
       "      <td>1.101284</td>\n",
       "      <td>0.852743</td>\n",
       "      <td>-3.248742</td>\n",
       "      <td>-0.381149</td>\n",
       "      <td>-0.456397</td>\n",
       "      <td>0.923648</td>\n",
       "      <td>0.363335</td>\n",
       "      <td>-2.862642</td>\n",
       "      <td>0.069218</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Location  Website         0  ...       125       126       127\n",
       "0      LOC3        8 -0.801097  ... -0.678699  0.533888  0.704908\n",
       "1      LOC3        8 -0.815983  ... -0.562493  0.485425  0.847019\n",
       "2      LOC3        8 -0.766306  ... -0.527670  0.664492  0.641329\n",
       "3      LOC3        8 -0.798408  ... -2.748366 -0.404062  0.763907\n",
       "4      LOC3        8 -0.737540  ... -0.412714  0.670108  0.754958\n",
       "5      LOC3        8 -0.855277  ... -1.247955  0.387049  0.504077\n",
       "6      LOC3        8 -0.829935  ... -0.378211  0.604950  0.895409\n",
       "7      LOC3        8 -0.845961  ... -2.722920 -0.421760  0.863308\n",
       "8      LOC3        8 -0.813898  ... -2.420316 -0.338516  0.792655\n",
       "9      LOC3        8 -0.813913  ... -2.789055 -0.489679  0.831497\n",
       "10     LOC3        8 -0.788128  ...  0.220491 -0.022146  0.171690\n",
       "11     LOC3        8 -0.786940  ...  0.224796  0.279014  0.136608\n",
       "12     LOC3        8 -0.832698  ...  0.953034  0.872468  0.863654\n",
       "13     LOC3        8 -0.807148  ...  0.615582  0.411099  0.812939\n",
       "14     LOC3        8 -0.792488  ...  0.354285  0.176951  0.150381\n",
       "15     LOC3        8 -0.829061  ...  0.106992  0.277957  0.549932\n",
       "16     LOC3        8 -0.873238  ...  0.159547 -2.791585  0.127529\n",
       "17     LOC3        8 -0.873589  ...  0.223944 -2.742046  0.183590\n",
       "18     LOC3        8 -0.868966  ... -0.034327 -2.396649  0.240292\n",
       "19     LOC3        8 -0.864637  ...  0.363335 -2.862642  0.069218\n",
       "\n",
       "[20 rows x 130 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_df = pd.read_csv(f'../synthesized/{target_location}-VAE-Sampling.csv')\n",
    "synthetic_df['Location'] = target_location\n",
    "synthetic_df = synthetic_df[synthetic_df['Location'] == target_location]\n",
    "synthetic_df.sort_values(by=['Website'], inplace=True)\n",
    "synthetic_df.reset_index(drop=True, inplace=True)\n",
    "synthetic_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "# synthetic data\n",
    "X_train = synthetic_df.iloc[:, 2:]\n",
    "y_train = le.fit_transform(synthetic_df.Website)\n",
    "X_test = target_df.iloc[:, 2:]\n",
    "y_test = le.transform(target_df.Website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e19372/anaconda3/envs/doh_synth_env/lib/python3.11/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = build_classifier(input_dim=length, hidden_dim=96, num_classes=len(test_web_samples))\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m9282/9282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 5ms/step - accuracy: 0.8589 - loss: 0.4479\n",
      "Epoch 2/10\n",
      "\u001b[1m9282/9282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 5ms/step - accuracy: 0.8598 - loss: 0.4434\n",
      "Epoch 3/10\n",
      "\u001b[1m9282/9282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 5ms/step - accuracy: 0.8597 - loss: 0.4383\n",
      "Epoch 4/10\n",
      "\u001b[1m9282/9282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 5ms/step - accuracy: 0.8626 - loss: 0.4349\n",
      "Epoch 5/10\n",
      "\u001b[1m9282/9282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 5ms/step - accuracy: 0.8623 - loss: 0.4311\n",
      "Epoch 6/10\n",
      "\u001b[1m9282/9282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 5ms/step - accuracy: 0.8658 - loss: 0.4242\n",
      "Epoch 7/10\n",
      "\u001b[1m9282/9282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 5ms/step - accuracy: 0.8671 - loss: 0.4193\n",
      "Epoch 8/10\n",
      "\u001b[1m9282/9282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 5ms/step - accuracy: 0.8673 - loss: 0.4172\n",
      "Epoch 9/10\n",
      "\u001b[1m9282/9282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 4ms/step - accuracy: 0.8692 - loss: 0.4095\n",
      "Epoch 10/10\n",
      "\u001b[1m9282/9282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 5ms/step - accuracy: 0.8711 - loss: 0.4079\n",
      "\u001b[1m929/929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "Accuracy: 0.1305\n",
      "F1 Score: 0.1191\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=10, shuffle=True)\n",
    "\n",
    "# Get logits from model prediction\n",
    "logits = model.predict(X_test)\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probabilities = tf.nn.softmax(logits).numpy()\n",
    "\n",
    "# Get predicted class by selecting the class with highest probability\n",
    "y_pred = np.argmax(probabilities, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')  # Use weighted average for imbalanced data\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "show_confusion_matrix_heatmap() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Compute the confusion matrix\u001b[39;00m\n\u001b[1;32m      5\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(y_test, y_pred)\n\u001b[0;32m----> 7\u001b[0m classification\u001b[38;5;241m.\u001b[39mshow_confusion_matrix_heatmap(cm, le, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfusion Matrix (Trained Completely on Synthetic Data)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: show_confusion_matrix_heatmap() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import scripts.classification as classification\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "classification.show_confusion_matrix_heatmap(cm, le, \"Confusion Matrix (Trained Completely on Synthetic Data)\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Real Target Data, Test on Target Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e19372/anaconda3/envs/doh_synth_env/lib/python3.11/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.0511 - loss: 5.3974\n",
      "Epoch 2/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3679 - loss: 2.8444\n",
      "Epoch 3/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6101 - loss: 1.6643\n",
      "Epoch 4/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7580 - loss: 1.0256\n",
      "Epoch 5/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8171 - loss: 0.7288\n",
      "Epoch 6/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8566 - loss: 0.5322\n",
      "Epoch 7/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8702 - loss: 0.4805\n",
      "Epoch 8/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8864 - loss: 0.4011\n",
      "Epoch 9/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8990 - loss: 0.3458\n",
      "Epoch 10/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9139 - loss: 0.2829\n",
      "Epoch 11/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9142 - loss: 0.2827\n",
      "Epoch 12/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9172 - loss: 0.2718\n",
      "Epoch 13/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9242 - loss: 0.2468\n",
      "Epoch 14/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9337 - loss: 0.2247\n",
      "Epoch 15/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9340 - loss: 0.2036\n",
      "Epoch 16/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9429 - loss: 0.1833\n",
      "Epoch 17/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9342 - loss: 0.2031\n",
      "Epoch 18/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9452 - loss: 0.1700\n",
      "Epoch 19/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9305 - loss: 0.2087\n",
      "Epoch 20/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9394 - loss: 0.1790\n",
      "Epoch 21/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9396 - loss: 0.1825\n",
      "Epoch 22/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9443 - loss: 0.1722\n",
      "Epoch 23/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9405 - loss: 0.1780\n",
      "Epoch 24/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9411 - loss: 0.1757\n",
      "Epoch 25/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9484 - loss: 0.1588\n",
      "Epoch 26/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9543 - loss: 0.1365\n",
      "Epoch 27/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9565 - loss: 0.1435\n",
      "Epoch 28/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9509 - loss: 0.1440\n",
      "Epoch 29/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9411 - loss: 0.1824\n",
      "Epoch 30/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9618 - loss: 0.1087\n",
      "Epoch 31/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9579 - loss: 0.1273\n",
      "Epoch 32/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9569 - loss: 0.1503\n",
      "Epoch 33/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9573 - loss: 0.1327\n",
      "Epoch 34/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9576 - loss: 0.1255\n",
      "Epoch 35/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9627 - loss: 0.1131\n",
      "Epoch 36/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9524 - loss: 0.1424\n",
      "Epoch 37/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9568 - loss: 0.1194\n",
      "Epoch 38/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9592 - loss: 0.1149\n",
      "Epoch 39/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9564 - loss: 0.1289\n",
      "Epoch 40/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9659 - loss: 0.1011\n",
      "Epoch 41/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9599 - loss: 0.1199\n",
      "Epoch 42/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9540 - loss: 0.1350\n",
      "Epoch 43/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9583 - loss: 0.1162\n",
      "Epoch 44/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9601 - loss: 0.1239\n",
      "Epoch 45/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9574 - loss: 0.1231\n",
      "Epoch 46/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9580 - loss: 0.1123\n",
      "Epoch 47/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9549 - loss: 0.1253\n",
      "Epoch 48/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9553 - loss: 0.1284\n",
      "Epoch 49/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9602 - loss: 0.1118\n",
      "Epoch 50/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9636 - loss: 0.1062\n",
      "Epoch 51/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9632 - loss: 0.1109\n",
      "Epoch 52/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9683 - loss: 0.0938\n",
      "Epoch 53/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9673 - loss: 0.0885\n",
      "Epoch 54/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9627 - loss: 0.1063\n",
      "Epoch 55/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9698 - loss: 0.0852\n",
      "Epoch 56/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9650 - loss: 0.0889\n",
      "Epoch 57/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9634 - loss: 0.1033\n",
      "Epoch 58/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9612 - loss: 0.1128\n",
      "Epoch 59/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9599 - loss: 0.1041\n",
      "Epoch 60/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9671 - loss: 0.0988\n",
      "Epoch 61/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9659 - loss: 0.0808\n",
      "Epoch 62/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9692 - loss: 0.0826\n",
      "Epoch 63/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9681 - loss: 0.0976\n",
      "Epoch 64/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9660 - loss: 0.0882\n",
      "Epoch 65/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9627 - loss: 0.0994\n",
      "Epoch 66/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9720 - loss: 0.0784\n",
      "Epoch 67/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9679 - loss: 0.0916\n",
      "Epoch 68/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9737 - loss: 0.0784\n",
      "Epoch 69/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9654 - loss: 0.0985\n",
      "Epoch 70/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9652 - loss: 0.0901\n",
      "Epoch 71/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9731 - loss: 0.0755\n",
      "Epoch 72/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9633 - loss: 0.1073\n",
      "Epoch 73/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9725 - loss: 0.0808\n",
      "Epoch 74/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9624 - loss: 0.0912\n",
      "Epoch 75/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9676 - loss: 0.0889\n",
      "Epoch 76/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9744 - loss: 0.0662\n",
      "Epoch 77/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9685 - loss: 0.0842\n",
      "Epoch 78/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9624 - loss: 0.1201\n",
      "Epoch 79/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9755 - loss: 0.0634\n",
      "Epoch 80/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9697 - loss: 0.0811\n",
      "Epoch 81/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9752 - loss: 0.0751\n",
      "Epoch 82/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9731 - loss: 0.0689\n",
      "Epoch 83/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9716 - loss: 0.0740\n",
      "Epoch 84/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9739 - loss: 0.0733\n",
      "Epoch 85/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9723 - loss: 0.0808\n",
      "Epoch 86/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9757 - loss: 0.0648\n",
      "Epoch 87/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9734 - loss: 0.0721\n",
      "Epoch 88/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9740 - loss: 0.0703\n",
      "Epoch 89/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9739 - loss: 0.0714\n",
      "Epoch 90/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9765 - loss: 0.0570\n",
      "Epoch 91/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9676 - loss: 0.0951\n",
      "Epoch 92/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9684 - loss: 0.0776\n",
      "Epoch 93/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9689 - loss: 0.0877\n",
      "Epoch 94/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9801 - loss: 0.0542\n",
      "Epoch 95/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9768 - loss: 0.0606\n",
      "Epoch 96/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9754 - loss: 0.0664\n",
      "Epoch 97/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9739 - loss: 0.0770\n",
      "Epoch 98/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9739 - loss: 0.0696\n",
      "Epoch 99/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9775 - loss: 0.0612\n",
      "Epoch 100/100\n",
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9783 - loss: 0.0580\n",
      "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "Accuracy: 0.8970\n",
      "F1 Score: 0.8938\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "random_state = 42\n",
    "target_train, target_test = train_test_split(target_df, test_size=0.8, random_state=random_state, stratify=target_df['Website'])\n",
    "\n",
    "X_train = target_train.iloc[:, 2:]\n",
    "y_train = le.fit_transform(target_train.Website)\n",
    "X_test = target_test.iloc[:, 2:]\n",
    "y_test = le.transform(target_test.Website)\n",
    "\n",
    "model = build_classifier(input_dim=length, hidden_dim=96, num_classes=len(test_web_samples))\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=100, shuffle=True)\n",
    "\n",
    "# Get logits from model prediction\n",
    "logits = model.predict(X_test)\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probabilities = tf.nn.softmax(logits).numpy()\n",
    "\n",
    "# Get predicted class by selecting the class with highest probability\n",
    "y_pred = np.argmax(probabilities, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')  # Use weighted average for imbalanced data\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Partially on Synthetic Data\n",
    "Real data + Lot of synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e19372/anaconda3/envs/doh_synth_env/lib/python3.11/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m18749/18749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 5ms/step - accuracy: 0.4674 - loss: 2.1604\n",
      "Epoch 2/20\n",
      "\u001b[1m18749/18749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 5ms/step - accuracy: 0.6639 - loss: 1.1940\n",
      "Epoch 3/20\n",
      "\u001b[1m18749/18749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 5ms/step - accuracy: 0.7103 - loss: 0.9978\n",
      "Epoch 4/20\n",
      "\u001b[1m18749/18749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 5ms/step - accuracy: 0.7411 - loss: 0.8785\n",
      "Epoch 5/20\n",
      "\u001b[1m18749/18749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 5ms/step - accuracy: 0.7610 - loss: 0.8017\n",
      "Epoch 6/20\n",
      "\u001b[1m18749/18749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 5ms/step - accuracy: 0.7760 - loss: 0.7436\n",
      "Epoch 7/20\n",
      "\u001b[1m18749/18749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 5ms/step - accuracy: 0.7880 - loss: 0.6998\n",
      "Epoch 8/20\n",
      "\u001b[1m18749/18749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 5ms/step - accuracy: 0.7973 - loss: 0.6652\n",
      "Epoch 9/20\n",
      "\u001b[1m18749/18749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 4ms/step - accuracy: 0.8057 - loss: 0.6347\n",
      "Epoch 10/20\n",
      "\u001b[1m18749/18749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 4ms/step - accuracy: 0.8122 - loss: 0.6083\n",
      "Epoch 11/20\n",
      "\u001b[1m18749/18749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 5ms/step - accuracy: 0.8194 - loss: 0.5863\n",
      "Epoch 12/20\n",
      "\u001b[1m18749/18749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 5ms/step - accuracy: 0.8236 - loss: 0.5698\n",
      "Epoch 13/20\n",
      "\u001b[1m18749/18749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 5ms/step - accuracy: 0.8284 - loss: 0.5507\n",
      "Epoch 14/20\n",
      "\u001b[1m18749/18749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 5ms/step - accuracy: 0.8335 - loss: 0.5345\n",
      "Epoch 15/20\n",
      "\u001b[1m18749/18749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 5ms/step - accuracy: 0.8373 - loss: 0.5217\n",
      "Epoch 16/20\n",
      "\u001b[1m18749/18749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 4ms/step - accuracy: 0.8407 - loss: 0.5083\n",
      "Epoch 17/20\n",
      "\u001b[1m18749/18749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 5ms/step - accuracy: 0.8441 - loss: 0.4959\n",
      "Epoch 18/20\n",
      "\u001b[1m18749/18749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 5ms/step - accuracy: 0.8470 - loss: 0.4861\n",
      "Epoch 19/20\n",
      "\u001b[1m18749/18749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 5ms/step - accuracy: 0.8507 - loss: 0.4741\n",
      "Epoch 20/20\n",
      "\u001b[1m18749/18749\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 5ms/step - accuracy: 0.8523 - loss: 0.4689\n",
      "\u001b[1m743/743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "Accuracy: 0.5668\n",
      "F1 Score: 0.5585\n"
     ]
    }
   ],
   "source": [
    "# Combine X_train with synthetic data\n",
    "X_train = pd.concat([X_train, synthetic_df.iloc[:, 2:]], ignore_index=True)\n",
    "\n",
    "# Combine y_train with synthetic data\n",
    "y_train = np.concatenate([y_train, le.transform(synthetic_df.Website)])\n",
    "\n",
    "model = build_classifier(input_dim=length, hidden_dim=96, num_classes=len(test_web_samples))\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=20, shuffle=True)\n",
    "\n",
    "# Get logits from model prediction\n",
    "logits = model.predict(X_test)\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probabilities = tf.nn.softmax(logits).numpy()\n",
    "\n",
    "# Get predicted class by selecting the class with highest probability\n",
    "y_pred = np.argmax(probabilities, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')  # Use weighted average for imbalanced data\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available websites: [394, 951, 885, 591, 1117, 991, 187, 1013, 1416, 705, 874, 1315, 510, 347, 876, 588, 785, 884, 992, 213, 325, 126, 1425, 562, 1095, 581, 1097, 1400, 1102, 1348, 106, 558, 794, 99, 872, 307, 373, 554, 762, 1296, 814, 597, 182, 128, 927, 1114, 444, 1065, 653, 1446, 238, 1176, 556, 846, 623, 760, 945, 353, 184, 420, 1264, 847, 359, 441, 782, 288, 789, 419, 520, 780, 961, 482, 297, 111, 11, 979, 112, 813, 434, 948, 137, 933, 515, 383, 1077, 754, 1413, 1193, 544, 372, 835, 787, 385, 550, 473, 733, 602, 158, 181, 491, 1162, 573, 1442, 1003, 315, 133, 243, 1286, 37, 1041, 1084, 1358, 1294, 1037, 397, 36, 381, 190, 795, 950, 1229, 391, 1331, 690, 310, 346, 214, 1441, 1164, 494, 44, 1141, 1281, 1401, 1244, 983, 15, 91, 675, 1289, 531, 374, 84, 635, 1113, 1076, 616, 1143, 845, 964, 1341, 720, 1305, 1094, 49, 669, 1249, 1008, 1230, 654, 34, 1297, 526, 1211, 8, 706, 547, 640, 917, 682, 1319, 1349, 827, 80, 1028, 1227, 575, 351, 1409, 915, 998, 722, 1311, 405, 1299, 805, 952, 966, 1067, 1212, 632, 207, 107, 738, 498, 402, 157, 1489, 1465, 68, 673, 715, 1033, 990, 1152, 401, 342, 854, 1056, 413, 1254, 132, 742, 766, 701, 1359, 1068, 1062, 549, 1292, 729, 368, 474, 100, 94, 879, 1432, 235, 1303, 321, 709, 869, 1032, 608, 386, 1082, 1291, 57, 1030, 1239, 793, 485, 294, 252, 739, 718, 98, 1494, 134, 708]\n",
      "Missing websites: [513, 899, 1156, 9, 396, 655, 1168, 657, 400, 916, 790, 24, 1181, 925, 32, 1184, 1443, 1321, 172, 686, 1464, 954, 1467, 1340, 1470, 1215, 192, 454, 71, 1224, 839, 969, 1357, 463, 977, 980, 1365, 993, 1122, 354, 1001, 1002, 492, 1007, 113, 1142, 1014, 376, 1145, 1276]\n",
      "Trained partially on the synthesized data: \n",
      "Accuracy: 75.22, F1 Score:  71.70, Precision:  70.22, Recall:  75.22\n",
      "Trained partially on the data from another location: \n",
      "Accuracy: 82.78, F1 Score:  84.06, Precision:  88.72, Recall:  82.78\n",
      "Available websites: [1095, 952, 847, 401, 992, 37, 391, 1003, 158, 789, 1432, 948, 635, 1067, 494, 84, 294, 581, 951, 1164, 36, 207, 383, 1494, 964, 961, 1152, 1340, 1348, 754, 1358, 359, 1315, 554, 1311, 49, 794, 588, 1357, 243, 722, 1145, 1305, 575, 1141, 397, 653, 128, 1441, 793, 1321, 795, 1184, 1465, 1286, 307, 405, 1007, 1443, 1264, 492, 491, 854, 134, 869, 1068, 1467, 813, 876, 835, 1008, 1244, 550, 790, 396, 182, 213, 1168, 8, 1401, 917, 510, 1113, 1162, 1013, 1229, 1102, 1114, 1076, 1084, 547, 632, 342, 1062, 1014, 954, 739, 482, 1409, 1033, 1442, 137, 372, 1294, 927, 925, 916, 872, 1001, 884, 297, 1296, 68, 315, 701, 15, 686, 657, 1489, 608, 190, 1292, 434, 1212, 94, 1249, 98, 376, 709, 879, 1176, 760, 827, 1215, 402, 669, 1230, 733, 181, 325, 80, 1291, 874, 1122, 1041, 1227, 157, 235, 1470, 1239, 485, 386, 1425, 785, 112, 192, 214, 57, 1211, 899, 91, 966, 1446, 520, 690, 780, 675, 1299, 1289, 814, 1181, 473, 1002, 44, 1030, 573, 708, 1037, 1297, 655, 133, 1341, 729, 34, 544, 1400, 1077, 787, 381, 738, 420, 346, 915, 654, 845, 1416, 126, 513, 526, 1028, 945, 1065, 979, 113, 1331, 998, 107, 374, 531, 394, 558, 1082, 24, 805, 187, 597, 977, 288, 1117, 1032, 385, 100, 990, 111, 933, 616, 1142, 1156, 705, 238, 885, 1193, 71, 252, 498, 1254, 706, 846, 419, 1319, 99, 562, 673, 591, 172, 782, 515, 1056, 106, 400]\n",
      "Missing websites: [640, 1281, 132, 1413, 9, 11, 1303, 413, 32, 549, 682, 556, 310, 950, 184, 441, 1464, 444, 321, 1349, 1094, 839, 1224, 1097, 454, 715, 969, 718, 1359, 720, 463, 980, 1365, 983, 602, 347, 474, 351, 991, 353, 354, 993, 742, 623, 368, 373, 1143, 762, 1276, 766]\n",
      "Trained partially on the synthesized data: \n",
      "Accuracy: 74.62, F1 Score:  70.85, Precision:  69.40, Recall:  74.62\n",
      "Trained partially on the data from another location: \n",
      "Accuracy: 82.28, F1 Score:  83.48, Precision:  88.61, Recall:  82.28\n",
      "Available websites: [1358, 325, 722, 1032, 1331, 562, 1319, 1227, 597, 1305, 111, 549, 814, 473, 1315, 653, 402, 925, 657, 112, 1068, 397, 34, 766, 682, 1001, 1193, 37, 1297, 1294, 444, 346, 1425, 1013, 1494, 669, 760, 1008, 1056, 795, 742, 100, 321, 381, 347, 1348, 701, 1413, 413, 1181, 137, 1097, 720, 297, 1441, 1176, 401, 434, 1239, 1002, 351, 463, 1340, 182, 36, 187, 1291, 872, 1095, 952, 632, 132, 554, 400, 133, 441, 1443, 126, 1212, 396, 310, 184, 68, 494, 190, 993, 1400, 526, 992, 654, 782, 1211, 113, 1299, 854, 531, 547, 1311, 353, 979, 990, 214, 1014, 482, 573, 11, 1077, 780, 847, 1446, 498, 485, 544, 608, 790, 1359, 1168, 420, 243, 835, 885, 1084, 1357, 192, 933, 157, 372, 964, 454, 1076, 733, 315, 675, 640, 623, 951, 556, 520, 1442, 991, 917, 673, 1264, 558, 107, 106, 879, 1113, 1142, 718, 80, 1067, 1007, 1467, 373, 1094, 927, 1489, 1152, 1164, 32, 739, 1143, 1145, 846, 754, 376, 213, 1114, 252, 1276, 491, 874, 793, 789, 1030, 1416, 354, 1281, 969, 94, 977, 715, 391, 1401, 359, 15, 998, 1321, 591, 655, 729, 98, 1349, 84, 1464, 686, 1224, 1033, 705, 383, 1409, 207, 510, 708, 1162, 813, 1244, 1184, 706, 235, 948, 550, 884, 602, 1122, 845, 24, 794, 288, 1082, 915, 386, 575, 44, 1432, 368, 839, 876, 950, 1062, 1230, 419, 128, 1289, 134, 307, 57, 785, 99, 1465, 827, 374, 869, 1003, 1215, 980, 1141, 1028, 515]\n",
      "Missing websites: [513, 385, 899, 1156, 1286, 8, 9, 394, 1292, 1037, 1296, 1041, 787, 916, 405, 1303, 158, 805, 294, 1065, 172, 49, 690, 945, 181, 954, 1341, 1470, 961, 581, 709, 71, 966, 588, 1229, 1102, 1365, 342, 983, 474, 91, 1117, 1249, 738, 1254, 616, 492, 238, 762, 635]\n",
      "Trained partially on the synthesized data: \n",
      "Accuracy: 74.33, F1 Score:  70.86, Precision:  69.54, Recall:  74.33\n",
      "Trained partially on the data from another location: \n",
      "Accuracy: 83.31, F1 Score:  84.76, Precision:  89.53, Recall:  83.31\n",
      "Available websites: [575, 550, 1008, 733, 181, 1003, 879, 24, 945, 473, 252, 1291, 1441, 718, 354, 1286, 394, 57, 558, 640, 385, 876, 998, 402, 172, 805, 1142, 785, 1299, 1319, 347, 106, 835, 444, 1113, 969, 1227, 915, 99, 1249, 391, 207, 1141, 1102, 1062, 1143, 1425, 1181, 1400, 782, 1013, 100, 927, 933, 952, 573, 869, 133, 916, 762, 315, 474, 588, 846, 413, 720, 214, 708, 397, 491, 126, 1416, 9, 1014, 1184, 722, 706, 98, 602, 1276, 654, 1037, 884, 950, 951, 993, 793, 288, 709, 977, 738, 1028, 1065, 187, 562, 107, 1340, 657, 401, 682, 854, 15, 1193, 1041, 243, 1264, 948, 1168, 235, 872, 34, 1443, 383, 192, 1254, 623, 1296, 1489, 1292, 990, 1068, 1056, 925, 157, 137, 238, 1359, 1077, 591, 1001, 213, 1357, 1230, 526, 1358, 1033, 1076, 1156, 1224, 1239, 705, 1082, 780, 396, 1465, 1446, 1152, 510, 297, 1145, 294, 1341, 795, 441, 485, 754, 766, 1297, 8, 1176, 134, 44, 690, 1413, 520, 1331, 454, 1117, 847, 715, 581, 669, 991, 686, 1162, 1067, 325, 1348, 1244, 653, 531, 1321, 1432, 91, 400, 597, 547, 84, 1164, 554, 701, 1122, 673, 760, 608, 827, 1315, 11, 376, 544, 964, 353, 917, 368, 742, 1401, 729, 980, 346, 549, 463, 111, 1002, 112, 71, 132, 49, 359, 979, 1215, 1229, 420, 494, 1294, 739, 845, 983, 321, 954, 655, 1289, 1094, 966, 405, 1007, 1349, 1311, 184, 675, 839, 80, 789, 961, 381, 1114, 787, 899, 885, 632, 158]\n",
      "Missing websites: [128, 513, 1281, 515, 386, 1409, 1030, 1032, 790, 1303, 1305, 794, 32, 1442, 419, 36, 37, 556, 813, 814, 434, 307, 182, 310, 1464, 1211, 1084, 1212, 190, 1467, 1470, 68, 1095, 1097, 1365, 342, 1494, 94, 351, 992, 482, 616, 874, 492, 113, 498, 372, 373, 374, 635]\n",
      "Trained partially on the synthesized data: \n",
      "Accuracy: 74.96, F1 Score:  71.37, Precision:  69.74, Recall:  74.96\n",
      "Trained partially on the data from another location: \n",
      "Accuracy: 82.78, F1 Score:  84.04, Precision:  88.82, Recall:  82.78\n",
      "Available websites: [214, 925, 184, 682, 979, 739, 998, 742, 113, 602, 715, 1229, 1062, 969, 192, 708, 554, 80, 874, 795, 917, 991, 492, 1028, 640, 869, 1432, 413, 1193, 762, 789, 760, 558, 71, 1215, 547, 653, 879, 654, 34, 346, 1446, 157, 252, 1465, 980, 1145, 243, 1291, 1065, 402, 1156, 1141, 705, 454, 1003, 1244, 207, 899, 405, 562, 374, 722, 993, 112, 1102, 354, 44, 134, 575, 1341, 706, 790, 616, 310, 1097, 1030, 321, 137, 854, 632, 1443, 288, 839, 397, 1094, 1292, 1299, 498, 1013, 510, 133, 549, 485, 1311, 1056, 383, 1076, 1002, 132, 100, 515, 106, 1181, 608, 1212, 1349, 111, 1416, 635, 1276, 827, 964, 945, 1357, 793, 1168, 297, 846, 24, 1305, 961, 401, 126, 573, 990, 1230, 128, 1321, 876, 376, 1176, 782, 91, 1303, 916, 342, 1084, 927, 780, 950, 690, 1254, 709, 738, 491, 1359, 1286, 36, 977, 845, 733, 385, 434, 754, 394, 814, 315, 419, 983, 785, 1082, 68, 1489, 729, 190, 556, 84, 718, 550, 835, 531, 1297, 32, 386, 597, 1117, 1441, 11, 623, 94, 1400, 805, 787, 966, 1340, 591, 1068, 172, 513, 1296, 420, 1143, 1033, 9, 1113, 368, 444, 1032, 400, 1224, 675, 307, 8, 1077, 1239, 213, 544, 1365, 655, 1001, 952, 588, 1114, 1162, 1442, 526, 1184, 482, 1409, 1264, 396, 373, 766, 107, 1425, 884, 187, 235, 1007, 1331, 1211, 325, 948, 1067, 1227, 1008, 1294, 1152, 1037, 954, 473, 1095, 474, 673, 1464, 238, 463, 992, 720]\n",
      "Missing websites: [1281, 1413, 391, 520, 1289, 1164, 15, 1041, 657, 915, 794, 669, 158, 1315, 37, 294, 1319, 933, 813, 686, 49, 181, 182, 951, 57, 441, 1467, 701, 1470, 1348, 581, 1358, 847, 1494, 347, 351, 1249, 98, 99, 1122, 353, 359, 872, 494, 372, 885, 1142, 1014, 1401, 381]\n",
      "Trained partially on the synthesized data: \n",
      "Accuracy: 73.88, F1 Score:  70.08, Precision:  68.30, Recall:  73.88\n",
      "Trained partially on the data from another location: \n",
      "Accuracy: 82.90, F1 Score:  84.46, Precision:  89.14, Recall:  82.90\n",
      "Available websites: [1032, 1227, 686, 562, 80, 544, 1409, 1056, 718, 549, 351, 383, 708, 980, 1211, 44, 623, 950, 252, 998, 1299, 789, 1224, 1291, 1142, 1470, 190, 874, 1289, 1254, 1067, 653, 884, 1446, 1003, 1319, 846, 473, 879, 359, 1145, 969, 1349, 1176, 1425, 98, 374, 1114, 952, 376, 307, 1077, 353, 1215, 397, 531, 1264, 1013, 690, 238, 733, 498, 673, 99, 616, 84, 134, 790, 36, 192, 373, 948, 550, 738, 827, 876, 588, 1041, 111, 158, 288, 485, 961, 526, 34, 381, 1002, 845, 766, 385, 1311, 1432, 602, 715, 597, 1156, 635, 1030, 1113, 368, 835, 558, 325, 839, 1441, 401, 1321, 1489, 814, 925, 675, 945, 235, 132, 1141, 669, 474, 1249, 1303, 655, 126, 106, 1168, 214, 720, 310, 1062, 709, 1152, 1348, 494, 640, 434, 15, 754, 91, 57, 8, 847, 1341, 1276, 990, 917, 632, 1084, 591, 396, 1117, 315, 243, 1001, 294, 966, 137, 400, 492, 394, 706, 581, 654, 785, 1076, 869, 1097, 24, 1244, 510, 547, 1037, 515, 793, 739, 94, 729, 762, 1239, 213, 1065, 1122, 1286, 1331, 11, 128, 441, 347, 813, 321, 951, 1296, 1028, 513, 1400, 113, 1305, 1014, 187, 107, 297, 1230, 182, 1442, 172, 1494, 964, 760, 354, 491, 1401, 1465, 682, 1229, 1358, 1467, 405, 386, 1068, 71, 915, 991, 1413, 805, 1008, 420, 872, 454, 993, 1082, 794, 787, 419, 1212, 1365, 1184, 608, 927, 722, 37, 9, 1292, 342, 444, 575, 705, 100, 402, 1297, 520, 49, 1181, 413]\n",
      "Missing websites: [1281, 899, 133, 391, 1416, 1033, 1162, 1164, 780, 782, 1294, 657, 916, 795, 157, 32, 1315, 1443, 933, 1193, 554, 556, 181, 184, 1464, 954, 1340, 573, 701, 68, 1094, 1095, 1357, 1102, 207, 1359, 463, 977, 979, 854, 983, 346, 992, 482, 742, 1007, 112, 372, 885, 1143]\n",
      "Trained partially on the synthesized data: \n",
      "Accuracy: 74.08, F1 Score:  70.29, Precision:  68.49, Recall:  74.08\n",
      "Trained partially on the data from another location: \n",
      "Accuracy: 82.44, F1 Score:  83.76, Precision:  88.94, Recall:  82.44\n",
      "Available websites: [182, 491, 1432, 739, 550, 1082, 1264, 983, 1001, 32, 655, 1297, 1032, 690, 1142, 346, 1084, 1289, 133, 1227, 1359, 980, 795, 847, 623, 573, 948, 297, 1065, 591, 381, 1215, 1007, 805, 993, 793, 24, 742, 391, 933, 705, 1331, 1349, 1400, 157, 1470, 1276, 1494, 669, 37, 1357, 1294, 632, 347, 100, 951, 34, 420, 111, 992, 441, 413, 1365, 787, 405, 835, 673, 1152, 1168, 969, 1008, 515, 126, 190, 1416, 845, 112, 554, 49, 549, 1097, 187, 1212, 238, 373, 214, 172, 1401, 520, 885, 654, 235, 1413, 1286, 991, 402, 990, 1181, 1030, 1143, 686, 396, 1425, 827, 494, 99, 760, 729, 581, 294, 916, 325, 1076, 917, 635, 1340, 207, 57, 1014, 68, 1211, 113, 181, 1145, 474, 547, 385, 1122, 925, 785, 310, 353, 782, 966, 706, 954, 562, 1003, 1409, 675, 1156, 1442, 397, 134, 701, 1296, 884, 794, 1299, 1319, 132, 80, 715, 640, 71, 1305, 394, 927, 718, 383, 1467, 158, 979, 1184, 657, 556, 252, 1095, 874, 964, 597, 780, 372, 485, 1164, 998, 879, 510, 754, 1224, 84, 137, 1311, 544, 846, 961, 558, 1068, 1292, 945, 1002, 1113, 709, 588, 1033, 1114, 766, 98, 720, 950, 315, 288, 869, 722, 454, 8, 1239, 653, 1358, 682, 192, 526, 1041, 1067, 813, 243, 419, 1446, 899, 9, 1141, 1249, 814, 602, 1162, 492, 1037, 977, 44, 616, 106, 738, 1348, 376, 1303, 94, 351, 872, 463, 386, 368, 1013, 876, 839, 11, 498, 733, 473, 1254, 790]\n",
      "Missing websites: [128, 513, 1281, 1028, 1291, 15, 400, 401, 531, 915, 789, 1176, 1056, 1441, 1315, 36, 1443, 1062, 1193, 1321, 434, 307, 1077, 184, 952, 1464, 1465, 444, 1341, 575, 321, 708, 1094, 1229, 1102, 1230, 1489, 213, 342, 854, 91, 1244, 1117, 608, 354, 482, 359, 107, 374, 762]\n",
      "Trained partially on the synthesized data: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained partially on the synthesized data: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m accuracy, precision, recall, f1_score, cm \u001b[38;5;241m=\u001b[39m \u001b[43mclassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_classification_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# classification.show_confusion_matrix_heatmap(cm, le, f\"Synthetic Data: missing classes: {missing_websites}\")\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# using real data from X\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Prepare the data\u001b[39;00m\n\u001b[1;32m     57\u001b[0m X_train, y_train, X_test, y_test, le \u001b[38;5;241m=\u001b[39m prepare_data(target_df, train_location_df, available_websites, missing_websites)\n",
      "File \u001b[0;32m~/doh_traffic_analysis/code/scripts/classification.py:10\u001b[0m, in \u001b[0;36mevaluate_classification_model\u001b[0;34m(X_train, y_train, X_test, y_test, model)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_classification_model\u001b[39m(X_train, y_train, X_test, y_test, model) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, np\u001b[38;5;241m.\u001b[39marray]:\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mscore(X_test, y_test)\n\u001b[1;32m     12\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/doh_traffic_analysis/.venv/lib/python3.10/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/doh_traffic_analysis/.venv/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    481\u001b[0m ]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/doh_traffic_analysis/.venv/lib/python3.10/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/doh_traffic_analysis/.venv/lib/python3.10/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/doh_traffic_analysis/.venv/lib/python3.10/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/doh_traffic_analysis/.venv/lib/python3.10/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/doh_traffic_analysis/.venv/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    201\u001b[0m         X,\n\u001b[1;32m    202\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m~/doh_traffic_analysis/.venv/lib/python3.10/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scripts.classification as classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import random\n",
    "\n",
    "def prepare_data(target_df, synthetic_df, available_websites, missing_websites, test_size=0.2, random_state=42):\n",
    "    # Split the target data into initial train and test sets\n",
    "    target_train, target_test = train_test_split(target_df, test_size=test_size, random_state=random_state, stratify=target_df['Website'])\n",
    "    \n",
    "    # Filter target data for available websites (for training)\n",
    "    target_train_data = target_train[target_train['Website'].isin(available_websites)]\n",
    "    \n",
    "    # Get synthetic data for missing websites (for training)\n",
    "    synthetic_train_data = synthetic_df[synthetic_df['Website'].isin(missing_websites)]\n",
    "    \n",
    "    # Combine real and synthetic data for training\n",
    "    train_data = pd.concat([target_train_data, synthetic_train_data], axis=0)\n",
    "    \n",
    "    # Prepare features and labels for training\n",
    "    X_train = train_data.iloc[:, 2:]  # All columns except 'Location' and 'Website'\n",
    "    y_train = train_data['Website']\n",
    "    \n",
    "    # Prepare test data (only real data from the target test set)\n",
    "    X_test = target_test.iloc[:, 2:]\n",
    "    y_test = target_test['Website']\n",
    "    \n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y_train_encoded = le.fit_transform(y_train)\n",
    "    y_test_encoded = le.transform(y_test)\n",
    "    \n",
    "    return X_train, y_train_encoded, X_test, y_test_encoded, le\n",
    "\n",
    "for i in range(10):\n",
    "    available_websites = random.sample(test_web_samples, 250)\n",
    "    missing_websites = list(set(test_web_samples) - set(available_websites))\n",
    "\n",
    "    print(\"Available websites:\", available_websites)\n",
    "    print(\"Missing websites:\", missing_websites)\n",
    "\n",
    "    # using synthetic\n",
    "    # Prepare the data\n",
    "    X_train, y_train, X_test, y_test, le = prepare_data(target_df, synthetic_df, available_websites, missing_websites)\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    model = RandomForestClassifier()\n",
    "    print(\"Trained partially on the synthesized data: \")\n",
    "    accuracy, precision, recall, f1_score, cm = classification.evaluate_classification_model(X_train, y_train, X_test, y_test, model)\n",
    "    # classification.show_confusion_matrix_heatmap(cm, le, f\"Synthetic Data: missing classes: {missing_websites}\")\n",
    "    \n",
    "    # using real data from X\n",
    "    # Prepare the data\n",
    "    X_train, y_train, X_test, y_test, le = prepare_data(target_df, train_location_df, available_websites, missing_websites)\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    model = RandomForestClassifier()\n",
    "    print(\"Trained partially on the data from another location: \")\n",
    "    accuracy, precision, recall, f1_score, confusion_matrix = classification.evaluate_classification_model(X_train, y_train, X_test, y_test, model)\n",
    "    # classification.show_confusion_matrix_heatmap(cm, le, f\"From LOC1: missing classes: {missing_websites}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Synthetic, Test on Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_for_web_classification(train_location_df, target_location_df, train_location, test_location):\n",
    "    le = LabelEncoder()\n",
    "    X_train = train_location_df[train_location_df['Location'] == train_location].drop(\n",
    "        ['Location', 'Website'], axis=1)\n",
    "    X_test = target_location_df[target_location_df['Location'] == test_location].drop(\n",
    "        ['Location', 'Website'], axis=1)\n",
    "    \n",
    "    y_train = train_location_df[train_location_df['Location'] == train_location]['Website']\n",
    "    y_test = target_location_df[target_location_df['Location'] == test_location]['Website']\n",
    "\n",
    "    y_test = le.fit_transform(y_test)\n",
    "    y_train = le.fit_transform(y_train)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, le\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 11.76, F1 Score:  8.87, Precision:  11.01, Recall:  11.76\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import scripts.classification as classification\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "X_train, y_train, X_test, y_test, le = preprocess_data_for_web_classification(synthetic_df, target_df, 'LOC2', 'LOC2')\n",
    "accuracy, precision, recall, f1_score, cm = classification.evaluate_classification_model(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00551038, 0.00865423, 0.00499711, 0.00930905, 0.00657184,\n",
       "       0.00582409, 0.00491639, 0.00445065, 0.00935057, 0.00445735,\n",
       "       0.00599664, 0.01181599, 0.01009915, 0.00987499, 0.00678659,\n",
       "       0.00784897, 0.01096159, 0.0110471 , 0.01190982, 0.0083488 ,\n",
       "       0.00934716, 0.01057891, 0.01077117, 0.01087088, 0.00809168,\n",
       "       0.00940118, 0.01053337, 0.01014823, 0.0098367 , 0.00756151,\n",
       "       0.00939466, 0.00902372, 0.0096759 , 0.00971414, 0.00686099,\n",
       "       0.00902069, 0.00925922, 0.0091321 , 0.00925508, 0.0060042 ,\n",
       "       0.00922879, 0.00883642, 0.00873171, 0.00921564, 0.00595471,\n",
       "       0.00871605, 0.00949108, 0.00927673, 0.00763584, 0.00633038,\n",
       "       0.00829786, 0.00863335, 0.00873818, 0.00842418, 0.00566738,\n",
       "       0.00906406, 0.00794059, 0.0088508 , 0.00759698, 0.0056253 ,\n",
       "       0.00730147, 0.00795783, 0.00725209, 0.00767712, 0.00515663,\n",
       "       0.00846798, 0.00887356, 0.00750193, 0.00831806, 0.00588214,\n",
       "       0.00823078, 0.00964374, 0.00782754, 0.00814804, 0.00582354,\n",
       "       0.0065054 , 0.00862982, 0.00603134, 0.00735013, 0.00570132,\n",
       "       0.00925641, 0.00770682, 0.00739323, 0.00770152, 0.00565784,\n",
       "       0.00812132, 0.00841453, 0.00714387, 0.00716799, 0.00684014,\n",
       "       0.00665847, 0.00857032, 0.00656238, 0.00747011, 0.00671006,\n",
       "       0.00911702, 0.00811987, 0.00731467, 0.00745658, 0.00684052,\n",
       "       0.00759739, 0.00816898, 0.00695531, 0.00720066, 0.00742335,\n",
       "       0.00730196, 0.00775889, 0.00723026, 0.00782974, 0.0064841 ,\n",
       "       0.00764416, 0.00810519, 0.00722743, 0.00742434, 0.00644235,\n",
       "       0.00765935, 0.0078748 , 0.00734825, 0.00879323, 0.00626123,\n",
       "       0.00837392, 0.00861986, 0.00869581, 0.00742984, 0.0068987 ,\n",
       "       0.00727802])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 35.  33.  16. 137.   0.   0.   0.  41.   4.   4. 104.  49.   0.   7.\n",
      "   7.   0.  19.  17.   0.   0.  41. 127.  88.   0.   0.   0.   2.  24.\n",
      "   2.   3.   8.   0.   0.  47.   1.  14.   0.   2.   0.   0.   0.   0.\n",
      "   0.   6.   7.  42.   0.   0.   0.   0. 111.   0.   0.  21.   0.   0.\n",
      "  54.   0.   3.  42.   0.   2.   0.  11.  23.   0.  67.   0.   0.   1.\n",
      "   0.   0.   0.   0.  62.  56.  19.   0.   0.   4.  15.   0.  61.   0.\n",
      "   0.   0.  12.  99.   4. 159.  65.   0.   2.   0.  51.   2.   5.  17.\n",
      "  61.  88.   4. 131.  14.   5.   0. 118.   0.   2.   0.   0.  28.   0.\n",
      "  71.   0. 142.   0. 124.   0.   9.   0.   0.  36.   0.  18.  77.   0.\n",
      "   0.   0.  29.  53.  50.  31.   0.  12.   0.   2. 145.  61.  35.   0.\n",
      "   1.   2.   0.  56.  10.  84.   0. 113.  83. 182.   0. 118.   0.  52.\n",
      "   0. 183.   2.   4.   1.  66.  50.   0.  39.   0.   0.  28. 101.   0.\n",
      "   0.   8.   0.  54.  24.  38.  69.   0.   0.   3. 105.   0.  81.   0.\n",
      "   1.   0.  83.   0.   3.  94.   0.   0.  46.   0. 125.  29.  62.   7.\n",
      "   0.   0. 138.   2.   0.   0.   1.   0.   0.   0. 137.   0.   0.  65.\n",
      "   0.   0.   0.   0.   2.  66.  17.  38.   0.  25.  12.   0.  62.   0.\n",
      "   0.   1.   0.   8.   1.   0.  77.   0.   0.   0.   0.   0.   0.  58.\n",
      " 100.   7.  51.   0. 108.   0.   0. 129.   0.   1.   0.   0.   0.   0.\n",
      "  10.   0.   0.   1.  54.   4.  89.   8.  35.   0.   3.   0.   0.   0.\n",
      "   0. 114.   0.   0.   0.   0.  52.   0.   7.  14.  68.   0. 103.   0.\n",
      "   0.   0.   0. 105.   6.   0.   0.   0.  98.   1.   0.   0.  10.   0.\n",
      "   0.  69.   0.   0.   0. 142.]\n"
     ]
    }
   ],
   "source": [
    "true_pos_synths = np.zeros(shape=(len(test_web_samples)))\n",
    "for i in range(len(test_web_samples)):\n",
    "    true_pos_synths[i] = cm[i][i]\n",
    "    \n",
    "print(true_pos_synths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train of another Location, test of Target location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.81, F1 Score:  65.51, Precision:  72.42, Recall:  63.81\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "X_train, y_train, X_test, y_test, le = preprocess_data_for_web_classification(train_location_df, target_df, 'LOC1', 'LOC2')\n",
    "accuracy, precision, recall, f1_score, confusion_matrix = classification.evaluate_classification_model(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(model.feature_importances_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.5\n",
      "0.785\n",
      "0.695\n",
      "0.78\n",
      "0.66\n",
      "0.79\n",
      "0.83\n",
      "0.825\n",
      "0.82\n",
      "0.81\n",
      "0.825\n",
      "0.8\n",
      "0.795\n",
      "0.815\n"
     ]
    }
   ],
   "source": [
    "step = 10\n",
    "# Loop through each step and set elements to zero\n",
    "print(\"0.815\")\n",
    "for i in range(0, X_train.shape[1], step):\n",
    "    _, _, X_test, y_test, le = preprocess_data_for_web_classification(train_location_df[train_location_df['Website'] == 8], target_df[target_df['Website'] == 8], 'LOC1', 'LOC2')\n",
    "    # print(X_test.shape, y_test.shape)\n",
    "    X_test_copy = X_test.copy()\n",
    "    X_test_copy.iloc[:, i:i + step] = 0\n",
    "    accuracy = model.score(X_test_copy, y_test)\n",
    "    print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[154. 145. 136. 175.  73. 146. 162. 139. 139. 141. 179. 111. 139.  40.\n",
      "  12. 134. 146. 103. 153. 100.  87. 121. 126. 144. 178. 167.  55. 174.\n",
      " 117.  83. 126.  50.  77. 124. 134. 141.  95. 118. 143. 130.  27.  75.\n",
      " 114. 162. 106. 115.  99. 185. 121. 126. 156. 108. 122.  90. 175. 115.\n",
      " 152. 175. 173. 183. 153.  98. 117.  92. 155. 153. 148.   0. 154. 130.\n",
      " 186. 145. 185.   4. 125. 159.  37.   4. 146.  92. 181. 159. 159. 134.\n",
      " 126. 130. 125.  77. 175. 169.  96.  99.  96. 107. 139.  72. 139. 118.\n",
      "  54. 124. 176. 188. 140. 103. 137. 132. 134.  87. 135. 126. 169. 185.\n",
      " 173. 137. 175. 141. 139. 146. 116. 149. 126. 169. 112. 100. 138. 149.\n",
      " 150.  91. 152. 175. 116. 142. 146. 144.  62. 129. 181. 187. 181. 168.\n",
      " 131. 155. 178.  99. 166. 143. 119. 142.  28. 130.   0. 144. 129. 150.\n",
      " 181. 175.  98. 168. 105. 158. 110. 145. 141. 122. 110. 163. 171.  92.\n",
      " 142. 164. 110. 147. 154. 103. 123. 171. 116. 141. 182. 110. 126. 138.\n",
      " 179. 142. 117. 138. 113. 171. 143. 177. 144. 149. 180. 135. 127. 118.\n",
      "  53. 127. 140. 139. 132. 148. 143. 130. 151. 174. 120. 183. 183. 179.\n",
      " 188. 137.   0. 118. 132. 119. 147. 109. 130. 149. 112. 141. 116. 140.\n",
      "   0. 172. 107. 103.  53. 116. 131. 174. 144. 113. 145. 101.  32. 109.\n",
      " 130. 133. 150. 170. 171.  77. 133. 135. 155. 183. 111. 132. 167. 144.\n",
      " 119. 150.   0. 131. 122. 107.  37. 136.  89. 144.  22.  49. 152. 116.\n",
      "  80. 129.  74. 118. 116. 114. 187. 146. 123. 175. 115. 151. 134. 145.\n",
      " 110. 120.  81. 153. 173.  89. 113. 142. 113. 111. 131. 146. 117. 165.\n",
      "  44. 123. 121. 127.  98. 117.]\n"
     ]
    }
   ],
   "source": [
    "true_pos_dif_location = np.zeros(shape=(len(test_web_samples)))\n",
    "for i in range(len(test_web_samples)):\n",
    "    true_pos_dif_location[i] = confusion_matrix[i][i]\n",
    "    \n",
    "print(true_pos_dif_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False,  True,  True, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False,  True])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_pos_synths > true_pos_dif_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on both synthetic and other location's data and test on the actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_df = pd.concat((synthetic_df, train_location_df), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_df.sort_values(by=['Website'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mixed_df.iloc[:, 2:]\n",
    "y_train = mixed_df.Website\n",
    "X_test = target_df.iloc[:, 2:]\n",
    "y_test = target_df.Website\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_test = le.fit_transform(y_test)\n",
    "y_train = le.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.15, F1 Score:  60.53, Precision:  65.77, Recall:  60.15\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "accuracy, precision, recall, f1_score, confusion_matrix = classification.evaluate_classification_model(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location Classification using Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The location classification had an high accuracy with just a XGBoostClassifier model. If our synthetic data is good, it should also be easily distinguishable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_df = pd.concat((pd.read_csv('../synthesized/sampling-LOC1-target-LOC2.csv').iloc[:, 1:], \n",
    "                         pd.read_csv('../synthesized/sampling-LOC2-target-LOC1.csv').iloc[:, 1:] \n",
    "                          ), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Websites: [1309, 228, 51, 563, 501, 457, 285, 209, 1385, 1116, 178, 1209, 864, 65, 61, 191, 447, 476, 1034, 1232, 54, 1149, 407, 1466, 1330, 1436, 1490, 859, 451, 919, 1206, 569, 13, 326, 1429, 865, 696, 1468, 318, 440, 689, 1492, 189, 778, 198, 735, 704, 1236, 541, 88, 940, 1098, 255, 775, 161, 1130, 600, 1287, 1266, 740, 1182, 393, 142, 93, 1354, 466, 592, 163, 1482, 206, 1456, 1462, 928, 1301, 747, 333, 758, 727, 429, 1372, 546, 1399, 1327, 146, 1247, 1300, 350, 1093, 1495, 334, 946, 777, 552, 1310, 1140, 449, 1402, 664, 114, 469, 1486, 646, 821, 548, 135, 432, 1161, 644, 435, 1342, 1022, 810, 1316, 939, 292, 542, 1493, 505, 1478, 1103, 538, 1197, 877, 1195, 817, 741, 1404, 283, 1043, 1010, 186, 96, 224, 313, 1285, 327, 1487, 1221, 130, 788, 781, 1220, 958, 1083, 514, 1133, 23, 234, 1099, 1419, 1312, 1463, 1498, 601, 890, 323, 929, 6, 539, 1025, 365, 1039, 217, 1280, 611, 1308, 1338, 1415, 1477, 1366, 765, 330, 1104, 1086, 1, 1226, 663, 1000, 39, 229, 743, 629, 490, 118, 493, 1393, 1445, 175, 995, 141, 1090, 257, 262, 973, 1125, 338, 1384, 1080, 1242, 866, 433, 1417, 411, 638, 1375, 764, 897, 1059, 924, 247, 507, 460, 131, 692, 43, 1204, 1134, 471, 1205, 1471, 14, 145, 120, 468, 138, 64, 676, 1278, 1052, 487, 570, 994, 438, 1298, 270, 1169, 1180, 968, 497, 1262, 833, 389, 193, 1455, 882, 725, 867, 841, 956, 110, 201, 124, 824, 694, 223, 509, 392, 1258, 1448, 918, 287, 1363, 375, 1269, 947, 511, 154, 907, 1127, 200, 103, 1107, 30, 1484, 484, 340, 832, 1268, 985, 437, 1397, 1277, 337, 776, 4, 799, 543, 931, 584, 1414, 1138, 996, 317, 388, 607, 445, 119, 1186, 1110, 1248, 642, 117, 102, 1196, 976, 1029, 1087, 322, 116, 1040, 164, 380, 140, 139, 481, 826, 245, 1166, 504, 81, 167, 858, 1157, 1070, 647, 534, 418, 643, 488, 1213, 1388, 268, 614, 936, 1175, 148, 19, 938, 1153, 204, 150, 1101, 436, 1036, 1170, 271, 714, 1187, 500, 756, 583, 1344, 1293, 1112, 619, 1356, 16, 1135, 613, 212, 275, 1451, 236, 219, 1435, 1461, 557, 577, 431, 702, 416, 540, 1035, 1322, 1355, 104, 1457, 1253, 566, 90, 7, 683, 267, 536, 1328, 904, 875, 1163, 1320, 1233, 305, 73, 1150, 303, 880, 261, 85, 631, 746, 1263, 732, 430, 1234, 210, 724, 1223, 316, 1225, 332, 362, 844, 50, 367, 680, 843, 508, 1350, 1476, 221, 783, 79, 963, 455, 408, 942, 716, 625, 1434, 456, 48, 395, 816, 672, 1452, 1437, 571, 719, 1371, 818, 678, 56, 1137, 1174, 1339, 1155, 78, 222, 889, 707, 1199, 893, 1047, 1058, 1360, 1426, 521, 1120, 1049, 3, 403, 745, 883, 143, 1273, 1050, 1447, 615, 633, 836, 668, 1332, 605, 260, 1243, 861, 1216, 356, 630, 582, 308, 415, 561, 853, 0, 311, 293, 215, 1460, 804, 593, 621, 670, 329, 1431, 452, 1005, 691, 218, 523, 1092, 812, 922, 982, 815, 753, 173, 674, 86, 290, 527, 679, 648, 634, 343, 95, 838, 974, 769, 240, 688, 1207, 230, 825, 203, 1159, 25, 47, 250, 486, 1073, 870, 786, 74, 1072, 424, 1480, 1392, 589, 199, 1454, 713, 1438, 506, 409, 249, 151, 671, 1453, 5, 914, 768, 881, 1046, 906, 109, 797, 1391, 1367, 180, 823, 712, 530, 475, 1497, 1066, 1481, 868, 1200, 467, 136, 820, 937, 1118, 1055, 572, 609, 324, 773, 912, 453, 627, 834, 736, 913, 516, 1177, 850, 1018, 1071, 162, 761, 1255, 971, 1288, 265, 997, 253, 860, 652, 1420, 784, 796, 533, 496, 641, 244, 281, 450, 1079, 730, 1491, 981, 278, 986, 1364, 553, 82, 1406, 1201, 1048, 1026, 710, 156, 723, 1136, 1418, 965, 417, 1304, 555, 477, 425, 63, 211, 852, 1241, 398, 1235, 598, 1386, 20, 1302, 962, 1045, 1171, 1390, 360, 1109, 771, 399, 1421, 551, 1329, 752, 559, 819, 617, 225, 499, 1075, 279, 446, 1261, 29, 863, 344, 684, 695, 1295, 414, 1374, 169, 478, 1361, 637, 1144, 27, 1190, 606, 1132, 1060, 1449, 1380, 658, 1267, 1275, 472, 1369, 1439, 266, 1469, 335, 216, 465, 1410, 345, 779, 809, 284, 770, 1131, 258, 83, 1185, 1146, 767, 1407, 53, 358, 1111, 665, 70, 667, 41, 772, 31, 903, 1160, 1472, 636, 1377, 894, 129, 1126, 685, 1198, 1343, 1245, 1006, 1074, 1307, 377, 171, 620, 1009, 960, 774, 1179, 1283, 1250, 1433, 26, 319, 857, 693, 384, 406, 1351, 1376, 77, 1219, 1051, 1231, 248, 1124, 959, 1020, 700, 1167, 123, 579, 42, 355, 545, 1208, 677, 379, 862, 518, 1323, 349, 12, 1238, 1411, 108, 443, 370, 650, 470, 803, 1284, 941, 1057, 666, 276, 1389, 848, 495, 851, 984, 749, 274, 1115, 251, 1450, 1383, 461, 955, 1427, 1381, 624, 1259, 802, 1240, 1031, 957, 1091, 999, 1252, 1337, 363, 264, 348, 286, 610, 282, 1428, 10, 529, 195, 87, 1290, 1129, 1151, 568, 246, 1270, 661, 502, 458, 17, 1362, 301, 226, 830, 1444, 1475, 595, 949, 1024, 1121, 926, 352, 943, 1496, 871, 1017, 464, 277, 1345, 1334, 1105, 1440, 197, 1148, 122, 1396, 1123, 196, 1081, 902, 900, 603, 537, 1335, 289, 1378, 1256, 1106, 232, 369, 183, 309, 1279, 1194, 1408, 280, 46, 55, 659, 299, 699, 953, 105, 728, 587, 291, 480, 1317, 1336, 687, 188, 52, 798, 489, 1191, 66, 410, 503, 75, 590, 1479, 155, 152, 576, 1015, 989, 254, 121, 1064, 426, 231, 535, 856, 703, 920, 304, 439, 312, 1485, 101, 1405, 807, 1265, 944, 160, 1183, 177, 565, 76, 574, 2, 1173, 585, 898, 298, 33, 237, 295, 987, 901, 72, 239, 662, 202, 656, 763, 978, 596, 272, 1272, 1108, 580, 828, 314, 921, 1373, 127, 479, 594, 412, 887, 512, 1382, 448, 1038, 40, 442, 748, 256, 1423, 1474, 1352, 21, 1139, 1260, 179, 599, 1004, 801, 185, 878, 1346, 528, 522, 1023, 567, 341, 328, 886, 792, 1021, 717, 168, 1096, 737, 1178, 147, 339, 483, 205, 734, 586, 1042, 18, 1314, 45, 1313, 618, 165, 59, 1069, 1430, 532, 263, 422, 1016, 336, 1063, 651, 988, 1210, 1061, 1368, 905, 519, 909, 387, 934, 320, 800, 837, 681, 1333, 930, 896, 67, 1085, 840, 892, 357, 1158, 62, 626, 1192, 1128, 1251, 1078, 1459, 1100, 159, 698, 1119, 829, 208, 1306, 115, 1422, 58, 1488, 60, 331, 1228, 1054, 1282, 366, 149, 1027, 361, 1202, 578, 427, 1089, 241, 932, 233, 731, 967, 895, 97, 306, 1394, 382, 69, 35, 908, 855, 404, 849, 174, 822, 259, 806, 1325, 144, 371, 744, 300, 296, 1217, 972, 935, 1347, 525, 428, 176, 170, 423, 390, 1379, 1257, 873, 1189, 711, 459, 1044, 1271, 421, 1203, 1473, 22, 910, 242, 1214, 1326, 1398, 726, 1424, 750, 517, 639, 1274, 649, 302, 970, 811, 842, 364, 269, 697, 1483, 1172, 1458, 808, 891, 38, 888, 1395, 1222, 757, 751, 755, 524, 1246, 1011, 273, 194, 378, 721, 1403, 612, 1318, 1412, 1019, 1218, 645, 462, 604, 622, 1053, 1088, 923, 1499, 227, 831, 153, 911, 1353, 166, 28, 975, 628, 1324, 220, 660, 125, 1154, 1188, 560, 92, 1370, 89, 1147, 1237, 1165, 759, 564, 791, 1387, 1012]\n",
      "Training Locations: ['LOC1', 'LOC2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asil0892/doh_traffic_analysis/code/scripts/init_dataset.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df.sort_values(by=[\"Location\"], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "from scripts.init_dataset import get_sample\n",
    "\n",
    "train_synthetic_df, test_synthetic_df, _, _ = get_sample(synthetic_df, ['LOC1', 'LOC2'], test_web_samples, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.85, F1 Score:  90.83, Precision:  91.21, Recall:  90.85\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9085166666666666,\n",
       " 0.9120678790950376,\n",
       " 0.9085166666666666,\n",
       " 0.9083191396162973,\n",
       " array([[51726,  8274],\n",
       "        [ 2704, 57296]]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_data(train_df: pd.DataFrame, test_df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, LabelEncoder]:\n",
    "    le = LabelEncoder()\n",
    "    X_train = train_df.iloc[:, 2:]\n",
    "    X_test = test_df.iloc[:, 2:]\n",
    "\n",
    "    y_train = le.fit_transform(train_df['Location'])\n",
    "    y_test = le.transform(test_df['Location'])\n",
    "\n",
    "    return (X_train, X_test, y_train, y_test, le)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "X_train, X_test, y_train, y_test, le = preprocess_data(train_synthetic_df, test_synthetic_df)\n",
    "classification.evaluate_classification_model(X_train, y_train, X_test, y_test, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doh_synth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
